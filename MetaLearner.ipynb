{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:00:04.526413Z",
     "start_time": "2023-06-14T15:00:00.646625Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.saving import load_model\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Why 0 GPUs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:00:50.866898Z",
     "start_time": "2023-06-14T15:00:50.755859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# set global seed\n",
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:05.564191Z",
     "start_time": "2023-06-14T15:02:05.488079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/arberimbibaj/dataset_example_indicatorCATE.csv', header=None, index_col=[0])\n",
    "data = data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:06.396502Z",
     "start_time": "2023-06-14T15:02:06.346964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "d = len(data[0,:]) - 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:06.920152Z",
     "start_time": "2023-06-14T15:02:06.875892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# train test split\n",
    "random.shuffle(data)\n",
    "training, test = data[:700,:], data[700:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:07.353322Z",
     "start_time": "2023-06-14T15:02:07.315354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# slice dataset by treatment status\n",
    "training_control = training[training[:,26]==0]\n",
    "training_treatment = training[training[:,26]==1]\n",
    "\n",
    "# slice test set by treatment status\n",
    "test_control = test[test[:,26]==0]\n",
    "test_treatment = test[test[:,26]==1]\n",
    "\n",
    "# Y_train by treatment status\n",
    "Y_train_control = training_control[:,0]\n",
    "Y_train_treatment = training_treatment[:,0]\n",
    "\n",
    "# Y_test by treatment status\n",
    "Y_test_control = test_control[:,0]\n",
    "Y_test_treatment = test_treatment[:,0]\n",
    "\n",
    "# X_train by treatment status\n",
    "X_train_control = training_control[:,1:26]\n",
    "X_train_treatment = training_treatment[:,1:26]\n",
    "\n",
    "# X_test by treatment status\n",
    "X_test_control = test_control[:,1:26]\n",
    "X_test_treatment = test_treatment[:,1:26]\n",
    "\n",
    "# X and Y test\n",
    "X_test = test[:,1:26]\n",
    "Y_test = test[:,0]\n",
    "\n",
    "# X_train and Y_train (no split by treatment status)\n",
    "X_train = training[:,1:26]\n",
    "Y_train = training[:,0]\n",
    "\n",
    "# W_train and W_test\n",
    "W_train = training[:,26]\n",
    "W_test = test[:,26]\n",
    "\n",
    "# tau_test\n",
    "tau_test = test[:,27]\n",
    "tau_test_control = test_control[:,27]\n",
    "tau_test_treatment = test_treatment[:,27]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:07.768679Z",
     "start_time": "2023-06-14T15:02:07.722747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# set training and test features for the S-Learner (it views W as no different from other X's)\n",
    "X_W_train = training[:,1:27]\n",
    "X_W_test = test[:,1:27]\n",
    "X_test_0 = np.concatenate((test[:,1:26],np.zeros((300,1))), axis=1)\n",
    "X_test_1 = np.concatenate((test[:,1:26],np.ones((300,1))), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:02:08.364077Z",
     "start_time": "2023-06-14T15:02:08.324705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# T-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "s_learner.fit(X_W_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "s_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_0_hat.fit(X_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_1_hat.fit(X_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "g_x_hat.fit(X_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_test) + probas_0 * x_tau_1_hat.predict(X_test)\n",
    "x_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error (much lower here!)\n",
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "r_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_train)\n",
    "r_probas_0 = r_probas[:,0] # probabilities of W=0\n",
    "r_probas_1 = r_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_tau.fit(X_train,r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)\n",
    "r_tau_hats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_train)\n",
    "dr_probas_0 = dr_probas[:,0] # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_mu_0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_mu_1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_train) + (1 - W_train) * dr_learner_mu_0.predict(X_train) # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(X_train) - dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_train,dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "dr_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.91490677,  8.26301955,  0.97106317,  3.5528225 ,  8.67564974,\n       -4.68236172, 10.6688935 , -1.17413595, -2.63692004,  7.44641532,\n        0.2918939 ,  6.43137586,  3.73575211,  2.59984884, -0.86203239,\n        7.66755618,  2.51321221,  8.82989392, -4.41247407, -0.60076672,\n        2.64336168,  1.99061936,  8.96845941,  7.7862815 ,  8.21040191,\n        7.55267284, -1.14354599, -1.6653916 ,  1.25465375,  8.52964283,\n       -0.46274891, -0.08576136,  0.63671972,  1.27520691, 11.51410645,\n        2.00179904,  8.06064817,  0.79332432,  1.88817362,  0.45527881,\n        1.06693595, -0.11435547,  2.77432839,  4.61230113,  8.27338701,\n       10.53841517,  0.61909964, -4.58885619, -0.24858074,  1.83739071,\n        7.30205278,  0.52086065,  9.27698339,  7.95976077,  7.9325154 ,\n        8.00236788,  2.13039965, -2.13439482,  7.9438079 ,  1.6070221 ,\n        7.90002368,  2.89252177, -2.14836991, -4.90215694,  8.36905704,\n        7.70217215,  5.54715161, -1.83924182,  9.59718485,  6.96651919,\n        9.97623643, 10.50304114,  8.46248293,  5.44752101,  8.23284585,\n        1.87035848,  1.76253894,  1.096323  , -0.77281902,  4.65886033,\n        4.08693553,  9.06629475,  7.2846707 ,  6.91463276, -2.45478032,\n        4.29423858,  1.047026  , -4.00246238,  8.45749121,  7.3308703 ,\n        7.48653022,  8.34125527,  7.69065389,  4.52825058,  1.28770055,\n        0.63088139,  0.09525266,  8.75691332,  4.47194583,  7.23818097,\n        2.40536132,  6.45496018, -0.09073443,  8.38050831, 11.67956658,\n        9.05400165, -1.35824746,  3.67146358,  9.19640487,  1.58340767,\n        1.1739344 ,  3.73147696,  8.57807127, -0.7306568 ,  2.10871377,\n        1.93744957,  6.98260567,  2.12583702, 11.66317864,  1.92150873,\n        7.28413872,  8.8332668 ,  0.97069832,  3.13148742,  9.16374074,\n        1.1991766 ,  2.7466835 ,  7.87338095, -0.53782637,  0.16073633,\n        2.20942804,  1.5098778 ,  1.71295282,  0.97727523,  6.83324377,\n        8.6180895 , -3.92330714,  2.82209429, -2.0266383 , -7.59311155,\n        6.3671048 ,  3.15615404,  7.98434222,  3.33001691,  2.74047294,\n        1.03254886, -1.25837108, -2.92810248,  0.9371009 , -0.22489178,\n        9.36536038,  1.15156331, -4.86577529,  7.69639877,  1.66780274,\n        8.11559443, -1.14733722,  2.34846067, -1.13603509,  7.02301746,\n        2.05258472, -0.34833145,  3.93935158, -1.75590286,  7.08446459,\n        9.12730268, -1.34887892, -4.68200697,  0.03928503,  7.59127757,\n        3.1800284 , -0.60252759,  1.82650608,  0.70264057,  2.97982366,\n        3.89388756, -3.84804754, 13.27910146, 10.12442684,  6.17366687,\n        5.04829352,  1.67815604,  0.15533602,  6.07207105,  9.50717836,\n       -1.17030761, -0.39387043,  2.84623984, -1.77919363,  0.19379624,\n       -5.57453544,  9.40441703,  7.15254168,  7.4551914 ,  1.93933243,\n        1.54918812,  3.41283043,  8.42730052, -0.06615665,  0.59497905,\n       -2.35644412,  0.65944959,  5.69666181,  5.94491705,  8.09355056,\n       -1.49469063,  2.48439797,  0.37201801,  1.78266505,  8.01401948,\n        7.76224435,  8.2296767 ,  8.63921695,  7.83134102,  1.62885603,\n        1.22704062, -0.21580536, -0.27625405,  5.9063476 , -0.25489677,\n        8.93256632,  1.53016088,  3.33282188,  2.74638323, -0.91148359,\n        9.66951246,  7.62377911,  9.3824503 ,  8.49887363, 11.88052343,\n        8.72153874,  2.43572754, 10.23488754,  7.25785079,  1.52223446,\n        7.04445635,  7.69460576,  7.84594312,  8.70958812,  3.68954893,\n        1.27426381,  9.95993074,  3.9941588 ,  3.07839905, -4.39324739,\n        8.34635162,  2.14669136,  3.56857981,  8.43191879, -2.48142331,\n        3.85694975, 10.29505727,  8.20617483, -0.48670216,  0.34770498,\n        0.11407752,  7.6853113 ,  0.69642855,  0.05512238,  9.49577312,\n        9.80476658,  0.55308993,  7.7286017 ,  3.0569524 , -4.27417536,\n       -0.54798416,  5.77410651, -1.91132226,  8.28758938, -0.65651156,\n        7.00550478, -2.82245468,  0.10368094,  7.29295899,  5.76176052,\n        0.99843677,  0.38747159, -0.50173003,  9.4128133 ,  7.91981657,\n        0.61867981,  3.71024214,  3.4696848 ,  1.25762509,  2.14036319,\n        8.57665685, -3.8086749 ,  7.58413691,  5.01934423,  9.18869017,\n       -5.33839562,  7.69921217, -0.1654332 ,  2.41413054,  6.41967436,\n       -0.30457117,  1.75865245,  3.39697429,  8.06744271,  8.0756532 ])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "ra_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0.predict(X_train)) + (1 - W_train)*(ra_learner_mu1.predict(X_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:20:58.068598Z",
     "start_time": "2023-06-14T15:20:56.802871Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "5.355494017645751"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((ra_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:20:58.074773Z",
     "start_time": "2023-06-14T15:20:58.071674Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1.82258255e+00,  3.30253612e+00,  2.55699703e+00,  1.52583192e+00,\n        3.05871242e+00, -7.93422335e+00,  1.66045421e+01,  6.15961297e-01,\n       -4.57479980e+00,  2.83237001e+00, -1.72211845e+00,  2.29531910e+00,\n        1.36240571e+00,  9.35168611e+00, -3.82059473e+00,  3.24921597e+00,\n        4.72670705e+00,  2.61065262e+00,  4.19853116e+00, -1.09661101e+00,\n        4.63329534e+00,  2.21814235e+00,  6.21359562e+00,  2.71285893e+00,\n        7.53868582e+00,  1.72688984e+00,  4.69543777e-01, -6.14387931e-01,\n       -5.41671550e+00, -1.00237007e+00, -2.05090451e+00,  4.79920008e+00,\n        3.04292536e+00,  4.84487128e+00,  2.27057392e+00,  3.01363921e-01,\n       -1.79640246e+00,  3.02498879e+00,  2.21134199e+00,  3.00075400e-01,\n       -1.06077085e+01, -4.78677322e+00,  3.77265746e+00,  7.38518006e+00,\n        3.09171163e+00,  8.09618033e+00, -1.83045696e+00, -7.97791039e+00,\n        1.26115023e+00,  1.30512966e+00,  1.45183221e+00, -1.25497004e+01,\n        2.64006377e+00, -1.07418671e+00,  5.04348802e+00,  6.02661225e+00,\n        1.04933170e+00,  7.97815202e+00,  5.57115699e+00,  4.22883340e+00,\n        6.33749198e+00,  5.89046841e+00,  7.68585993e+00, -2.21031726e+00,\n        4.41905895e+00,  7.27868615e-01,  3.48095488e+00, -1.10005714e+00,\n        4.52322546e+00,  8.76291148e-01, -2.61482766e+00,  4.46839425e+00,\n        5.65380170e+00,  6.29284284e+00,  3.96295140e+00, -1.33718704e+01,\n        9.59760564e-01,  1.13232364e+00, -1.06857754e+00, -6.94970528e+00,\n        1.57381987e+00, -1.35258872e+00,  7.13737200e+00, -5.36909428e+00,\n        2.10550523e+00,  3.12216537e+00,  4.26588598e+00, -5.68366593e-01,\n        1.29108268e+01,  1.36647976e+01,  3.03317307e+00,  1.78850735e+00,\n        4.09589891e+00, -1.03458342e+01,  5.65992709e+00,  7.14521158e+00,\n       -2.37676021e+00,  9.27303412e+00,  3.10103507e+00, -4.30615989e+00,\n        4.87419270e+00,  7.86900838e+00,  3.97782850e+00,  5.00885908e+00,\n        8.74326191e+00,  4.76571678e+00, -1.08773063e+00,  7.38819954e+00,\n        1.62262652e+00,  3.81900379e+00,  1.02354829e+00,  3.87462008e+00,\n        9.22481331e-01,  5.69469881e-01,  1.23756377e+00,  5.07442099e+00,\n        4.80579680e+00,  5.63415612e+00,  3.26966500e+00, -4.34137155e+00,\n        4.16049281e+00,  5.13054958e+00, -1.45028871e+00,  2.46243337e+00,\n       -4.15512787e-01,  9.75561898e-01,  5.22021159e+00,  5.59562018e+00,\n       -4.35770953e+00,  8.35278577e+00,  4.42113556e+00,  2.63690251e+00,\n        4.37108609e+00,  9.11688894e-01,  4.22485576e+00,  2.99440114e+00,\n        1.22345967e+01, -1.68845429e+00, -2.58694240e+00,  3.82013577e+00,\n        3.77912393e+00, -4.05402349e+00,  3.99128581e+00, -8.59463801e-02,\n        1.14335446e+01,  7.28842396e+00,  9.58041550e-01,  4.51141946e+00,\n        3.16602471e+00,  1.75433488e+00,  6.08615547e+00,  4.44180181e+00,\n       -2.80612597e+00, -1.42975507e+00,  2.04395584e+00,  2.20713619e+00,\n       -1.42105100e+00, -2.76184903e+00,  7.59467864e-01,  4.21301847e+00,\n        2.55507072e+00, -1.31901061e+00,  9.38087666e+00,  3.94813702e+00,\n        4.85375302e+00, -3.71439785e-02,  7.68253995e+00, -1.04225448e+00,\n        3.64288714e+00,  5.75851400e+00,  2.65598039e+00,  2.25693395e+00,\n       -3.86434597e-01,  1.32804066e+00, -1.67247194e+01,  1.64851758e+01,\n       -9.30288456e-01,  1.82509416e+01,  5.67351735e+00,  6.15135166e+00,\n        1.29407675e+01,  3.10447119e+00,  4.21165706e+00,  1.16231825e+01,\n        1.71178296e+00, -3.03036218e+00, -6.75180526e+00,  3.07442396e+00,\n        9.12085449e-01,  9.26538405e-01, -3.40865203e+00,  9.79461389e+00,\n        4.70121283e+00,  9.52717466e-01,  2.57934741e-01,  1.91916963e+00,\n       -1.96947859e-01,  5.30628290e-01, -2.24369560e-03,  7.10492633e-01,\n        4.78015898e+00, -1.67362608e+00,  2.45041223e+00,  2.76535812e+00,\n        5.37916055e+00,  9.57145666e+00,  2.09724445e+00, -2.26398295e+00,\n        5.64363479e+00,  7.94612669e+00,  4.48391202e+00,  2.30552666e+00,\n        1.01693380e+01,  2.45100295e+00,  2.57546183e+00, -3.78701155e+00,\n        2.10867530e-01,  3.73603719e+00,  3.57801603e+00, -1.09273918e+00,\n        3.56493062e+00,  3.45827331e+00, -2.89139182e-01,  8.48163348e+00,\n        1.95961388e+00,  5.31136810e-01,  5.17678369e+00,  1.21137826e+01,\n        6.53308306e+00,  1.38177417e+00,  2.95677457e+00,  2.52148613e+00,\n        7.98813810e+00, -1.70209145e+00,  2.38990487e+00,  1.51194673e+00,\n        6.93170492e-01,  2.53934831e+00,  2.76251763e+00,  6.88423172e+00,\n        3.27910852e+00,  7.51165991e+00, -3.65345322e+00, -6.57148526e-01,\n       -5.75015287e+00,  3.89055396e+00,  3.65067272e+00,  1.79818499e+00,\n        2.09724795e+00, -2.81032181e+00,  3.01336867e+00,  4.74854324e+00,\n        5.47661505e+00,  7.14886361e-01,  8.60653097e-01,  9.34259832e-02,\n        8.69490061e+00,  1.10112801e+00, -3.43096127e+00,  7.97803373e+00,\n        1.02171987e+01,  1.31141601e+00,  8.44342348e-01,  4.51721706e+00,\n       -8.93919717e+00, -1.39077010e+00,  3.10926445e+00,  3.27146249e-01,\n        2.93779996e+00, -7.27467842e-02,  4.02239631e+00,  1.74133380e+00,\n        4.03851410e-01,  2.49598293e+00, -7.31996280e+00,  1.17594163e+00,\n        8.13995907e-01, -2.04990705e+00,  8.16159549e+00,  2.89914897e+01,\n       -1.75899786e+00,  7.39483192e+00, -8.68440667e+00,  4.01885086e+00,\n        2.04759494e+00,  2.62195481e+00,  2.67469557e+00,  5.17258698e+00,\n        2.00744372e+00,  3.29384595e+00, -4.08052371e+00,  5.08157354e-01,\n        2.54545432e+00, -5.02330329e+00,  2.59447717e+00,  1.47529137e+00,\n        4.11158091e+00,  9.20427993e-01,  5.07501693e+00,  2.61136450e+00])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PW-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "pw_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train/pw_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(pw_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:37:45.593371Z",
     "start_time": "2023-06-14T15:37:43.965361Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "30.529802728890644"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((pw_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:37:45.600234Z",
     "start_time": "2023-06-14T15:37:45.596356Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F-Learner: AKA THE SAME AS PW-LEARNER!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### F-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "f_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "f_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "f_pseudo_outcome = (W_train/f_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(f_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "f_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "f_tau_hat = f_tau_hat_learner.predict(X_test)\n",
    "f_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "print(((f_tau_hat - tau_test)**2).mean())\n",
    "print(\"Same as for PW-Learner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2.85250904e+00,  1.00547688e+01, -4.92574582e-01,  1.91452713e+00,\n        7.92620400e+00, -1.16463139e+01,  1.31921917e+01, -2.11709957e+00,\n       -7.16250459e+00,  6.00462123e+00,  2.72981362e+00,  7.66322441e+00,\n        2.45976887e+00,  1.18104536e+00,  9.14107486e-01,  6.16511795e+00,\n        1.08497420e+01,  1.01644602e+01, -2.16899051e+00, -1.81475606e+00,\n        1.34111458e+00,  3.41936913e+00,  1.04168301e+01,  7.57884238e+00,\n        9.50578740e+00,  2.09883177e+00,  1.69555973e+00,  2.79744550e+00,\n        4.16811513e-03,  1.50075375e+01, -4.28915305e+00, -8.77925502e-01,\n       -2.78016855e+00,  3.32467561e+00,  1.06499625e+01,  3.00485772e+00,\n        8.31998895e+00,  4.89600153e+00,  6.81894459e-02,  2.19437557e-02,\n       -1.49448971e+00, -1.36872242e+00,  4.83187150e+00,  9.35912173e+00,\n        6.47748569e+00,  1.37123644e+01,  4.08149426e+00, -2.59808756e+00,\n        3.33981472e+00,  1.33195860e+00,  7.00985818e+00,  2.88766820e+00,\n        1.12951611e+01,  9.49361535e+00,  8.27421488e+00,  9.65030646e+00,\n        1.66799120e+00, -4.27047407e+00,  5.46663229e+00,  5.45520384e+00,\n        5.67327631e+00,  3.40459522e+00, -3.44338015e+00, -6.12381311e+00,\n        9.49446434e+00,  7.70510149e+00,  6.20623224e+00, -5.47830310e+00,\n        8.40988433e+00,  1.97504134e+00,  5.41909400e+00,  8.66600078e+00,\n        7.82765671e+00,  7.11545164e+00,  2.04529673e+01,  4.40529511e-01,\n        3.37877850e+00, -3.56197473e-03, -6.86873419e-02,  4.57507701e+00,\n        6.59528519e+00,  9.10911278e+00,  9.41461153e+00,  6.89354467e+00,\n       -1.47101875e+00,  4.48008331e+00, -2.54218086e+00, -3.59854074e+00,\n        8.98837471e+00,  1.03377520e+01,  6.94540158e+00,  6.49344661e+00,\n        7.70222005e+00,  5.04733032e+00,  6.24781139e-01,  3.07909370e+00,\n       -3.12769196e+00,  1.49029864e+01,  6.29949225e+00,  6.63651138e+00,\n        4.61421670e+00,  3.67322564e+00,  1.43165296e+00,  9.89797870e+00,\n        1.31450812e+01,  8.97206431e+00, -2.22841353e+00,  7.85190017e+00,\n        2.80039878e+01,  9.66392220e+00, -6.51791900e-01,  3.97786190e+00,\n        1.23782826e+01, -7.80540663e-01,  5.26144751e+00,  3.36665356e+00,\n        5.86354648e+00,  3.64687801e+00,  1.12314667e+01,  1.76252990e+00,\n        8.47935090e+00,  1.83555436e+01,  5.96450093e+00,  4.13233759e+00,\n        7.77252980e+00,  1.37507557e+00,  4.46851179e+00,  9.77506013e+00,\n        1.80642718e+00,  2.41555820e+00,  2.20732427e+00,  3.84527284e+00,\n        3.66475808e+00,  2.32778550e+00,  9.16702510e+00, -5.83256253e-01,\n       -1.04964039e+01, -8.16031335e-01, -6.20261281e+00, -1.43077006e+01,\n        3.31902372e+00,  9.46399611e+00,  2.87005749e+01,  6.86013081e-01,\n        1.45515077e+00,  3.46689925e-01, -6.48229240e-01, -5.61947813e-01,\n        2.78998828e+00, -8.03682461e+00,  9.12770739e+00, -5.75156238e-01,\n       -4.43598381e+00, -7.19008038e-02,  1.26663628e+00,  6.86145562e+00,\n        1.93697961e-01,  9.07254794e-01, -2.72445241e-01,  1.07944844e+01,\n        6.39337734e+00,  1.33017331e-01,  5.62652197e+00, -3.19763361e+00,\n        8.48954009e+00,  1.81307848e+01,  5.39347373e-01, -9.28889636e+00,\n       -5.51078616e-01,  1.02733607e+01,  7.31211334e+00,  3.85022172e-01,\n        4.48516976e-01,  2.22105207e+00, -6.48613734e+00,  4.35275483e+00,\n       -8.76228628e+00,  1.39775752e+01,  1.01309095e+01,  8.73729369e+00,\n        3.97215982e+00,  6.02264572e+00,  2.56328305e+00,  1.01349625e+01,\n        1.19917344e+01, -6.67790460e+00,  8.26548189e-02,  2.74261297e+00,\n        2.85720624e+00,  5.97073093e-01, -8.41997318e+00,  2.41724609e+01,\n        7.21191612e+00,  8.46737936e+00, -9.19178491e-01,  4.70694059e+00,\n        3.64573241e+00,  1.05688983e+01, -3.23810244e+00,  2.02691448e+00,\n       -7.93244530e+00,  5.44136528e+00,  8.64528575e+00,  9.03190285e+00,\n        8.33858429e+00,  2.60432708e+00,  2.31530191e+00,  7.20192325e-01,\n       -4.21964337e-02,  8.18908829e+00,  7.28568702e+00,  8.34612959e+00,\n        1.15142092e+01,  5.76128388e+00, -3.34582334e-01,  2.19462888e+00,\n        2.09085698e-01,  2.82480677e-01,  6.60714368e+00,  6.77216767e+00,\n        7.15360305e+00,  3.96479096e+00, -1.64606847e-01, -4.82386291e+00,\n        3.52695177e+00,  9.71360470e+00,  9.27205676e+00,  9.74753554e+00,\n        8.15734382e+00,  1.54316967e+01,  5.88685109e+00,  4.06011379e+00,\n        1.09673740e+01,  3.15390269e+00,  1.74028967e+00,  5.78397121e+00,\n        7.59080327e+00, -2.44058883e+00,  7.11314551e+00,  1.91227419e+01,\n        5.37158902e+00,  1.25067094e+01,  5.16746027e+00,  1.39687565e+00,\n       -1.07915642e+01,  1.17879854e+01,  3.57076284e+00, -1.76982812e+00,\n        3.71536540e+00, -2.98371938e+00, -2.95070740e+01,  7.56902650e+00,\n        8.34883584e+00,  4.38851979e+00, -2.80794330e+00, -2.98553571e-01,\n       -2.16876572e+00, -7.97058936e-01, -2.66855112e+00,  3.40190650e+01,\n        1.78093237e+01,  7.58489102e-01,  8.41063862e+00,  8.89314259e-01,\n       -3.68213887e+00, -2.34205409e+00,  8.21394882e+00,  1.65366763e-01,\n        2.35475500e+01,  1.17073606e+00, -3.90064380e-01, -4.29804364e+00,\n       -9.21310752e-01,  1.39088594e+01,  2.09249398e+00,  2.63959978e-01,\n        1.10591085e+00, -8.29732626e-01,  1.07572133e+01,  8.17239651e+00,\n       -1.84561323e+00,  1.81579044e+00,  4.39918375e+00, -6.40101573e-01,\n        7.69861706e-01,  3.91485286e+01, -5.07151018e+00,  7.43479396e+00,\n        5.86257661e+00,  3.36854123e+00, -7.76981926e+00,  8.78801234e+00,\n        5.74807320e+00,  3.05998382e+00,  1.05205518e+01, -6.46393362e-01,\n       -1.93027465e+00,  5.24818207e+00,  8.14458904e+00,  6.16366838e+00])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "u_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_train))/(W_train - u_learner_e_x.predict_proba(X_train)[:,1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_tau_hat_learner.fit(X_train,u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:59:34.707701Z",
     "start_time": "2023-06-14T15:59:33.259583Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "30.921286420155806"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((u_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:59:34.710985Z",
     "start_time": "2023-06-14T15:59:34.705359Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Just some lasso tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_poly = LassoCV(cv=10, random_state=0, tol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# degree 3: 10 seconds to fit (100% cpu)\n",
    "# degree 4: 90 seconds to fit (100% cpu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predictions = lasso_poly.predict(X_poly_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((y_predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# just some Neural Network test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(d,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, start_from_epoch=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training Model\")\n",
    "training = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Generate predictions for test samples\")\n",
    "predictions = np.reshape(model.predict(X_test),(300,))\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, Y_test, batch_size=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRY SAME WITH LASSO AND NEURAL NETWORK!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Lasso (or L1-loss for logistic regression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-4.17996000e-01, -2.58928000e-01,  4.39047000e-01, ...,\n         8.77308859e-02, -5.72355474e-02,  3.73404172e-02],\n       [-7.55952000e-01, -2.64981000e-01,  4.66614000e-01, ...,\n        -8.53171876e-02, -4.85438862e-02, -2.76205645e-02],\n       [-1.69104000e-01,  5.08556000e-01, -6.89860000e-01, ...,\n        -4.17789894e-03, -3.62827576e-04, -3.15095821e-05],\n       ...,\n       [ 7.11108000e-01,  2.16507000e-01, -6.45339000e-01, ...,\n         2.42391301e-01,  3.27017404e-01,  4.41189028e-01],\n       [-1.60640000e-01,  6.23831000e-01, -3.81564000e-01, ...,\n         3.10338596e-04, -1.69771130e-04,  9.28735159e-05],\n       [ 4.43016000e-01, -6.36760000e-01,  5.13939000e-01, ...,\n        -1.35528909e+00,  1.44432897e+00, -1.53921860e+00]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_train = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:21:15.932259Z",
     "start_time": "2023-06-14T15:21:15.839747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "poly_train_treatment = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_treatment = poly_train_treatment.fit_transform(X_train_treatment)\n",
    "poly_train_control = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_control = poly_train_treatment.fit_transform(X_train_control)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:21:17.059881Z",
     "start_time": "2023-06-14T15:21:16.458843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_poly_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_poly_train_treatment\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T-learner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# T-Learner (example with Lasso)\n",
    "tic = time.perf_counter()\n",
    "# mu_0\n",
    "t_learner_mu0 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_poly_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_poly_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Time for calculations: {toc-tic}') # took 69 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S-learner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "xw_poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_W_poly_train = poly_train_treatment.fit_transform(X_W_train)\n",
    "\n",
    "xw_poly_test_0 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_0 = xw_poly_test_0.fit_transform(X_test_0)\n",
    "\n",
    "xw_poly_test_1 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_1 = xw_poly_test_0.fit_transform(X_test_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "tic = time.perf_counter()\n",
    "# mu_x\n",
    "s_learner = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "s_learner.fit(X_W_poly_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_poly_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_poly_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "print(f'Time for computation: {toc-tic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## X-learner with lasso (or l1-penalty)\n",
    "### TAKES A LOT OF TIME!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_poly_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_poly_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_0_hat.fit(X_poly_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_1_hat.fit(X_poly_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "g_x_hat.fit(X_poly_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_poly_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_poly_test) + probas_0 * x_tau_1_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic}') # 127 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## R-learner with lasso (or l1-penalty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "r_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_poly_train)\n",
    "r_probas_0 = r_probas[:,0] # probabilities of W=0\n",
    "r_probas_1 = r_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_mu_x.fit(X_poly_train,Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_poly_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_tau.fit(X_poly_train,r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_poly_test)\n",
    "r_tau_hats\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds') # 98 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dr-learner with lasso (l1-penalty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "dr_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_poly_train)\n",
    "dr_probas_0 = dr_probas[:,0] # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_poly_train) + (1 - W_train) * dr_learner_mu_0.predict(X_poly_train) # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(X_poly_train) - dr_learner_mu_0.predict(X_poly_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_poly_train,dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc-tic} seconds') # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ra-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computation: 16.094570000001113 seconds.\n"
     ]
    }
   ],
   "source": [
    "### RA-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "ra_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0.predict(X_poly_train)) + (1 - W_train)*(ra_learner_mu1.predict(X_poly_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_poly_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 121 seconds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:21:39.724010Z",
     "start_time": "2023-06-14T15:21:23.636836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "8.283890654355236"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((ra_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:21:39.871342Z",
     "start_time": "2023-06-14T15:21:39.720109Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PW-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computation: 15.782583167005214 seconds.\n"
     ]
    }
   ],
   "source": [
    "### PW-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "pw_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train/pw_learner_e_x.predict_proba(X_poly_train)[:,1] - (1 - W_train)/(pw_learner_e_x.predict_proba(X_poly_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_poly_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 117 seconds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:38:16.558295Z",
     "start_time": "2023-06-14T15:38:00.740915Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "16.03059004204301"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pw_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:38:16.800209Z",
     "start_time": "2023-06-14T15:38:16.539049Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## U-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computation: 14.17206100000476 seconds.\n"
     ]
    }
   ],
   "source": [
    "### U-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "u_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "u_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_learner_mu_x.fit(X_poly_train,Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_poly_train))/(W_train - u_learner_e_x.predict_proba(X_poly_train)[:,1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_tau_hat_learner.fit(X_poly_train,u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 98 seconds\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:59:57.986044Z",
     "start_time": "2023-06-14T15:59:43.787512Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "7.6762472449663495"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((u_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:59:58.222619Z",
     "start_time": "2023-06-14T15:59:58.000195Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# now do it with neural networks\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model_25 = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_25.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_25.save('model_25')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# same model, but input shape=26, for t-learner only\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model_26 = keras.Sequential([\n",
    "    keras.Input(shape=(26,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_26.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_26.save('model_26')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ex = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network_Classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_ex.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.5),\n",
    "    # List of metrics to monitor\n",
    "    metrics=keras.metrics.BinaryAccuracy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_ex.save('model_ex')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, start_from_epoch=30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ex_sigmoid = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network_Classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_ex_sigmoid.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.5),\n",
    "    # List of metrics to monitor\n",
    "    metrics=keras.metrics.BinaryAccuracy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_ex_sigmoid.save('model_ex_sigmoid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = load_model('model_25')\n",
    "print('Training mu0')\n",
    "t_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = load_model('model_25')\n",
    "print('Training mu1')\n",
    "t_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 3 seconds\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(t_tau_hat,(300,)) - tau_test)**2).mean() # 3.18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S-learner with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = load_model('model_26')\n",
    "s_learner.fit(X_W_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_W_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(s_tau_hat,(300,)) - tau_test)**2).mean() # 1.98"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = load_model('model_25')\n",
    "x_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - np.reshape(x_learner_mu0.predict(X_train_treatment),(len(Y_train_treatment),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = load_model('model_25')\n",
    "x_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# d_0\n",
    "imputed_0 = np.reshape(x_learner_mu1.predict(X_train_control),(len(Y_train_control),)) - Y_train_control\n",
    "\n",
    "\n",
    "# regress imputed on X\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = load_model('model_25')\n",
    "x_tau_1_hat.fit(X_train_treatment,imputed_1,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, tau_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "x_tau_1_hat_predicts = np.reshape(x_tau_1_hat.predict(X_test),(len(X_test),))\n",
    "\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = load_model('model_25')\n",
    "x_tau_0_hat.fit(X_train_control,imputed_0,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, tau_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "x_tau_0_hat_predicts = np.reshape(x_tau_0_hat.predict(X_test),(len(X_test),))\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = load_model('model_ex')\n",
    "g_x_hat.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "x_probabilities = g_x_hat.predict(X_test)\n",
    "x_probs_1 = np.reshape(keras.activations.sigmoid(x_probabilities),(len(x_probabilities,)))\n",
    "x_probs_0 = 1 - x_probs_1\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = x_probs_1 * x_tau_0_hat_predicts + x_probs_0 * x_tau_1_hat_predicts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(x_tau_hat,(300,)) - tau_test)**2).mean() # 3.1614 with smoothing of 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-learner with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = load_model('model_ex')\n",
    "r_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get e_x predictions\n",
    "r_probabilities = np.reshape(keras.activations.sigmoid(r_learner_e_x.predict(X_train)),len(X_train,))\n",
    "r_probas_1 = r_probabilities# probabilities of W=1\n",
    "r_probas_0 = 1 - r_probabilities # probabilities of W=0\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = load_model('model_25')\n",
    "r_learner_mu_x.fit(X_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - np.reshape(r_learner_mu_x.predict(X_train),(len(X_train),))) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = load_model('model_25')\n",
    "r_learner_tau.fit(X_train,r_learner_pseudo_outcomes,\n",
    "    sample_weight=r_learner_weights,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(r_tau_hats,(len(X_test))) - tau_test)**2).mean() #47.81"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's reproducible now!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DR-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = load_model('model_ex')\n",
    "dr_learner_e_x.fit(X_train, W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_probabilities = np.reshape(keras.activations.sigmoid(dr_learner_e_x.predict(X_train)),len(X_train,))\n",
    "dr_probas_0 = 1 - dr_probabilities # probabilities of W=0\n",
    "dr_probas_1 = dr_probabilities # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = load_model('model_25')\n",
    "dr_learner_mu_0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_learner_mu_0_predictions = dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = load_model('model_25')\n",
    "dr_learner_mu_1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_learner_mu_1_predictions = dr_learner_mu_1.predict(X_train)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1_predictions + (1 - W_train) * dr_learner_mu_0_predictions # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1_predictions - dr_learner_mu_0_predictions\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = load_model('model_25')\n",
    "dr_learner_tau_hat.fit(X_train,dr_pseudo_outcomes,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc-tic} seconds') # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(dr_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 8.3514"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RA-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 148.4175 - mean_squared_error: 148.4175 - val_loss: 144.6764 - val_mean_squared_error: 144.6764\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 131.0420 - mean_squared_error: 131.0420 - val_loss: 113.7808 - val_mean_squared_error: 113.7808\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 97.0849 - mean_squared_error: 97.0849 - val_loss: 61.1599 - val_mean_squared_error: 61.1599\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 48.3961 - mean_squared_error: 48.3961 - val_loss: 45.0901 - val_mean_squared_error: 45.0901\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 37.2469 - mean_squared_error: 37.2469 - val_loss: 32.6569 - val_mean_squared_error: 32.6569\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 21.8929 - mean_squared_error: 21.8929 - val_loss: 26.7816 - val_mean_squared_error: 26.7816\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 21.6946 - mean_squared_error: 21.6946 - val_loss: 16.9520 - val_mean_squared_error: 16.9520\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 12.6886 - mean_squared_error: 12.6886 - val_loss: 17.2209 - val_mean_squared_error: 17.2209\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 10.2888 - mean_squared_error: 10.2888 - val_loss: 9.2715 - val_mean_squared_error: 9.2715\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 7.2416 - mean_squared_error: 7.2416 - val_loss: 6.9428 - val_mean_squared_error: 6.9428\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.3782 - mean_squared_error: 5.3782 - val_loss: 6.0840 - val_mean_squared_error: 6.0840\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.6594 - mean_squared_error: 4.6594 - val_loss: 4.4912 - val_mean_squared_error: 4.4912\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.9666 - mean_squared_error: 3.9666 - val_loss: 4.0391 - val_mean_squared_error: 4.0391\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.4591 - mean_squared_error: 3.4591 - val_loss: 3.2269 - val_mean_squared_error: 3.2269\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8674 - mean_squared_error: 2.8674 - val_loss: 3.4916 - val_mean_squared_error: 3.4916\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8558 - mean_squared_error: 2.8558 - val_loss: 3.4912 - val_mean_squared_error: 3.4912\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0180 - mean_squared_error: 3.0180 - val_loss: 4.2427 - val_mean_squared_error: 4.2427\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.0753 - mean_squared_error: 3.0753 - val_loss: 3.3735 - val_mean_squared_error: 3.3735\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.5703 - mean_squared_error: 2.5703 - val_loss: 3.3749 - val_mean_squared_error: 3.3749\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8039 - mean_squared_error: 2.8039 - val_loss: 3.5365 - val_mean_squared_error: 3.5365\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6459 - mean_squared_error: 2.6459 - val_loss: 2.8694 - val_mean_squared_error: 2.8694\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2336 - mean_squared_error: 2.2336 - val_loss: 4.0059 - val_mean_squared_error: 4.0059\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5665 - mean_squared_error: 2.5665 - val_loss: 3.2793 - val_mean_squared_error: 3.2793\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2890 - mean_squared_error: 2.2890 - val_loss: 2.8356 - val_mean_squared_error: 2.8356\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9345 - mean_squared_error: 1.9345 - val_loss: 2.9139 - val_mean_squared_error: 2.9139\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8735 - mean_squared_error: 1.8735 - val_loss: 2.7924 - val_mean_squared_error: 2.7924\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.8332 - mean_squared_error: 1.8332 - val_loss: 2.6671 - val_mean_squared_error: 2.6671\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.8771 - mean_squared_error: 1.8771 - val_loss: 3.0786 - val_mean_squared_error: 3.0786\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9265 - mean_squared_error: 1.9265 - val_loss: 2.6715 - val_mean_squared_error: 2.6715\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.8370 - mean_squared_error: 1.8370 - val_loss: 2.6992 - val_mean_squared_error: 2.6992\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7942 - mean_squared_error: 1.7942 - val_loss: 2.8865 - val_mean_squared_error: 2.8865\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7782 - mean_squared_error: 1.7782 - val_loss: 2.5931 - val_mean_squared_error: 2.5931\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5989 - mean_squared_error: 1.5989 - val_loss: 2.6306 - val_mean_squared_error: 2.6306\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7104 - mean_squared_error: 1.7104 - val_loss: 2.4315 - val_mean_squared_error: 2.4315\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.6130 - mean_squared_error: 1.6130 - val_loss: 3.0416 - val_mean_squared_error: 3.0416\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.5394 - mean_squared_error: 1.5394 - val_loss: 2.5211 - val_mean_squared_error: 2.5211\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5335 - mean_squared_error: 1.5335 - val_loss: 2.5964 - val_mean_squared_error: 2.5964\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4840 - mean_squared_error: 1.4840 - val_loss: 2.4675 - val_mean_squared_error: 2.4675\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4115 - mean_squared_error: 1.4115 - val_loss: 2.6055 - val_mean_squared_error: 2.6055\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3974 - mean_squared_error: 1.3974 - val_loss: 2.4143 - val_mean_squared_error: 2.4143\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3919 - mean_squared_error: 1.3919 - val_loss: 2.9882 - val_mean_squared_error: 2.9882\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8466 - mean_squared_error: 1.8466 - val_loss: 2.8518 - val_mean_squared_error: 2.8518\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5472 - mean_squared_error: 1.5472 - val_loss: 2.4647 - val_mean_squared_error: 2.4647\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.3858 - mean_squared_error: 1.3858 - val_loss: 2.5164 - val_mean_squared_error: 2.5164\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.5459 - mean_squared_error: 1.5459 - val_loss: 2.5938 - val_mean_squared_error: 2.5938\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5758 - mean_squared_error: 1.5758 - val_loss: 2.6665 - val_mean_squared_error: 2.6665\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4703 - mean_squared_error: 1.4703 - val_loss: 2.6646 - val_mean_squared_error: 2.6646\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.6497 - mean_squared_error: 1.6497 - val_loss: 2.5359 - val_mean_squared_error: 2.5359\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5740 - mean_squared_error: 1.5740 - val_loss: 2.5224 - val_mean_squared_error: 2.5224\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3565 - mean_squared_error: 1.3565 - val_loss: 2.4915 - val_mean_squared_error: 2.4915\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2314 - mean_squared_error: 1.2314 - val_loss: 2.5633 - val_mean_squared_error: 2.5633\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1409 - mean_squared_error: 1.1409 - val_loss: 2.4369 - val_mean_squared_error: 2.4369\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.4500 - mean_squared_error: 1.4500 - val_loss: 2.3148 - val_mean_squared_error: 2.3148\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2161 - mean_squared_error: 1.2161 - val_loss: 2.3513 - val_mean_squared_error: 2.3513\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 2.4279 - val_mean_squared_error: 2.4279\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1715 - mean_squared_error: 1.1715 - val_loss: 2.3093 - val_mean_squared_error: 2.3093\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0279 - mean_squared_error: 1.0279 - val_loss: 2.2944 - val_mean_squared_error: 2.2944\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 2.5101 - val_mean_squared_error: 2.5101\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 2.3299 - val_mean_squared_error: 2.3299\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9922 - mean_squared_error: 0.9922 - val_loss: 2.3346 - val_mean_squared_error: 2.3346\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0269 - mean_squared_error: 1.0269 - val_loss: 2.5553 - val_mean_squared_error: 2.5553\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0281 - mean_squared_error: 1.0281 - val_loss: 2.2743 - val_mean_squared_error: 2.2743\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9400 - mean_squared_error: 0.9400 - val_loss: 2.4349 - val_mean_squared_error: 2.4349\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 2.6105 - val_mean_squared_error: 2.6105\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 3.0612 - val_mean_squared_error: 3.0612\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2905 - mean_squared_error: 1.2905 - val_loss: 3.2804 - val_mean_squared_error: 3.2804\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2158 - mean_squared_error: 1.2158 - val_loss: 2.6824 - val_mean_squared_error: 2.6824\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9601 - mean_squared_error: 0.9601 - val_loss: 2.4136 - val_mean_squared_error: 2.4136\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9503 - mean_squared_error: 0.9503 - val_loss: 2.3537 - val_mean_squared_error: 2.3537\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9836 - mean_squared_error: 0.9836 - val_loss: 2.1622 - val_mean_squared_error: 2.1622\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9942 - mean_squared_error: 0.9942 - val_loss: 2.3953 - val_mean_squared_error: 2.3953\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8975 - mean_squared_error: 0.8975 - val_loss: 2.2673 - val_mean_squared_error: 2.2673\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7950 - mean_squared_error: 0.7950 - val_loss: 2.4418 - val_mean_squared_error: 2.4418\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8649 - mean_squared_error: 0.8649 - val_loss: 2.6717 - val_mean_squared_error: 2.6717\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 3.1053 - val_mean_squared_error: 3.1053\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 2.8006 - val_mean_squared_error: 2.8006\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9714 - mean_squared_error: 0.9714 - val_loss: 2.3400 - val_mean_squared_error: 2.3400\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8183 - mean_squared_error: 0.8183 - val_loss: 2.1979 - val_mean_squared_error: 2.1979\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7943 - mean_squared_error: 0.7943 - val_loss: 2.4284 - val_mean_squared_error: 2.4284\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7691 - mean_squared_error: 0.7691 - val_loss: 2.2440 - val_mean_squared_error: 2.2440\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7135 - mean_squared_error: 0.7135 - val_loss: 2.2395 - val_mean_squared_error: 2.2395\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6764 - mean_squared_error: 0.6764 - val_loss: 2.2452 - val_mean_squared_error: 2.2452\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7164 - mean_squared_error: 0.7164 - val_loss: 2.3186 - val_mean_squared_error: 2.3186\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6653 - mean_squared_error: 0.6653 - val_loss: 2.3069 - val_mean_squared_error: 2.3069\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6615 - mean_squared_error: 0.6615 - val_loss: 2.2438 - val_mean_squared_error: 2.2438\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6313 - mean_squared_error: 0.6313 - val_loss: 2.2939 - val_mean_squared_error: 2.2939\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7224 - mean_squared_error: 0.7224 - val_loss: 2.1765 - val_mean_squared_error: 2.1765\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6338 - mean_squared_error: 0.6338 - val_loss: 2.1566 - val_mean_squared_error: 2.1566\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7514 - mean_squared_error: 0.7514 - val_loss: 2.3688 - val_mean_squared_error: 2.3688\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2110 - mean_squared_error: 1.2110 - val_loss: 2.9430 - val_mean_squared_error: 2.9430\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9771 - mean_squared_error: 0.9771 - val_loss: 3.0323 - val_mean_squared_error: 3.0323\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 3.5042 - val_mean_squared_error: 3.5042\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 3.0814 - val_mean_squared_error: 3.0814\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 2.7930 - val_mean_squared_error: 2.7930\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 2.3203 - val_mean_squared_error: 2.3203\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8933 - mean_squared_error: 0.8933 - val_loss: 2.3479 - val_mean_squared_error: 2.3479\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7904 - mean_squared_error: 0.7904 - val_loss: 2.1331 - val_mean_squared_error: 2.1331\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6645 - mean_squared_error: 0.6645 - val_loss: 2.5655 - val_mean_squared_error: 2.5655\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7394 - mean_squared_error: 0.7394 - val_loss: 2.2567 - val_mean_squared_error: 2.2567\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6475 - mean_squared_error: 0.6475 - val_loss: 2.4129 - val_mean_squared_error: 2.4129\n",
      "22/22 [==============================] - 0s 542us/step\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 208.2931 - mean_squared_error: 208.2931 - val_loss: 179.4893 - val_mean_squared_error: 179.4893\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 185.2244 - mean_squared_error: 185.2244 - val_loss: 148.3023 - val_mean_squared_error: 148.3023\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 139.3503 - mean_squared_error: 139.3503 - val_loss: 93.1833 - val_mean_squared_error: 93.1833\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 82.3942 - mean_squared_error: 82.3942 - val_loss: 51.8682 - val_mean_squared_error: 51.8682\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 56.1727 - mean_squared_error: 56.1727 - val_loss: 52.3189 - val_mean_squared_error: 52.3189\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 42.1256 - mean_squared_error: 42.1256 - val_loss: 31.0199 - val_mean_squared_error: 31.0199\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 30.8396 - mean_squared_error: 30.8396 - val_loss: 24.9663 - val_mean_squared_error: 24.9663\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 21.9672 - mean_squared_error: 21.9672 - val_loss: 18.0169 - val_mean_squared_error: 18.0169\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.1779 - mean_squared_error: 15.1779 - val_loss: 12.8020 - val_mean_squared_error: 12.8020\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 11.6017 - mean_squared_error: 11.6017 - val_loss: 8.5981 - val_mean_squared_error: 8.5981\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 10.4382 - mean_squared_error: 10.4382 - val_loss: 10.6197 - val_mean_squared_error: 10.6197\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 9.5200 - mean_squared_error: 9.5200 - val_loss: 8.6449 - val_mean_squared_error: 8.6449\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 7.3366 - mean_squared_error: 7.3366 - val_loss: 7.3904 - val_mean_squared_error: 7.3904\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 6.7110 - mean_squared_error: 6.7110 - val_loss: 8.0893 - val_mean_squared_error: 8.0893\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 6.3200 - mean_squared_error: 6.3200 - val_loss: 6.5635 - val_mean_squared_error: 6.5635\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.6278 - mean_squared_error: 5.6278 - val_loss: 6.0385 - val_mean_squared_error: 6.0385\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.5355 - mean_squared_error: 5.5355 - val_loss: 6.2703 - val_mean_squared_error: 6.2703\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.0134 - mean_squared_error: 5.0134 - val_loss: 5.7908 - val_mean_squared_error: 5.7908\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.9300 - mean_squared_error: 4.9300 - val_loss: 5.8341 - val_mean_squared_error: 5.8341\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.9355 - mean_squared_error: 4.9355 - val_loss: 5.7566 - val_mean_squared_error: 5.7566\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.4358 - mean_squared_error: 4.4358 - val_loss: 5.9341 - val_mean_squared_error: 5.9341\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.5123 - mean_squared_error: 4.5123 - val_loss: 6.1631 - val_mean_squared_error: 6.1631\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.3322 - mean_squared_error: 4.3322 - val_loss: 6.6786 - val_mean_squared_error: 6.6786\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.5507 - mean_squared_error: 4.5507 - val_loss: 6.5094 - val_mean_squared_error: 6.5094\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.5437 - mean_squared_error: 4.5437 - val_loss: 5.9697 - val_mean_squared_error: 5.9697\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.5801 - mean_squared_error: 4.5801 - val_loss: 6.8939 - val_mean_squared_error: 6.8939\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.2859 - mean_squared_error: 4.2859 - val_loss: 5.6054 - val_mean_squared_error: 5.6054\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.6140 - mean_squared_error: 3.6140 - val_loss: 6.0349 - val_mean_squared_error: 6.0349\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.5894 - mean_squared_error: 3.5894 - val_loss: 5.5059 - val_mean_squared_error: 5.5059\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1870 - mean_squared_error: 3.1870 - val_loss: 5.2909 - val_mean_squared_error: 5.2909\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1352 - mean_squared_error: 3.1352 - val_loss: 5.3981 - val_mean_squared_error: 5.3981\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.0562 - mean_squared_error: 3.0562 - val_loss: 5.2934 - val_mean_squared_error: 5.2934\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8315 - mean_squared_error: 2.8315 - val_loss: 5.2297 - val_mean_squared_error: 5.2297\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6685 - mean_squared_error: 2.6685 - val_loss: 5.2791 - val_mean_squared_error: 5.2791\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6620 - mean_squared_error: 2.6620 - val_loss: 5.2518 - val_mean_squared_error: 5.2518\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6765 - mean_squared_error: 2.6765 - val_loss: 5.3192 - val_mean_squared_error: 5.3192\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3408 - mean_squared_error: 2.3408 - val_loss: 5.0896 - val_mean_squared_error: 5.0896\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3918 - mean_squared_error: 2.3918 - val_loss: 5.2524 - val_mean_squared_error: 5.2524\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4061 - mean_squared_error: 2.4061 - val_loss: 5.1047 - val_mean_squared_error: 5.1047\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1809 - mean_squared_error: 2.1809 - val_loss: 5.5236 - val_mean_squared_error: 5.5236\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.5559 - mean_squared_error: 2.5559 - val_loss: 7.7721 - val_mean_squared_error: 7.7721\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1264 - mean_squared_error: 3.1264 - val_loss: 5.2009 - val_mean_squared_error: 5.2009\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3875 - mean_squared_error: 2.3875 - val_loss: 4.9751 - val_mean_squared_error: 4.9751\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1988 - mean_squared_error: 2.1988 - val_loss: 5.0375 - val_mean_squared_error: 5.0375\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1153 - mean_squared_error: 2.1153 - val_loss: 5.1391 - val_mean_squared_error: 5.1391\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2776 - mean_squared_error: 2.2776 - val_loss: 5.3920 - val_mean_squared_error: 5.3920\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2879 - mean_squared_error: 2.2879 - val_loss: 5.0717 - val_mean_squared_error: 5.0717\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2336 - mean_squared_error: 2.2336 - val_loss: 5.3234 - val_mean_squared_error: 5.3234\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2892 - mean_squared_error: 2.2892 - val_loss: 5.1682 - val_mean_squared_error: 5.1682\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1933 - mean_squared_error: 2.1933 - val_loss: 5.0905 - val_mean_squared_error: 5.0905\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9010 - mean_squared_error: 1.9010 - val_loss: 5.3860 - val_mean_squared_error: 5.3860\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0289 - mean_squared_error: 2.0289 - val_loss: 5.2240 - val_mean_squared_error: 5.2240\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.2782 - mean_squared_error: 2.2782 - val_loss: 5.3815 - val_mean_squared_error: 5.3815\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9246 - mean_squared_error: 1.9246 - val_loss: 5.2736 - val_mean_squared_error: 5.2736\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7118 - mean_squared_error: 1.7118 - val_loss: 5.5474 - val_mean_squared_error: 5.5474\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8637 - mean_squared_error: 1.8637 - val_loss: 5.4143 - val_mean_squared_error: 5.4143\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9853 - mean_squared_error: 1.9853 - val_loss: 7.1474 - val_mean_squared_error: 7.1474\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3478 - mean_squared_error: 2.3478 - val_loss: 5.8096 - val_mean_squared_error: 5.8096\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1367 - mean_squared_error: 2.1367 - val_loss: 5.2305 - val_mean_squared_error: 5.2305\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0016 - mean_squared_error: 2.0016 - val_loss: 5.4510 - val_mean_squared_error: 5.4510\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8146 - mean_squared_error: 1.8146 - val_loss: 5.0554 - val_mean_squared_error: 5.0554\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.6102 - mean_squared_error: 1.6102 - val_loss: 4.9123 - val_mean_squared_error: 4.9123\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.4574 - mean_squared_error: 1.4574 - val_loss: 5.1666 - val_mean_squared_error: 5.1666\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.6760 - mean_squared_error: 1.6760 - val_loss: 5.2848 - val_mean_squared_error: 5.2848\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5720 - mean_squared_error: 1.5720 - val_loss: 5.4006 - val_mean_squared_error: 5.4006\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.4829 - mean_squared_error: 1.4829 - val_loss: 5.0454 - val_mean_squared_error: 5.0454\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2920 - mean_squared_error: 1.2920 - val_loss: 5.5053 - val_mean_squared_error: 5.5053\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3499 - mean_squared_error: 1.3499 - val_loss: 5.0389 - val_mean_squared_error: 5.0389\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2343 - mean_squared_error: 1.2343 - val_loss: 5.1697 - val_mean_squared_error: 5.1697\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.3019 - mean_squared_error: 1.3019 - val_loss: 4.9033 - val_mean_squared_error: 4.9033\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1766 - mean_squared_error: 1.1766 - val_loss: 5.0266 - val_mean_squared_error: 5.0266\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.3154 - mean_squared_error: 1.3154 - val_loss: 4.8490 - val_mean_squared_error: 4.8490\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2333 - mean_squared_error: 1.2333 - val_loss: 5.0621 - val_mean_squared_error: 5.0621\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1995 - mean_squared_error: 1.1995 - val_loss: 4.9825 - val_mean_squared_error: 4.9825\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 4.9963 - val_mean_squared_error: 4.9963\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 4.9689 - val_mean_squared_error: 4.9689\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9827 - mean_squared_error: 0.9827 - val_loss: 4.8162 - val_mean_squared_error: 4.8162\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9459 - mean_squared_error: 0.9459 - val_loss: 4.9258 - val_mean_squared_error: 4.9258\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9100 - mean_squared_error: 0.9100 - val_loss: 4.9821 - val_mean_squared_error: 4.9821\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9580 - mean_squared_error: 0.9580 - val_loss: 4.9602 - val_mean_squared_error: 4.9602\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9549 - mean_squared_error: 0.9549 - val_loss: 5.4327 - val_mean_squared_error: 5.4327\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 5.0290 - val_mean_squared_error: 5.0290\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9072 - mean_squared_error: 0.9072 - val_loss: 5.2333 - val_mean_squared_error: 5.2333\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9948 - mean_squared_error: 0.9948 - val_loss: 5.2847 - val_mean_squared_error: 5.2847\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9241 - mean_squared_error: 0.9241 - val_loss: 4.8375 - val_mean_squared_error: 4.8375\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 4.8873 - val_mean_squared_error: 4.8873\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8795 - mean_squared_error: 0.8795 - val_loss: 5.1580 - val_mean_squared_error: 5.1580\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.0241 - mean_squared_error: 1.0241 - val_loss: 5.2768 - val_mean_squared_error: 5.2768\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9255 - mean_squared_error: 0.9255 - val_loss: 5.1734 - val_mean_squared_error: 5.1734\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8570 - mean_squared_error: 0.8570 - val_loss: 4.7450 - val_mean_squared_error: 4.7450\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9005 - mean_squared_error: 0.9005 - val_loss: 5.1348 - val_mean_squared_error: 5.1348\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9000 - mean_squared_error: 0.9000 - val_loss: 5.4504 - val_mean_squared_error: 5.4504\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9119 - mean_squared_error: 0.9119 - val_loss: 4.8887 - val_mean_squared_error: 4.8887\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7952 - mean_squared_error: 0.7952 - val_loss: 5.4142 - val_mean_squared_error: 5.4142\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 5.1206 - val_mean_squared_error: 5.1206\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 5.4048 - val_mean_squared_error: 5.4048\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8146 - mean_squared_error: 0.8146 - val_loss: 5.0666 - val_mean_squared_error: 5.0666\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6550 - mean_squared_error: 0.6550 - val_loss: 5.1919 - val_mean_squared_error: 5.1919\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7034 - mean_squared_error: 0.7034 - val_loss: 4.9118 - val_mean_squared_error: 4.9118\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6711 - mean_squared_error: 0.6711 - val_loss: 4.8974 - val_mean_squared_error: 4.8974\n",
      "22/22 [==============================] - 0s 468us/step\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 26.4510 - mean_squared_error: 26.4510\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12.6023 - mean_squared_error: 12.6023\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0231 - mean_squared_error: 9.0231\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 8.4403 - mean_squared_error: 8.4403\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5511 - mean_squared_error: 7.5511\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7806 - mean_squared_error: 6.7806\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.8787 - mean_squared_error: 5.8787\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 4.9694 - mean_squared_error: 4.9694\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 4.3363 - mean_squared_error: 4.3363\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.9893 - mean_squared_error: 3.9893\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.6005 - mean_squared_error: 3.6005\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.3734 - mean_squared_error: 3.3734\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.1937 - mean_squared_error: 3.1937\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.9486 - mean_squared_error: 2.9486\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8143 - mean_squared_error: 2.8143\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5702 - mean_squared_error: 2.5702\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4895 - mean_squared_error: 2.4895\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3578 - mean_squared_error: 2.3578\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2662 - mean_squared_error: 2.2662\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0765 - mean_squared_error: 2.0765\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0273 - mean_squared_error: 2.0273\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0171 - mean_squared_error: 2.0171\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0282 - mean_squared_error: 2.0282\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.1682 - mean_squared_error: 2.1682\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8230 - mean_squared_error: 1.8230\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6634 - mean_squared_error: 1.6634\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.7135 - mean_squared_error: 1.7135\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6464 - mean_squared_error: 1.6464\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5602 - mean_squared_error: 1.5602\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4028 - mean_squared_error: 1.4028\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2886 - mean_squared_error: 1.2886\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1771 - mean_squared_error: 1.1771\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1914 - mean_squared_error: 1.1914\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.2334 - mean_squared_error: 1.2334\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0782 - mean_squared_error: 1.0782\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.0272 - mean_squared_error: 1.0272\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0085 - mean_squared_error: 1.0085\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0209 - mean_squared_error: 1.0209\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0304 - mean_squared_error: 1.0304\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.9208 - mean_squared_error: 0.9208\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8274 - mean_squared_error: 0.8274\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7698 - mean_squared_error: 0.7698\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7445 - mean_squared_error: 0.7445\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6935 - mean_squared_error: 0.6935\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6741 - mean_squared_error: 0.6741\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6959 - mean_squared_error: 0.6959\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6724 - mean_squared_error: 0.6724\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6146 - mean_squared_error: 0.6146\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5743 - mean_squared_error: 0.5743\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6021 - mean_squared_error: 0.6021\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5460 - mean_squared_error: 0.5460\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5082 - mean_squared_error: 0.5082\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4517 - mean_squared_error: 0.4517\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4336 - mean_squared_error: 0.4336\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4765 - mean_squared_error: 0.4765\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4573 - mean_squared_error: 0.4573\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4740 - mean_squared_error: 0.4740\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5577 - mean_squared_error: 0.5577\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6598 - mean_squared_error: 0.6598\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6378 - mean_squared_error: 0.6378\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5793 - mean_squared_error: 0.5793\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5793 - mean_squared_error: 0.5793\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7002 - mean_squared_error: 0.7002\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5953 - mean_squared_error: 0.5953\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4742 - mean_squared_error: 0.4742\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4028 - mean_squared_error: 0.4028\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4032 - mean_squared_error: 0.4032\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3485 - mean_squared_error: 0.3485\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2654 - mean_squared_error: 0.2654\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2674 - mean_squared_error: 0.2674\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2534 - mean_squared_error: 0.2534\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2501 - mean_squared_error: 0.2501\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2771 - mean_squared_error: 0.2771\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3014 - mean_squared_error: 0.3014\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2943 - mean_squared_error: 0.2943\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2636 - mean_squared_error: 0.2636\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2737 - mean_squared_error: 0.2737\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2622 - mean_squared_error: 0.2622\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2809 - mean_squared_error: 0.2809\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2397 - mean_squared_error: 0.2397\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2435 - mean_squared_error: 0.2435\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2731 - mean_squared_error: 0.2731\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2715 - mean_squared_error: 0.2715\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2652 - mean_squared_error: 0.2652\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2519 - mean_squared_error: 0.2519\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2426\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2227 - mean_squared_error: 0.2227\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1968 - mean_squared_error: 0.1968\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2094 - mean_squared_error: 0.2094\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2274 - mean_squared_error: 0.2274\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2131 - mean_squared_error: 0.2131\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1788 - mean_squared_error: 0.1788\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1910 - mean_squared_error: 0.1910\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1449 - mean_squared_error: 0.1449\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1548 - mean_squared_error: 0.1548\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1630\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1387 - mean_squared_error: 0.1387\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1603 - mean_squared_error: 0.1603\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1880 - mean_squared_error: 0.1880\n",
      "10/10 [==============================] - 0s 509us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 2.1547678e+00],\n       [ 7.0934086e+00],\n       [-1.1933132e+00],\n       [-1.7330331e+00],\n       [ 4.1358261e+00],\n       [ 3.0794033e-01],\n       [ 5.6670022e+00],\n       [-4.5303479e-01],\n       [ 1.3761189e+00],\n       [ 6.5797367e+00],\n       [ 1.5958121e+00],\n       [ 9.8061094e+00],\n       [ 6.1213851e-01],\n       [ 5.4904618e+00],\n       [-5.1979393e-02],\n       [ 8.1233473e+00],\n       [ 4.4370487e-02],\n       [ 8.1692820e+00],\n       [ 1.0963433e+00],\n       [-3.9844456e-01],\n       [ 6.2407166e-01],\n       [-2.8072139e-02],\n       [ 9.1494427e+00],\n       [ 7.8003716e+00],\n       [ 9.3238668e+00],\n       [ 5.3199363e+00],\n       [ 2.6112435e+00],\n       [ 2.7554927e+00],\n       [ 8.6637962e-01],\n       [ 8.1214581e+00],\n       [ 7.6176071e-01],\n       [-1.4713858e+00],\n       [ 1.2543915e-01],\n       [ 1.8318594e+00],\n       [ 8.0083828e+00],\n       [ 2.1588488e+00],\n       [ 8.8536005e+00],\n       [ 4.1732451e-01],\n       [ 1.7001538e+00],\n       [ 1.5187305e+00],\n       [ 1.1656525e-02],\n       [-1.7919815e+00],\n       [ 4.0616718e-01],\n       [ 6.1538820e+00],\n       [ 8.5820093e+00],\n       [ 7.9268570e+00],\n       [ 2.4026179e-01],\n       [-1.1445227e+00],\n       [ 3.5309880e+00],\n       [ 3.2335727e+00],\n       [ 9.7434959e+00],\n       [ 1.7128487e+00],\n       [ 8.6234579e+00],\n       [ 8.3517342e+00],\n       [ 6.2877364e+00],\n       [ 8.8299227e+00],\n       [-2.8014782e-01],\n       [ 1.6408010e+00],\n       [ 2.4736798e+00],\n       [ 4.9728413e+00],\n       [ 7.9680266e+00],\n       [ 6.8936319e+00],\n       [-3.2354996e+00],\n       [-4.0243344e+00],\n       [ 5.7380385e+00],\n       [ 8.0687447e+00],\n       [ 8.4884605e+00],\n       [ 7.1186459e-01],\n       [ 1.0020278e+01],\n       [ 8.8826475e+00],\n       [ 9.4266405e+00],\n       [ 8.6660414e+00],\n       [ 8.5599079e+00],\n       [ 3.9780409e+00],\n       [ 8.6395063e+00],\n       [ 9.8006573e+00],\n       [-1.2815218e-01],\n       [ 1.1127500e-01],\n       [-2.8491595e+00],\n       [ 9.0946407e+00],\n       [ 5.4955597e+00],\n       [ 7.1652169e+00],\n       [ 8.8499317e+00],\n       [ 7.9653964e+00],\n       [-2.2507198e+00],\n       [ 3.9204116e+00],\n       [ 3.2976103e-01],\n       [-4.8352475e+00],\n       [ 8.7798758e+00],\n       [ 7.6202359e+00],\n       [ 8.8705177e+00],\n       [ 7.6779284e+00],\n       [ 8.8660488e+00],\n       [ 4.2326994e+00],\n       [-6.1345339e-01],\n       [-1.8685106e+00],\n       [-6.6291600e-02],\n       [ 5.8289375e+00],\n       [ 3.2496932e+00],\n       [ 6.2839332e+00],\n       [ 1.8507730e+00],\n       [ 1.0800534e+01],\n       [ 5.2476174e-01],\n       [ 8.4739122e+00],\n       [ 8.0780525e+00],\n       [ 8.1213894e+00],\n       [ 1.8382215e-01],\n       [ 5.4557166e+00],\n       [ 1.0473660e+01],\n       [ 1.0765482e+01],\n       [ 1.1344264e+00],\n       [-5.9559977e-01],\n       [ 7.2902713e+00],\n       [-1.1137404e+00],\n       [ 8.7932795e-02],\n       [-1.1143481e+00],\n       [ 6.5850844e+00],\n       [-4.4351819e-01],\n       [ 8.5416756e+00],\n       [-7.0020622e-01],\n       [ 7.2358022e+00],\n       [ 7.3310041e+00],\n       [ 1.1886964e+00],\n       [ 1.0988642e+00],\n       [ 8.3524504e+00],\n       [ 1.0919465e+00],\n       [-1.4760768e+00],\n       [ 7.3661308e+00],\n       [ 1.7328932e+00],\n       [-7.6056039e-01],\n       [-2.3185098e-01],\n       [ 2.4975570e-01],\n       [-1.0259168e+00],\n       [-5.1035678e-01],\n       [ 8.8357515e+00],\n       [ 9.5234241e+00],\n       [-3.5766616e-01],\n       [ 6.9020271e-01],\n       [ 3.6456373e-01],\n       [ 8.8740802e-01],\n       [ 1.0991789e+01],\n       [ 1.9029913e+00],\n       [ 7.4713011e+00],\n       [-4.2005047e-01],\n       [ 1.1232584e+00],\n       [-1.0458287e+00],\n       [-1.4103806e-01],\n       [-1.2465694e+00],\n       [ 5.7967811e+00],\n       [ 2.6016474e+00],\n       [ 8.0833025e+00],\n       [ 3.8404444e-01],\n       [-1.1318722e+00],\n       [ 9.4986229e+00],\n       [ 5.9563303e-01],\n       [ 7.6191025e+00],\n       [-2.6604828e-01],\n       [-1.2294874e+00],\n       [ 1.2762377e+00],\n       [ 8.9712009e+00],\n       [ 6.8169463e-01],\n       [ 2.1093638e+00],\n       [ 8.0491695e+00],\n       [-1.9222867e+00],\n       [ 7.9748864e+00],\n       [ 6.2049251e+00],\n       [ 6.5457523e-01],\n       [-3.6532232e-01],\n       [-9.0967554e-01],\n       [ 7.9583306e+00],\n       [ 9.4644451e+00],\n       [ 3.3346364e-01],\n       [ 1.7160044e+00],\n       [-6.1399305e-01],\n       [ 9.9295940e+00],\n       [ 7.2240591e+00],\n       [-1.1336265e-02],\n       [ 7.8735008e+00],\n       [ 7.7687964e+00],\n       [ 7.9136868e+00],\n       [ 6.0182424e+00],\n       [ 5.5878043e-02],\n       [-6.1284626e-01],\n       [ 8.4589005e+00],\n       [ 7.0988283e+00],\n       [-1.3601151e+00],\n       [-1.1582208e-01],\n       [ 1.0705709e+00],\n       [-1.9364148e+00],\n       [ 1.0092750e+00],\n       [-1.2108128e+00],\n       [ 2.3450251e+00],\n       [ 9.2015743e+00],\n       [ 6.8507957e+00],\n       [ 1.1470511e+00],\n       [-2.8302887e-01],\n       [-4.0858978e-01],\n       [ 7.6731315e+00],\n       [ 1.6834220e+00],\n       [ 3.2472480e-03],\n       [ 3.0443773e+00],\n       [-6.3826591e-01],\n       [ 9.7347832e+00],\n       [ 9.1474171e+00],\n       [ 7.3799272e+00],\n       [-9.9165523e-01],\n       [-1.1218224e+00],\n       [ 1.8868928e+00],\n       [-4.8679534e-01],\n       [ 9.0493584e+00],\n       [ 9.0438814e+00],\n       [ 7.5849309e+00],\n       [ 7.7961226e+00],\n       [ 9.0118074e+00],\n       [ 6.6258121e-01],\n       [ 2.1706893e+00],\n       [ 2.3819611e+00],\n       [-7.9046702e-01],\n       [ 6.4483290e+00],\n       [ 2.6267445e+00],\n       [ 7.1366506e+00],\n       [ 1.4463949e+00],\n       [-8.0248499e-01],\n       [ 6.2294078e+00],\n       [ 1.2119077e+00],\n       [ 6.9181404e+00],\n       [ 1.0937227e+01],\n       [ 8.2597275e+00],\n       [ 9.2369347e+00],\n       [ 4.7565513e+00],\n       [ 7.6405439e+00],\n       [ 9.7443515e-01],\n       [ 6.0417585e+00],\n       [ 8.1020708e+00],\n       [ 3.3321822e-01],\n       [ 9.8899498e+00],\n       [ 8.0226946e+00],\n       [ 8.4978809e+00],\n       [ 7.5528893e+00],\n       [ 7.2585788e+00],\n       [ 1.5266801e+00],\n       [ 1.0861174e+01],\n       [ 4.1214170e+00],\n       [ 6.1640997e+00],\n       [-2.4264238e+00],\n       [ 9.1274891e+00],\n       [-1.1103299e+00],\n       [ 1.4954045e+00],\n       [ 1.0165228e+01],\n       [-2.2899997e-01],\n       [ 9.5904751e+00],\n       [ 5.6061215e+00],\n       [ 9.1966591e+00],\n       [ 2.1273062e+00],\n       [ 7.4828959e-01],\n       [ 1.2012469e+00],\n       [ 2.1460114e+00],\n       [ 7.9243338e-01],\n       [ 2.4958755e-01],\n       [ 8.4510412e+00],\n       [ 7.7809949e+00],\n       [ 4.2631313e-01],\n       [ 9.5888500e+00],\n       [-7.7426171e-01],\n       [-1.3994751e+00],\n       [-1.8809725e+00],\n       [ 9.8507538e+00],\n       [ 4.4860789e-01],\n       [ 5.9844337e+00],\n       [-4.8602298e-01],\n       [ 7.2861748e+00],\n       [ 1.9767511e+00],\n       [ 5.8343673e-01],\n       [ 9.4033232e+00],\n       [ 1.2150571e+01],\n       [ 2.4355333e+00],\n       [ 2.5337817e-02],\n       [-8.6971265e-01],\n       [ 9.1197872e+00],\n       [ 6.0419688e+00],\n       [ 1.9857988e+00],\n       [-1.6629261e+00],\n       [ 3.2192833e+00],\n       [ 1.5774938e+00],\n       [-1.0803450e+00],\n       [ 4.2528701e+00],\n       [-3.3102224e+00],\n       [ 9.5871582e+00],\n       [ 5.0285053e+00],\n       [ 6.9560328e+00],\n       [-2.2565238e+00],\n       [ 9.6754532e+00],\n       [ 1.5090115e+00],\n       [-8.1331834e-02],\n       [ 7.1303039e+00],\n       [ 1.2033770e+00],\n       [ 1.7132187e+00],\n       [-1.6145239e+00],\n       [ 7.8831735e+00],\n       [ 8.0776796e+00]], dtype=float32)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = load_model('model_25')\n",
    "ra_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu0_predictions = np.reshape(ra_learner_mu0.predict(X_train),(len(X_train),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = load_model('model_25')\n",
    "ra_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu1_predictions = np.reshape(ra_learner_mu1.predict(X_train),(len(X_train),))\n",
    "\n",
    "\n",
    "# e_x TODO: IS IT NEEDED?\n",
    "\"\"\"ra_learner_e_x = load_model('model_ex')\n",
    "ra_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0_predictions) + (1 - W_train)*(ra_learner_mu1_predictions - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = load_model('model_25')\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:22:04.789043Z",
     "start_time": "2023-06-14T15:21:58.685845Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "3.3973654461530867"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.reshape(ra_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 3.397"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:22:04.799401Z",
     "start_time": "2023-06-14T15:22:04.755105Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PW-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6941 - binary_accuracy: 0.4971 - val_loss: 0.6991 - val_binary_accuracy: 0.4833\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6898 - binary_accuracy: 0.5000 - val_loss: 0.7003 - val_binary_accuracy: 0.4833\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6887 - binary_accuracy: 0.4986 - val_loss: 0.7013 - val_binary_accuracy: 0.4833\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6875 - binary_accuracy: 0.5071 - val_loss: 0.7057 - val_binary_accuracy: 0.4767\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6842 - binary_accuracy: 0.5029 - val_loss: 0.7039 - val_binary_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6806 - binary_accuracy: 0.5143 - val_loss: 0.7067 - val_binary_accuracy: 0.4800\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6782 - binary_accuracy: 0.5114 - val_loss: 0.7099 - val_binary_accuracy: 0.4733\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6750 - binary_accuracy: 0.5400 - val_loss: 0.7105 - val_binary_accuracy: 0.4667\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6702 - binary_accuracy: 0.5543 - val_loss: 0.7163 - val_binary_accuracy: 0.4733\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6695 - binary_accuracy: 0.5743 - val_loss: 0.7168 - val_binary_accuracy: 0.4733\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6653 - binary_accuracy: 0.5629 - val_loss: 0.7210 - val_binary_accuracy: 0.4433\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6630 - binary_accuracy: 0.5886 - val_loss: 0.7241 - val_binary_accuracy: 0.4467\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6574 - binary_accuracy: 0.6000 - val_loss: 0.7460 - val_binary_accuracy: 0.4767\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6546 - binary_accuracy: 0.6471 - val_loss: 0.7339 - val_binary_accuracy: 0.4933\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6511 - binary_accuracy: 0.6586 - val_loss: 0.7435 - val_binary_accuracy: 0.4633\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6483 - binary_accuracy: 0.6686 - val_loss: 0.7352 - val_binary_accuracy: 0.4633\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6500 - binary_accuracy: 0.6814 - val_loss: 0.7374 - val_binary_accuracy: 0.4733\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6464 - binary_accuracy: 0.6771 - val_loss: 0.7539 - val_binary_accuracy: 0.4567\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6407 - binary_accuracy: 0.7043 - val_loss: 0.7439 - val_binary_accuracy: 0.4867\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6360 - binary_accuracy: 0.7214 - val_loss: 0.7586 - val_binary_accuracy: 0.4700\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6288 - binary_accuracy: 0.7357 - val_loss: 0.7630 - val_binary_accuracy: 0.4700\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6255 - binary_accuracy: 0.7571 - val_loss: 0.7691 - val_binary_accuracy: 0.4667\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6240 - binary_accuracy: 0.7714 - val_loss: 0.7805 - val_binary_accuracy: 0.4533\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6271 - binary_accuracy: 0.7457 - val_loss: 0.7835 - val_binary_accuracy: 0.4367\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6259 - binary_accuracy: 0.7657 - val_loss: 0.7759 - val_binary_accuracy: 0.4700\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6260 - binary_accuracy: 0.7786 - val_loss: 0.7744 - val_binary_accuracy: 0.4700\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6194 - binary_accuracy: 0.7657 - val_loss: 0.7729 - val_binary_accuracy: 0.4600\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6129 - binary_accuracy: 0.7929 - val_loss: 0.7869 - val_binary_accuracy: 0.4700\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6074 - binary_accuracy: 0.8243 - val_loss: 0.7956 - val_binary_accuracy: 0.4333\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6071 - binary_accuracy: 0.8157 - val_loss: 0.7907 - val_binary_accuracy: 0.4667\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6077 - binary_accuracy: 0.8357 - val_loss: 0.7969 - val_binary_accuracy: 0.4700\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6021 - binary_accuracy: 0.8371 - val_loss: 0.8012 - val_binary_accuracy: 0.4367\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6032 - binary_accuracy: 0.8329 - val_loss: 0.7900 - val_binary_accuracy: 0.4867\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6005 - binary_accuracy: 0.8486 - val_loss: 0.8126 - val_binary_accuracy: 0.4567\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5997 - binary_accuracy: 0.8586 - val_loss: 0.7985 - val_binary_accuracy: 0.4633\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5950 - binary_accuracy: 0.8543 - val_loss: 0.8216 - val_binary_accuracy: 0.4633\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5937 - binary_accuracy: 0.8657 - val_loss: 0.8103 - val_binary_accuracy: 0.4767\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5888 - binary_accuracy: 0.8771 - val_loss: 0.8065 - val_binary_accuracy: 0.4567\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5895 - binary_accuracy: 0.8829 - val_loss: 0.8078 - val_binary_accuracy: 0.4600\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5892 - binary_accuracy: 0.9029 - val_loss: 0.8213 - val_binary_accuracy: 0.4567\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5899 - binary_accuracy: 0.9057 - val_loss: 0.8064 - val_binary_accuracy: 0.4733\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5871 - binary_accuracy: 0.8957 - val_loss: 0.8203 - val_binary_accuracy: 0.4567\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5856 - binary_accuracy: 0.9171 - val_loss: 0.8197 - val_binary_accuracy: 0.4867\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5833 - binary_accuracy: 0.9129 - val_loss: 0.8250 - val_binary_accuracy: 0.4433\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5815 - binary_accuracy: 0.9186 - val_loss: 0.8186 - val_binary_accuracy: 0.4533\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5797 - binary_accuracy: 0.9386 - val_loss: 0.8207 - val_binary_accuracy: 0.4667\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5807 - binary_accuracy: 0.9386 - val_loss: 0.8217 - val_binary_accuracy: 0.4867\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5815 - binary_accuracy: 0.9200 - val_loss: 0.8218 - val_binary_accuracy: 0.4533\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5783 - binary_accuracy: 0.9486 - val_loss: 0.8355 - val_binary_accuracy: 0.4833\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5757 - binary_accuracy: 0.9571 - val_loss: 0.8113 - val_binary_accuracy: 0.4667\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5747 - binary_accuracy: 0.9571 - val_loss: 0.8332 - val_binary_accuracy: 0.4700\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5753 - binary_accuracy: 0.9629 - val_loss: 0.8387 - val_binary_accuracy: 0.4533\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5756 - binary_accuracy: 0.9529 - val_loss: 0.8173 - val_binary_accuracy: 0.4567\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5751 - binary_accuracy: 0.9529 - val_loss: 0.8286 - val_binary_accuracy: 0.4733\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5757 - binary_accuracy: 0.9500 - val_loss: 0.8297 - val_binary_accuracy: 0.4333\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5767 - binary_accuracy: 0.9614 - val_loss: 0.8365 - val_binary_accuracy: 0.4800\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5771 - binary_accuracy: 0.9657 - val_loss: 0.8225 - val_binary_accuracy: 0.4533\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5780 - binary_accuracy: 0.9400 - val_loss: 0.8348 - val_binary_accuracy: 0.4900\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5738 - binary_accuracy: 0.9657 - val_loss: 0.8178 - val_binary_accuracy: 0.4500\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5728 - binary_accuracy: 0.9686 - val_loss: 0.8337 - val_binary_accuracy: 0.4500\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5725 - binary_accuracy: 0.9729 - val_loss: 0.8258 - val_binary_accuracy: 0.4633\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5706 - binary_accuracy: 0.9743 - val_loss: 0.8317 - val_binary_accuracy: 0.4600\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5696 - binary_accuracy: 0.9829 - val_loss: 0.8279 - val_binary_accuracy: 0.4567\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5688 - binary_accuracy: 0.9757 - val_loss: 0.8258 - val_binary_accuracy: 0.4767\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5674 - binary_accuracy: 0.9857 - val_loss: 0.8331 - val_binary_accuracy: 0.4533\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5670 - binary_accuracy: 0.9929 - val_loss: 0.8281 - val_binary_accuracy: 0.4767\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5677 - binary_accuracy: 0.9857 - val_loss: 0.8350 - val_binary_accuracy: 0.4700\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5671 - binary_accuracy: 0.9800 - val_loss: 0.8213 - val_binary_accuracy: 0.4733\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5668 - binary_accuracy: 0.9886 - val_loss: 0.8355 - val_binary_accuracy: 0.4567\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5675 - binary_accuracy: 0.9857 - val_loss: 0.8373 - val_binary_accuracy: 0.4867\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5668 - binary_accuracy: 0.9957 - val_loss: 0.8321 - val_binary_accuracy: 0.4733\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5662 - binary_accuracy: 0.9943 - val_loss: 0.8323 - val_binary_accuracy: 0.4967\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5662 - binary_accuracy: 0.9900 - val_loss: 0.8335 - val_binary_accuracy: 0.4567\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5655 - binary_accuracy: 0.9986 - val_loss: 0.8296 - val_binary_accuracy: 0.4800\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5652 - binary_accuracy: 0.9986 - val_loss: 0.8383 - val_binary_accuracy: 0.4567\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5649 - binary_accuracy: 0.9986 - val_loss: 0.8245 - val_binary_accuracy: 0.4767\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5646 - binary_accuracy: 0.9971 - val_loss: 0.8313 - val_binary_accuracy: 0.4633\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5643 - binary_accuracy: 0.9986 - val_loss: 0.8306 - val_binary_accuracy: 0.4800\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5642 - binary_accuracy: 0.9971 - val_loss: 0.8302 - val_binary_accuracy: 0.4600\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 0.9986 - val_loss: 0.8306 - val_binary_accuracy: 0.4700\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8315 - val_binary_accuracy: 0.4633\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4767\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8271 - val_binary_accuracy: 0.4733\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 0.9971 - val_loss: 0.8307 - val_binary_accuracy: 0.4667\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8328 - val_binary_accuracy: 0.4633\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8274 - val_binary_accuracy: 0.4733\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 0.9971 - val_loss: 0.8307 - val_binary_accuracy: 0.4667\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5635 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4733\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5633 - binary_accuracy: 1.0000 - val_loss: 0.8309 - val_binary_accuracy: 0.4733\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4567\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 1.0000 - val_loss: 0.8290 - val_binary_accuracy: 0.4800\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 0.9986 - val_loss: 0.8263 - val_binary_accuracy: 0.4500\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8300 - val_binary_accuracy: 0.4600\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5641 - binary_accuracy: 0.9986 - val_loss: 0.8315 - val_binary_accuracy: 0.4767\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5643 - binary_accuracy: 0.9986 - val_loss: 0.8290 - val_binary_accuracy: 0.4600\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 1.0000 - val_loss: 0.8302 - val_binary_accuracy: 0.4833\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8303 - val_binary_accuracy: 0.4733\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8295 - val_binary_accuracy: 0.4767\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8297 - val_binary_accuracy: 0.4733\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8274 - val_binary_accuracy: 0.4767\n",
      "22/22 [==============================] - 0s 458us/step\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 329.0055 - mean_squared_error: 329.0055\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 324.7202 - mean_squared_error: 324.7202\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 322.7691 - mean_squared_error: 322.7691\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 321.0472 - mean_squared_error: 321.0472\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 319.7467 - mean_squared_error: 319.7467\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 317.3828 - mean_squared_error: 317.3828\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 315.2651 - mean_squared_error: 315.2651\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 311.5569 - mean_squared_error: 311.5569\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 308.5987 - mean_squared_error: 308.5987\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 306.7698 - mean_squared_error: 306.7698\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 303.3466 - mean_squared_error: 303.3466\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 297.2262 - mean_squared_error: 297.2262\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 292.3088 - mean_squared_error: 292.3088\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 287.0016 - mean_squared_error: 287.0016\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 279.9354 - mean_squared_error: 279.9353\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 265.9887 - mean_squared_error: 265.9887\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 264.4864 - mean_squared_error: 264.4864\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 251.3025 - mean_squared_error: 251.3025\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 236.2124 - mean_squared_error: 236.2124\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 233.6914 - mean_squared_error: 233.6914\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 220.0323 - mean_squared_error: 220.0323\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 208.8705 - mean_squared_error: 208.8705\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 209.8312 - mean_squared_error: 209.8312\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 201.6464 - mean_squared_error: 201.6464\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 182.8919 - mean_squared_error: 182.8919\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 180.4285 - mean_squared_error: 180.4285\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 200.0673 - mean_squared_error: 200.0673\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 173.5682 - mean_squared_error: 173.5682\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 159.7644 - mean_squared_error: 159.7644\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 144.6144 - mean_squared_error: 144.6144\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 136.7012 - mean_squared_error: 136.7012\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 134.9524 - mean_squared_error: 134.9524\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 128.3616 - mean_squared_error: 128.3616\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 127.1880 - mean_squared_error: 127.1880\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 118.6640 - mean_squared_error: 118.6640\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 114.2206 - mean_squared_error: 114.2206\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 113.1839 - mean_squared_error: 113.1839\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 93.5658 - mean_squared_error: 93.5658\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 87.6837 - mean_squared_error: 87.6837\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 81.2989 - mean_squared_error: 81.2989\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 76.7334 - mean_squared_error: 76.7334\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 71.3146 - mean_squared_error: 71.3146\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 73.1428 - mean_squared_error: 73.1428\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 72.6916 - mean_squared_error: 72.6916\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 89.2747 - mean_squared_error: 89.2747\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 80.0004 - mean_squared_error: 80.0004\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 73.3824 - mean_squared_error: 73.3824\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 68.7458 - mean_squared_error: 68.7458\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 56.2111 - mean_squared_error: 56.2111\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 51.0195 - mean_squared_error: 51.0195\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 41.8137 - mean_squared_error: 41.8137\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 34.8652 - mean_squared_error: 34.8652\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 36.5441 - mean_squared_error: 36.5441\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 36.8733 - mean_squared_error: 36.8733\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 35.4332 - mean_squared_error: 35.4332\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 34.2489 - mean_squared_error: 34.2489\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 29.5732 - mean_squared_error: 29.5732\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 27.7948 - mean_squared_error: 27.7948\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 28.9232 - mean_squared_error: 28.9232\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 23.3314 - mean_squared_error: 23.3314\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 21.6500 - mean_squared_error: 21.6500\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 21.3821 - mean_squared_error: 21.3821\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 19.3673 - mean_squared_error: 19.3673\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 17.1961 - mean_squared_error: 17.1961\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 15.0184 - mean_squared_error: 15.0184\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 13.6123 - mean_squared_error: 13.6123\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12.2070 - mean_squared_error: 12.2070\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 11.6888 - mean_squared_error: 11.6888\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12.7129 - mean_squared_error: 12.7129\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 13.4356 - mean_squared_error: 13.4356\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 13.5314 - mean_squared_error: 13.5314\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12.2904 - mean_squared_error: 12.2904\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 11.0388 - mean_squared_error: 11.0388\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 11.4661 - mean_squared_error: 11.4661\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3471 - mean_squared_error: 9.3471\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5704 - mean_squared_error: 8.5704\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1577 - mean_squared_error: 8.1577\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7531 - mean_squared_error: 9.7531\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0827 - mean_squared_error: 8.0827\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3731 - mean_squared_error: 7.3731\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1179 - mean_squared_error: 7.1179\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0064 - mean_squared_error: 8.0064\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1560 - mean_squared_error: 6.1560\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1536 - mean_squared_error: 5.1536\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7172 - mean_squared_error: 4.7172\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1577 - mean_squared_error: 4.1577\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0095 - mean_squared_error: 5.0095\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0262 - mean_squared_error: 5.0262\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8886 - mean_squared_error: 3.8886\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1321 - mean_squared_error: 4.1321\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1135 - mean_squared_error: 5.1135\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0468 - mean_squared_error: 6.0468\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5795 - mean_squared_error: 5.5795\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8461 - mean_squared_error: 4.8461\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2080 - mean_squared_error: 4.2080\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3326 - mean_squared_error: 4.3326\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3156 - mean_squared_error: 5.3156\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5945 - mean_squared_error: 6.5945\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3627 - mean_squared_error: 7.3627\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0293 - mean_squared_error: 6.0293\n",
      "10/10 [==============================] - 0s 526us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 3.64949203e+00],\n       [ 2.68186867e-01],\n       [ 5.46157551e+00],\n       [ 1.00083704e+01],\n       [ 8.89912128e+00],\n       [ 1.21294508e+01],\n       [-2.65738220e+01],\n       [-1.24039388e+00],\n       [-1.44171696e+01],\n       [-1.27449636e+01],\n       [ 8.80474949e+00],\n       [ 5.65772200e+00],\n       [ 1.80343723e+00],\n       [ 4.48548927e+01],\n       [-1.65572357e+01],\n       [ 2.13477573e+01],\n       [ 8.48676586e+00],\n       [ 2.41762218e+01],\n       [ 1.37603703e+01],\n       [-1.65333402e+00],\n       [ 3.22607231e+01],\n       [ 1.16547155e+01],\n       [ 4.15380669e+00],\n       [ 7.91893291e+00],\n       [-6.72032022e+00],\n       [-2.16316280e+01],\n       [-1.15251217e+01],\n       [ 1.70729294e+01],\n       [-1.37344446e+01],\n       [ 8.47141361e+00],\n       [-3.37798524e+00],\n       [ 5.92239666e+00],\n       [ 1.19024897e+01],\n       [-3.11693883e+00],\n       [ 8.01198006e+00],\n       [ 3.57970452e+00],\n       [-4.39426756e+00],\n       [ 5.65015221e+00],\n       [ 1.94020557e+00],\n       [-4.52745199e+00],\n       [-2.79133911e+01],\n       [ 1.18859224e+01],\n       [-2.47006953e-01],\n       [ 7.28536940e+00],\n       [ 1.39324636e+01],\n       [-1.68022499e+01],\n       [-1.07619562e+01],\n       [-2.37640076e+01],\n       [ 7.67531693e-01],\n       [-3.48618293e+00],\n       [ 1.14266691e+01],\n       [-2.54684143e+01],\n       [-1.77459991e+00],\n       [-3.16368878e-01],\n       [ 6.44865990e+00],\n       [-3.03788042e+00],\n       [ 1.77646599e+01],\n       [-1.11289206e+01],\n       [ 4.43130684e+00],\n       [-3.07890058e+00],\n       [ 2.28088055e+01],\n       [-6.55608714e-01],\n       [ 1.31687574e+01],\n       [-1.51541777e+01],\n       [ 1.52089605e+01],\n       [ 3.29555631e+00],\n       [ 4.27738571e+00],\n       [-2.03810525e+00],\n       [ 1.83286648e+01],\n       [ 3.23395157e+01],\n       [ 1.35878878e+01],\n       [ 1.89599621e+00],\n       [ 4.88184929e+00],\n       [-1.53157063e+01],\n       [-5.60556507e+00],\n       [-1.68120575e+01],\n       [-4.96499634e+00],\n       [ 5.51365280e+00],\n       [ 2.62912107e+00],\n       [ 4.18356743e+01],\n       [-3.48185310e+01],\n       [-9.34615612e+00],\n       [ 1.02408466e+01],\n       [-1.90869389e+01],\n       [-2.32769156e+00],\n       [-1.22192585e+00],\n       [-1.56660080e+01],\n       [-3.99041200e+00],\n       [ 2.88565445e+01],\n       [ 3.00178337e+01],\n       [ 7.78025007e+00],\n       [ 5.94618130e+00],\n       [ 2.86966057e+01],\n       [ 1.18578997e+01],\n       [-3.81083179e+00],\n       [ 1.24224806e+01],\n       [-1.25170803e+01],\n       [-2.15051003e+01],\n       [-4.54146767e+01],\n       [ 4.69759130e+00],\n       [ 3.43230247e+00],\n       [ 2.81133690e+01],\n       [ 4.72969627e+00],\n       [ 5.61926460e+00],\n       [-5.22806835e+00],\n       [-1.24821508e+00],\n       [-1.40654778e+00],\n       [-1.95176010e+01],\n       [-2.12717476e+01],\n       [ 2.99497390e+00],\n       [ 2.64880276e+00],\n       [-5.85147524e+00],\n       [-3.58096671e+00],\n       [ 9.49123383e-01],\n       [ 1.30111694e+01],\n       [ 3.51920629e+00],\n       [-1.00049257e+00],\n       [ 1.89954891e+01],\n       [ 7.23103189e+00],\n       [-1.12496603e+00],\n       [ 1.66089094e+00],\n       [ 4.30285072e+00],\n       [-3.64992929e+00],\n       [-9.55481434e+00],\n       [ 4.34227991e+00],\n       [-8.27105045e+00],\n       [ 1.25217171e+01],\n       [ 1.05771360e+01],\n       [ 3.39827442e+00],\n       [ 1.40564322e+00],\n       [ 2.89344139e+01],\n       [-2.97338238e+01],\n       [ 8.72809124e+00],\n       [-2.62934852e+00],\n       [ 6.45349932e+00],\n       [-7.00895023e+00],\n       [ 1.65020943e+01],\n       [-5.49234962e+00],\n       [-7.90452862e+00],\n       [-4.97839928e+00],\n       [ 8.41066599e-01],\n       [-2.87787704e+01],\n       [ 1.45542488e+01],\n       [ 6.06399632e+00],\n       [ 1.75779819e+01],\n       [ 4.24104309e+00],\n       [ 1.50038853e+01],\n       [-2.36741772e+01],\n       [ 1.20290709e+01],\n       [ 1.97017555e+01],\n       [ 1.10673475e+01],\n       [-3.04437866e+01],\n       [-1.76245600e-01],\n       [ 1.09100962e+01],\n       [ 1.37165852e+01],\n       [-9.30340767e+00],\n       [ 2.43785799e-01],\n       [-7.46353817e+00],\n       [ 1.09025872e+00],\n       [ 1.76897836e+00],\n       [ 2.58659439e+01],\n       [ 1.06737108e+01],\n       [ 3.64076653e+01],\n       [ 4.61962128e+00],\n       [ 5.71231728e+01],\n       [-1.61218643e+01],\n       [ 1.15473967e+01],\n       [-3.06104779e+00],\n       [-1.36316776e+00],\n       [ 9.63599777e+00],\n       [-1.15820103e+01],\n       [ 1.04929342e+01],\n       [ 1.47072020e+01],\n       [-6.24600172e+00],\n       [-4.83505440e+01],\n       [ 3.97838783e+01],\n       [ 7.51075315e+00],\n       [ 4.61601524e+01],\n       [-1.41274395e+01],\n       [ 1.57739849e+01],\n       [ 6.17874489e+01],\n       [ 1.19402370e+01],\n       [ 3.31120262e+01],\n       [ 1.26669321e+01],\n       [ 4.98765993e+00],\n       [-2.37371979e+01],\n       [ 1.25329924e+01],\n       [-9.36453056e+00],\n       [-4.08285236e+00],\n       [ 3.56003070e+00],\n       [ 3.46200287e-01],\n       [-4.19985914e+00],\n       [-7.04917336e+00],\n       [ 9.33054543e+00],\n       [-6.56816673e+00],\n       [ 3.85743928e+00],\n       [ 2.19923973e+01],\n       [-4.51698399e+00],\n       [ 2.57495079e+01],\n       [ 3.84564757e+00],\n       [ 1.25383072e+01],\n       [-9.60116100e+00],\n       [ 2.10680790e+01],\n       [ 8.18735218e+00],\n       [-1.05165184e+00],\n       [ 1.67089653e+01],\n       [-3.36203194e+00],\n       [-1.51843863e+01],\n       [-2.01899853e+01],\n       [ 2.15344963e+01],\n       [ 2.30177612e+01],\n       [-1.59302428e-02],\n       [ 1.27480030e+00],\n       [-1.02541656e+01],\n       [ 1.64020691e+01],\n       [-1.53485050e+01],\n       [ 1.56737602e+00],\n       [-8.45213032e+00],\n       [ 4.42180157e+00],\n       [-8.95563793e+00],\n       [-2.22266464e+01],\n       [-2.08659935e+01],\n       [-1.45816624e+00],\n       [ 3.39755416e+00],\n       [ 1.11904640e+01],\n       [-5.85642099e+00],\n       [ 3.37979927e+01],\n       [-1.94518173e+00],\n       [ 2.11578083e+01],\n       [-9.23930931e+00],\n       [ 9.68637988e-02],\n       [ 1.04604883e+01],\n       [ 1.81277924e+01],\n       [ 3.72746730e+00],\n       [ 1.83818817e+01],\n       [-1.17638035e+01],\n       [-1.01444712e+01],\n       [ 6.35347366e+00],\n       [-3.99163270e+00],\n       [ 3.85978365e+00],\n       [ 9.74763966e+00],\n       [-2.68676400e+00],\n       [-2.43049164e+01],\n       [ 3.75435982e+01],\n       [ 2.50932455e+00],\n       [ 1.25204229e+00],\n       [-8.78740215e+00],\n       [ 1.15319223e+01],\n       [ 3.83745456e+00],\n       [-4.94441185e+01],\n       [ 2.74865875e+01],\n       [-1.81417656e+01],\n       [ 4.74233246e+00],\n       [ 3.91658497e+00],\n       [-1.52907586e+00],\n       [-6.65871954e+00],\n       [ 1.91387119e+01],\n       [ 8.41459751e+00],\n       [ 1.51246796e+01],\n       [ 1.00114679e+01],\n       [ 8.28812540e-01],\n       [ 1.70258656e+01],\n       [ 1.15108395e+00],\n       [-1.71905060e+01],\n       [-3.98989792e+01],\n       [ 1.24590230e+00],\n       [ 1.14089479e+01],\n       [-8.94513035e+00],\n       [ 2.27451110e+00],\n       [-1.22416568e+00],\n       [-7.04570723e+00],\n       [ 1.65001469e+01],\n       [-4.79075480e+00],\n       [ 4.33843613e+00],\n       [-1.05883491e+00],\n       [ 1.47253904e+01],\n       [ 6.27156794e-01],\n       [ 1.16298180e+01],\n       [ 4.34356880e+01],\n       [ 8.62527771e+01],\n       [-2.01404171e+01],\n       [ 2.08139038e+00],\n       [-2.76646423e+01],\n       [ 1.32574682e+01],\n       [ 4.45682287e+00],\n       [ 5.41953516e+00],\n       [ 1.42794585e+00],\n       [ 2.20896053e+00],\n       [-1.10946822e+00],\n       [-7.23188305e+00],\n       [ 4.16987228e+00],\n       [-1.44884958e+01],\n       [ 1.16637030e+01],\n       [ 5.58893394e+00],\n       [ 1.66438770e+01],\n       [-8.14938024e-02],\n       [ 4.79738283e+00],\n       [-1.26816368e+00],\n       [ 1.03956919e+01],\n       [-2.43643832e+00]], dtype=float32)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PW-Learner\n",
    "# mu_0 TODO: really needed\n",
    "# pw_learner_mu0 = load_model('model_25')\n",
    "\"\"\"pw_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# mu_1 TODO: really needed?\n",
    "#pw_learner_mu1 = load_model('model_25')\n",
    "\"\"\"pw_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = load_model('model_ex')\n",
    "pw_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "pw_probabilities = np.reshape(keras.activations.sigmoid(pw_learner_e_x.predict(X_train)),len(X_train,))\n",
    "pw_probs_1 = pw_probabilities\n",
    "pw_probs_0 = 1 - pw_probabilities\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train / pw_probs_1 - (1 - W_train) / pw_probs_0)*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = load_model('model_25')\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:39:01.449081Z",
     "start_time": "2023-06-14T15:38:57.138051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "271.8425349295992"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.reshape(pw_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 271.842 TODO: CHECK IF IT REALLY IS CORRECT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:39:01.458835Z",
     "start_time": "2023-06-14T15:39:01.444466Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6941 - binary_accuracy: 0.4971 - val_loss: 0.6991 - val_binary_accuracy: 0.4833\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6898 - binary_accuracy: 0.5000 - val_loss: 0.7003 - val_binary_accuracy: 0.4833\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6887 - binary_accuracy: 0.4986 - val_loss: 0.7013 - val_binary_accuracy: 0.4833\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6875 - binary_accuracy: 0.5071 - val_loss: 0.7057 - val_binary_accuracy: 0.4767\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6842 - binary_accuracy: 0.5029 - val_loss: 0.7039 - val_binary_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6806 - binary_accuracy: 0.5143 - val_loss: 0.7067 - val_binary_accuracy: 0.4800\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6782 - binary_accuracy: 0.5114 - val_loss: 0.7099 - val_binary_accuracy: 0.4733\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6750 - binary_accuracy: 0.5400 - val_loss: 0.7105 - val_binary_accuracy: 0.4667\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6702 - binary_accuracy: 0.5543 - val_loss: 0.7163 - val_binary_accuracy: 0.4733\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6695 - binary_accuracy: 0.5743 - val_loss: 0.7168 - val_binary_accuracy: 0.4733\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6653 - binary_accuracy: 0.5629 - val_loss: 0.7210 - val_binary_accuracy: 0.4433\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6630 - binary_accuracy: 0.5886 - val_loss: 0.7241 - val_binary_accuracy: 0.4467\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6574 - binary_accuracy: 0.6000 - val_loss: 0.7460 - val_binary_accuracy: 0.4767\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6546 - binary_accuracy: 0.6471 - val_loss: 0.7339 - val_binary_accuracy: 0.4933\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6511 - binary_accuracy: 0.6586 - val_loss: 0.7435 - val_binary_accuracy: 0.4633\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6483 - binary_accuracy: 0.6686 - val_loss: 0.7352 - val_binary_accuracy: 0.4633\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6500 - binary_accuracy: 0.6814 - val_loss: 0.7374 - val_binary_accuracy: 0.4733\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6464 - binary_accuracy: 0.6771 - val_loss: 0.7539 - val_binary_accuracy: 0.4567\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6407 - binary_accuracy: 0.7043 - val_loss: 0.7439 - val_binary_accuracy: 0.4867\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6360 - binary_accuracy: 0.7214 - val_loss: 0.7586 - val_binary_accuracy: 0.4700\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6288 - binary_accuracy: 0.7357 - val_loss: 0.7630 - val_binary_accuracy: 0.4700\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6255 - binary_accuracy: 0.7571 - val_loss: 0.7691 - val_binary_accuracy: 0.4667\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6240 - binary_accuracy: 0.7714 - val_loss: 0.7805 - val_binary_accuracy: 0.4533\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6271 - binary_accuracy: 0.7457 - val_loss: 0.7835 - val_binary_accuracy: 0.4367\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6259 - binary_accuracy: 0.7657 - val_loss: 0.7759 - val_binary_accuracy: 0.4700\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6260 - binary_accuracy: 0.7786 - val_loss: 0.7744 - val_binary_accuracy: 0.4700\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6194 - binary_accuracy: 0.7657 - val_loss: 0.7729 - val_binary_accuracy: 0.4600\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6129 - binary_accuracy: 0.7929 - val_loss: 0.7869 - val_binary_accuracy: 0.4700\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6074 - binary_accuracy: 0.8243 - val_loss: 0.7956 - val_binary_accuracy: 0.4333\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6071 - binary_accuracy: 0.8157 - val_loss: 0.7907 - val_binary_accuracy: 0.4667\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6077 - binary_accuracy: 0.8357 - val_loss: 0.7969 - val_binary_accuracy: 0.4700\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6021 - binary_accuracy: 0.8371 - val_loss: 0.8012 - val_binary_accuracy: 0.4367\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6032 - binary_accuracy: 0.8329 - val_loss: 0.7900 - val_binary_accuracy: 0.4867\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6005 - binary_accuracy: 0.8486 - val_loss: 0.8126 - val_binary_accuracy: 0.4567\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5997 - binary_accuracy: 0.8586 - val_loss: 0.7985 - val_binary_accuracy: 0.4633\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5950 - binary_accuracy: 0.8543 - val_loss: 0.8216 - val_binary_accuracy: 0.4633\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5937 - binary_accuracy: 0.8657 - val_loss: 0.8103 - val_binary_accuracy: 0.4767\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5888 - binary_accuracy: 0.8771 - val_loss: 0.8065 - val_binary_accuracy: 0.4567\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5895 - binary_accuracy: 0.8829 - val_loss: 0.8078 - val_binary_accuracy: 0.4600\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5892 - binary_accuracy: 0.9029 - val_loss: 0.8213 - val_binary_accuracy: 0.4567\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5899 - binary_accuracy: 0.9057 - val_loss: 0.8064 - val_binary_accuracy: 0.4733\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5871 - binary_accuracy: 0.8957 - val_loss: 0.8203 - val_binary_accuracy: 0.4567\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5856 - binary_accuracy: 0.9171 - val_loss: 0.8197 - val_binary_accuracy: 0.4867\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5833 - binary_accuracy: 0.9129 - val_loss: 0.8250 - val_binary_accuracy: 0.4433\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5815 - binary_accuracy: 0.9186 - val_loss: 0.8186 - val_binary_accuracy: 0.4533\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5797 - binary_accuracy: 0.9386 - val_loss: 0.8207 - val_binary_accuracy: 0.4667\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5807 - binary_accuracy: 0.9386 - val_loss: 0.8217 - val_binary_accuracy: 0.4867\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5815 - binary_accuracy: 0.9200 - val_loss: 0.8218 - val_binary_accuracy: 0.4533\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5783 - binary_accuracy: 0.9486 - val_loss: 0.8355 - val_binary_accuracy: 0.4833\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5757 - binary_accuracy: 0.9571 - val_loss: 0.8113 - val_binary_accuracy: 0.4667\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5747 - binary_accuracy: 0.9571 - val_loss: 0.8332 - val_binary_accuracy: 0.4700\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5753 - binary_accuracy: 0.9629 - val_loss: 0.8387 - val_binary_accuracy: 0.4533\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5756 - binary_accuracy: 0.9529 - val_loss: 0.8173 - val_binary_accuracy: 0.4567\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5751 - binary_accuracy: 0.9529 - val_loss: 0.8286 - val_binary_accuracy: 0.4733\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5757 - binary_accuracy: 0.9500 - val_loss: 0.8297 - val_binary_accuracy: 0.4333\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5767 - binary_accuracy: 0.9614 - val_loss: 0.8365 - val_binary_accuracy: 0.4800\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5771 - binary_accuracy: 0.9657 - val_loss: 0.8225 - val_binary_accuracy: 0.4533\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5780 - binary_accuracy: 0.9400 - val_loss: 0.8348 - val_binary_accuracy: 0.4900\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5738 - binary_accuracy: 0.9657 - val_loss: 0.8178 - val_binary_accuracy: 0.4500\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5728 - binary_accuracy: 0.9686 - val_loss: 0.8337 - val_binary_accuracy: 0.4500\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5725 - binary_accuracy: 0.9729 - val_loss: 0.8258 - val_binary_accuracy: 0.4633\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5706 - binary_accuracy: 0.9743 - val_loss: 0.8317 - val_binary_accuracy: 0.4600\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5696 - binary_accuracy: 0.9829 - val_loss: 0.8279 - val_binary_accuracy: 0.4567\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5688 - binary_accuracy: 0.9757 - val_loss: 0.8258 - val_binary_accuracy: 0.4767\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5674 - binary_accuracy: 0.9857 - val_loss: 0.8331 - val_binary_accuracy: 0.4533\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5670 - binary_accuracy: 0.9929 - val_loss: 0.8281 - val_binary_accuracy: 0.4767\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5677 - binary_accuracy: 0.9857 - val_loss: 0.8350 - val_binary_accuracy: 0.4700\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5671 - binary_accuracy: 0.9800 - val_loss: 0.8213 - val_binary_accuracy: 0.4733\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5668 - binary_accuracy: 0.9886 - val_loss: 0.8355 - val_binary_accuracy: 0.4567\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5675 - binary_accuracy: 0.9857 - val_loss: 0.8373 - val_binary_accuracy: 0.4867\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5668 - binary_accuracy: 0.9957 - val_loss: 0.8321 - val_binary_accuracy: 0.4733\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5662 - binary_accuracy: 0.9943 - val_loss: 0.8323 - val_binary_accuracy: 0.4967\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5662 - binary_accuracy: 0.9900 - val_loss: 0.8335 - val_binary_accuracy: 0.4567\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5655 - binary_accuracy: 0.9986 - val_loss: 0.8296 - val_binary_accuracy: 0.4800\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5652 - binary_accuracy: 0.9986 - val_loss: 0.8383 - val_binary_accuracy: 0.4567\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5649 - binary_accuracy: 0.9986 - val_loss: 0.8245 - val_binary_accuracy: 0.4767\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5646 - binary_accuracy: 0.9971 - val_loss: 0.8313 - val_binary_accuracy: 0.4633\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5643 - binary_accuracy: 0.9986 - val_loss: 0.8306 - val_binary_accuracy: 0.4800\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5642 - binary_accuracy: 0.9971 - val_loss: 0.8302 - val_binary_accuracy: 0.4600\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 0.9986 - val_loss: 0.8306 - val_binary_accuracy: 0.4700\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8315 - val_binary_accuracy: 0.4633\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4767\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8271 - val_binary_accuracy: 0.4733\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 0.9971 - val_loss: 0.8307 - val_binary_accuracy: 0.4667\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8328 - val_binary_accuracy: 0.4633\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 1.0000 - val_loss: 0.8274 - val_binary_accuracy: 0.4733\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 0.9971 - val_loss: 0.8307 - val_binary_accuracy: 0.4667\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5635 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4733\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5633 - binary_accuracy: 1.0000 - val_loss: 0.8309 - val_binary_accuracy: 0.4733\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 1.0000 - val_loss: 0.8298 - val_binary_accuracy: 0.4567\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5636 - binary_accuracy: 1.0000 - val_loss: 0.8290 - val_binary_accuracy: 0.4800\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 0.9986 - val_loss: 0.8263 - val_binary_accuracy: 0.4500\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8300 - val_binary_accuracy: 0.4600\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5641 - binary_accuracy: 0.9986 - val_loss: 0.8315 - val_binary_accuracy: 0.4767\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5643 - binary_accuracy: 0.9986 - val_loss: 0.8290 - val_binary_accuracy: 0.4600\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5640 - binary_accuracy: 1.0000 - val_loss: 0.8302 - val_binary_accuracy: 0.4833\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8303 - val_binary_accuracy: 0.4733\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8295 - val_binary_accuracy: 0.4767\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5639 - binary_accuracy: 1.0000 - val_loss: 0.8297 - val_binary_accuracy: 0.4733\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5638 - binary_accuracy: 1.0000 - val_loss: 0.8274 - val_binary_accuracy: 0.4767\n",
      "22/22 [==============================] - 0s 411us/step\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 171.3580 - mean_squared_error: 171.3580 - val_loss: 142.4064 - val_mean_squared_error: 142.4064\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 108.7834 - mean_squared_error: 108.7833 - val_loss: 58.7053 - val_mean_squared_error: 58.7053\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 55.8461 - mean_squared_error: 55.8461 - val_loss: 42.5260 - val_mean_squared_error: 42.5260\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 35.9594 - mean_squared_error: 35.9594 - val_loss: 30.7706 - val_mean_squared_error: 30.7706\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 23.5035 - mean_squared_error: 23.5035 - val_loss: 20.5276 - val_mean_squared_error: 20.5276\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 16.2068 - mean_squared_error: 16.2068 - val_loss: 14.1922 - val_mean_squared_error: 14.1922\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 13.8702 - mean_squared_error: 13.8702 - val_loss: 12.8006 - val_mean_squared_error: 12.8006\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 13.0012 - mean_squared_error: 13.0012 - val_loss: 13.6316 - val_mean_squared_error: 13.6316\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 13.0786 - mean_squared_error: 13.0786 - val_loss: 14.4608 - val_mean_squared_error: 14.4608\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 11.8643 - mean_squared_error: 11.8643 - val_loss: 13.8920 - val_mean_squared_error: 13.8920\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 12.1140 - mean_squared_error: 12.1140 - val_loss: 13.4129 - val_mean_squared_error: 13.4129\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 11.0917 - mean_squared_error: 11.0917 - val_loss: 12.2531 - val_mean_squared_error: 12.2531\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 11.7153 - mean_squared_error: 11.7153 - val_loss: 13.6174 - val_mean_squared_error: 13.6174\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 11.8504 - mean_squared_error: 11.8504 - val_loss: 14.2932 - val_mean_squared_error: 14.2932\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 11.1220 - mean_squared_error: 11.1220 - val_loss: 12.6959 - val_mean_squared_error: 12.6959\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 10.1933 - mean_squared_error: 10.1933 - val_loss: 12.0716 - val_mean_squared_error: 12.0716\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 10.1701 - mean_squared_error: 10.1701 - val_loss: 12.9442 - val_mean_squared_error: 12.9442\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 10.5609 - mean_squared_error: 10.5609 - val_loss: 12.4551 - val_mean_squared_error: 12.4551\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 10.5914 - mean_squared_error: 10.5914 - val_loss: 12.3910 - val_mean_squared_error: 12.3910\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.7658 - mean_squared_error: 9.7658 - val_loss: 12.5989 - val_mean_squared_error: 12.5989\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.4898 - mean_squared_error: 9.4898 - val_loss: 12.0305 - val_mean_squared_error: 12.0305\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.8996 - mean_squared_error: 8.8996 - val_loss: 12.2652 - val_mean_squared_error: 12.2652\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.0380 - mean_squared_error: 9.0380 - val_loss: 12.0705 - val_mean_squared_error: 12.0705\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.1823 - mean_squared_error: 9.1823 - val_loss: 11.7923 - val_mean_squared_error: 11.7923\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.5706 - mean_squared_error: 8.5706 - val_loss: 12.1946 - val_mean_squared_error: 12.1946\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.5027 - mean_squared_error: 8.5027 - val_loss: 12.3120 - val_mean_squared_error: 12.3120\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.4149 - mean_squared_error: 8.4149 - val_loss: 12.8283 - val_mean_squared_error: 12.8283\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.2603 - mean_squared_error: 8.2603 - val_loss: 12.3098 - val_mean_squared_error: 12.3098\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.8805 - mean_squared_error: 7.8805 - val_loss: 12.0810 - val_mean_squared_error: 12.0810\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.6927 - mean_squared_error: 7.6927 - val_loss: 12.0424 - val_mean_squared_error: 12.0424\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.8555 - mean_squared_error: 7.8555 - val_loss: 12.3245 - val_mean_squared_error: 12.3245\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.1859 - mean_squared_error: 8.1859 - val_loss: 13.3051 - val_mean_squared_error: 13.3051\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 9.3964 - mean_squared_error: 9.3964 - val_loss: 12.4101 - val_mean_squared_error: 12.4101\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.4863 - mean_squared_error: 8.4863 - val_loss: 13.4831 - val_mean_squared_error: 13.4831\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.5448 - mean_squared_error: 8.5448 - val_loss: 13.1053 - val_mean_squared_error: 13.1053\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 8.1554 - mean_squared_error: 8.1554 - val_loss: 12.2007 - val_mean_squared_error: 12.2007\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.7087 - mean_squared_error: 7.7087 - val_loss: 12.1578 - val_mean_squared_error: 12.1578\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.2098 - mean_squared_error: 7.2098 - val_loss: 12.4348 - val_mean_squared_error: 12.4348\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.1445 - mean_squared_error: 7.1445 - val_loss: 11.9093 - val_mean_squared_error: 11.9093\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.3434 - mean_squared_error: 7.3434 - val_loss: 12.8996 - val_mean_squared_error: 12.8996\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.4592 - mean_squared_error: 7.4592 - val_loss: 12.3515 - val_mean_squared_error: 12.3515\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.1126 - mean_squared_error: 7.1126 - val_loss: 12.9118 - val_mean_squared_error: 12.9118\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 7.1043 - mean_squared_error: 7.1043 - val_loss: 13.0416 - val_mean_squared_error: 13.0416\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.7609 - mean_squared_error: 6.7609 - val_loss: 12.2311 - val_mean_squared_error: 12.2311\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.8624 - mean_squared_error: 6.8624 - val_loss: 12.5209 - val_mean_squared_error: 12.5209\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5037 - mean_squared_error: 6.5037 - val_loss: 12.0317 - val_mean_squared_error: 12.0317\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.6414 - mean_squared_error: 6.6414 - val_loss: 12.6416 - val_mean_squared_error: 12.6416\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.8129 - mean_squared_error: 6.8129 - val_loss: 13.2981 - val_mean_squared_error: 13.2981\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5728 - mean_squared_error: 6.5728 - val_loss: 12.5027 - val_mean_squared_error: 12.5027\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.1471 - mean_squared_error: 6.1471 - val_loss: 12.7725 - val_mean_squared_error: 12.7725\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.9121 - mean_squared_error: 5.9121 - val_loss: 12.1300 - val_mean_squared_error: 12.1300\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.9339 - mean_squared_error: 5.9339 - val_loss: 12.5611 - val_mean_squared_error: 12.5611\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.8334 - mean_squared_error: 5.8334 - val_loss: 13.7845 - val_mean_squared_error: 13.7845\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.0741 - mean_squared_error: 6.0741 - val_loss: 13.7314 - val_mean_squared_error: 13.7314\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.9605 - mean_squared_error: 6.9605 - val_loss: 14.8587 - val_mean_squared_error: 14.8587\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.4718 - mean_squared_error: 6.4718 - val_loss: 12.8762 - val_mean_squared_error: 12.8762\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.0003 - mean_squared_error: 6.0003 - val_loss: 13.7649 - val_mean_squared_error: 13.7649\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5364 - mean_squared_error: 6.5364 - val_loss: 14.7746 - val_mean_squared_error: 14.7746\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.5469 - mean_squared_error: 6.5469 - val_loss: 13.1577 - val_mean_squared_error: 13.1577\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.1549 - mean_squared_error: 6.1549 - val_loss: 13.9445 - val_mean_squared_error: 13.9445\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.3274 - mean_squared_error: 6.3274 - val_loss: 15.6794 - val_mean_squared_error: 15.6794\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.9307 - mean_squared_error: 5.9307 - val_loss: 13.1117 - val_mean_squared_error: 13.1117\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2404 - mean_squared_error: 5.2404 - val_loss: 12.7969 - val_mean_squared_error: 12.7969\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.9408 - mean_squared_error: 4.9408 - val_loss: 13.4399 - val_mean_squared_error: 13.4399\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.0429 - mean_squared_error: 5.0429 - val_loss: 13.0694 - val_mean_squared_error: 13.0694\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.9029 - mean_squared_error: 4.9029 - val_loss: 13.7193 - val_mean_squared_error: 13.7193\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8886 - mean_squared_error: 4.8886 - val_loss: 13.5975 - val_mean_squared_error: 13.5975\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.5675 - mean_squared_error: 4.5675 - val_loss: 13.8923 - val_mean_squared_error: 13.8923\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.9385 - mean_squared_error: 4.9385 - val_loss: 14.8723 - val_mean_squared_error: 14.8723\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2170 - mean_squared_error: 5.2170 - val_loss: 14.1928 - val_mean_squared_error: 14.1928\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2346 - mean_squared_error: 5.2346 - val_loss: 14.7617 - val_mean_squared_error: 14.7617\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.1844 - mean_squared_error: 5.1844 - val_loss: 14.5098 - val_mean_squared_error: 14.5098\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.5043 - mean_squared_error: 5.5043 - val_loss: 13.7514 - val_mean_squared_error: 13.7514\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2960 - mean_squared_error: 5.2960 - val_loss: 13.7577 - val_mean_squared_error: 13.7577\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3370 - mean_squared_error: 4.3370 - val_loss: 14.3097 - val_mean_squared_error: 14.3097\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3873 - mean_squared_error: 4.3873 - val_loss: 13.8932 - val_mean_squared_error: 13.8932\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.4656 - mean_squared_error: 4.4656 - val_loss: 14.6920 - val_mean_squared_error: 14.6920\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.5449 - mean_squared_error: 4.5449 - val_loss: 14.2003 - val_mean_squared_error: 14.2003\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.2090 - mean_squared_error: 4.2090 - val_loss: 15.6307 - val_mean_squared_error: 15.6307\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3790 - mean_squared_error: 4.3790 - val_loss: 15.3051 - val_mean_squared_error: 15.3051\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.7058 - mean_squared_error: 4.7058 - val_loss: 14.6357 - val_mean_squared_error: 14.6357\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1718 - mean_squared_error: 4.1718 - val_loss: 14.0769 - val_mean_squared_error: 14.0769\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.9815 - mean_squared_error: 3.9815 - val_loss: 13.8215 - val_mean_squared_error: 13.8215\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6138 - mean_squared_error: 3.6138 - val_loss: 13.7888 - val_mean_squared_error: 13.7888\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7231 - mean_squared_error: 3.7231 - val_loss: 14.6662 - val_mean_squared_error: 14.6662\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6795 - mean_squared_error: 3.6795 - val_loss: 14.4883 - val_mean_squared_error: 14.4883\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6389 - mean_squared_error: 3.6389 - val_loss: 14.3302 - val_mean_squared_error: 14.3302\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6448 - mean_squared_error: 3.6448 - val_loss: 15.1685 - val_mean_squared_error: 15.1685\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7247 - mean_squared_error: 3.7247 - val_loss: 14.5522 - val_mean_squared_error: 14.5522\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.4301 - mean_squared_error: 3.4301 - val_loss: 15.0805 - val_mean_squared_error: 15.0805\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.1276 - mean_squared_error: 3.1276 - val_loss: 14.8121 - val_mean_squared_error: 14.8121\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.0731 - mean_squared_error: 3.0731 - val_loss: 15.3073 - val_mean_squared_error: 15.3073\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.8610 - mean_squared_error: 2.8610 - val_loss: 15.2334 - val_mean_squared_error: 15.2334\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.8821 - mean_squared_error: 2.8821 - val_loss: 16.0852 - val_mean_squared_error: 16.0852\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3202 - mean_squared_error: 3.3202 - val_loss: 16.5846 - val_mean_squared_error: 16.5846\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.5329 - mean_squared_error: 3.5329 - val_loss: 16.2177 - val_mean_squared_error: 16.2177\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.2643 - mean_squared_error: 3.2643 - val_loss: 15.5154 - val_mean_squared_error: 15.5154\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.2325 - mean_squared_error: 3.2325 - val_loss: 17.0684 - val_mean_squared_error: 17.0684\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.0221 - mean_squared_error: 4.0221 - val_loss: 16.7421 - val_mean_squared_error: 16.7421\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6763 - mean_squared_error: 3.6763 - val_loss: 16.6420 - val_mean_squared_error: 16.6420\n",
      "22/22 [==============================] - 0s 462us/step\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 58810.1992 - mean_squared_error: 58810.1992\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 58792.0703 - mean_squared_error: 58792.0703\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58761.5898 - mean_squared_error: 58761.5898\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58696.8398 - mean_squared_error: 58696.8398\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58606.7891 - mean_squared_error: 58606.7891\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58567.1328 - mean_squared_error: 58567.1328\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58451.1758 - mean_squared_error: 58451.1758\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58371.3828 - mean_squared_error: 58371.3828\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 58202.6875 - mean_squared_error: 58202.6875\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 57880.4922 - mean_squared_error: 57880.4844\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 57714.8398 - mean_squared_error: 57714.8398\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 57099.4336 - mean_squared_error: 57099.4336\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 56962.1328 - mean_squared_error: 56962.1367\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 56111.4180 - mean_squared_error: 56111.4180\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 55785.3789 - mean_squared_error: 55785.3789\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 55455.7891 - mean_squared_error: 55455.7891\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 54579.6914 - mean_squared_error: 54579.6914\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 55516.9375 - mean_squared_error: 55516.9375\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 55993.5820 - mean_squared_error: 55993.5820\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 55154.2461 - mean_squared_error: 55154.2461\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 53619.4805 - mean_squared_error: 53619.4805\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 52843.0586 - mean_squared_error: 52843.0586\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 51582.5938 - mean_squared_error: 51582.5938\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 50451.4727 - mean_squared_error: 50451.4727\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 48828.8906 - mean_squared_error: 48828.8906\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 50997.8906 - mean_squared_error: 50997.8906\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 48268.0977 - mean_squared_error: 48268.0977\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 47781.7734 - mean_squared_error: 47781.7734\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 47265.6953 - mean_squared_error: 47265.6953\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 45076.2227 - mean_squared_error: 45076.2227\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 44355.9414 - mean_squared_error: 44355.9414\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 43277.7031 - mean_squared_error: 43277.7031\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 41421.3984 - mean_squared_error: 41421.3984\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 40840.5273 - mean_squared_error: 40840.5273\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 43145.8164 - mean_squared_error: 43145.8164\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 42053.2383 - mean_squared_error: 42053.2383\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 37356.5820 - mean_squared_error: 37356.5820\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 38838.7852 - mean_squared_error: 38838.7852\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 40403.1641 - mean_squared_error: 40403.1641\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 37766.1250 - mean_squared_error: 37766.1250\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 36468.1250 - mean_squared_error: 36468.1250\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 32594.4277 - mean_squared_error: 32594.4277\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 31344.7461 - mean_squared_error: 31344.7461\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 29307.7422 - mean_squared_error: 29307.7422\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 30111.2539 - mean_squared_error: 30111.2539\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 32457.6543 - mean_squared_error: 32457.6543\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 29462.7832 - mean_squared_error: 29462.7832\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 29240.8457 - mean_squared_error: 29240.8457\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 26238.0137 - mean_squared_error: 26238.0176\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 26812.1035 - mean_squared_error: 26812.1035\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 22443.6816 - mean_squared_error: 22443.6816\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 23276.8438 - mean_squared_error: 23276.8418\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 24952.2891 - mean_squared_error: 24952.2891\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 23340.8320 - mean_squared_error: 23340.8320\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 22276.7734 - mean_squared_error: 22276.7734\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 21349.7070 - mean_squared_error: 21349.7070\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 18633.6895 - mean_squared_error: 18633.6895\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 19243.3438 - mean_squared_error: 19243.3438\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 17583.1895 - mean_squared_error: 17583.1895\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 17932.5625 - mean_squared_error: 17932.5625\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 18482.6562 - mean_squared_error: 18482.6562\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 32839.7930 - mean_squared_error: 32839.7930\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 25657.9199 - mean_squared_error: 25657.9199\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 27829.3516 - mean_squared_error: 27829.3516\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 22351.0879 - mean_squared_error: 22351.0879\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 19609.1641 - mean_squared_error: 19609.1641\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 16741.8652 - mean_squared_error: 16741.8652\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 14606.6729 - mean_squared_error: 14606.6729\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 13503.6289 - mean_squared_error: 13503.6289\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 11786.0254 - mean_squared_error: 11786.0254\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12995.4912 - mean_squared_error: 12995.4912\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 10913.4102 - mean_squared_error: 10913.4102\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12529.3232 - mean_squared_error: 12529.3232\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9455.4268 - mean_squared_error: 9455.4268\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 12541.2305 - mean_squared_error: 12541.2305\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 15559.9883 - mean_squared_error: 15559.9883\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 19933.2598 - mean_squared_error: 19933.2617\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 25125.4121 - mean_squared_error: 25125.4121\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 16768.1094 - mean_squared_error: 16768.1094\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 19490.6152 - mean_squared_error: 19490.6152\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 14545.5625 - mean_squared_error: 14545.5625\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12996.1885 - mean_squared_error: 12996.1885\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 10969.3926 - mean_squared_error: 10969.3926\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8213.5918 - mean_squared_error: 8213.5918\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7920.1606 - mean_squared_error: 7920.1606\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6992.3735 - mean_squared_error: 6992.3745\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6433.8257 - mean_squared_error: 6433.8257\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6219.6714 - mean_squared_error: 6219.6714\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6200.5645 - mean_squared_error: 6200.5645\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5539.4116 - mean_squared_error: 5539.4116\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5052.9668 - mean_squared_error: 5052.9668\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4935.3960 - mean_squared_error: 4935.3960\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4758.8452 - mean_squared_error: 4758.8452\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4605.8691 - mean_squared_error: 4605.8691\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4595.6265 - mean_squared_error: 4595.6265\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4584.4395 - mean_squared_error: 4584.4395\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4602.6162 - mean_squared_error: 4602.6162\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4637.3682 - mean_squared_error: 4637.3682\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4332.1260 - mean_squared_error: 4332.1260\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4346.3857 - mean_squared_error: 4346.3857\n",
      "10/10 [==============================] - 0s 532us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 4.08133793e+00],\n       [-9.51504529e-01],\n       [-1.79076290e+00],\n       [-7.63607311e+00],\n       [-3.14084358e+01],\n       [-2.24210358e+00],\n       [-1.06579027e+01],\n       [-1.28826427e+00],\n       [-3.51480579e+00],\n       [ 2.12843063e+02],\n       [ 9.24310744e-01],\n       [-1.22622738e+01],\n       [-1.18085003e+00],\n       [-3.41351986e+00],\n       [-2.83067417e+00],\n       [-5.83536301e+01],\n       [-2.83002377e+00],\n       [ 6.36574173e+01],\n       [-1.04902945e+01],\n       [ 1.65513083e-01],\n       [-3.58274841e+00],\n       [-4.25617158e-01],\n       [-9.52702141e+00],\n       [-6.10580730e+00],\n       [-4.23984945e-01],\n       [-3.26232338e+01],\n       [-9.23550701e+00],\n       [ 6.32225096e-01],\n       [ 6.16047442e-01],\n       [-5.90938873e+01],\n       [-1.61812449e+00],\n       [-2.58650589e+00],\n       [ 1.12865295e+01],\n       [-9.62632847e+00],\n       [ 2.36428680e+01],\n       [-5.57943344e+00],\n       [-1.28381548e+01],\n       [-4.89553356e+00],\n       [-3.14357901e+00],\n       [-5.05969143e+00],\n       [ 3.79705071e+00],\n       [ 4.59805775e+00],\n       [-1.24487686e+00],\n       [-6.28578329e+00],\n       [-3.91657448e+00],\n       [-1.51664319e+03],\n       [ 7.14582026e-01],\n       [-4.49104881e+00],\n       [ 1.38287589e-01],\n       [-6.49360561e+00],\n       [-3.31417537e+00],\n       [ 2.25748730e+00],\n       [-3.40840530e+00],\n       [-6.79629211e+01],\n       [-3.00573521e+01],\n       [ 7.54707277e-01],\n       [-1.62171796e-01],\n       [ 9.32852268e+00],\n       [ 3.07627010e+00],\n       [-5.69021082e+00],\n       [-3.99183311e+01],\n       [-7.21476364e+00],\n       [ 2.48345089e+00],\n       [-6.85051250e+00],\n       [ 1.30175629e+02],\n       [-4.30661545e+01],\n       [-2.46770048e+00],\n       [-7.13101959e+00],\n       [-3.57374763e+01],\n       [-8.13490143e+01],\n       [-2.77591457e+01],\n       [-1.07702732e+00],\n       [-2.37819538e+01],\n       [ 1.79084030e+02],\n       [-1.23634995e+02],\n       [ 2.05266070e+00],\n       [-1.19485188e+00],\n       [ 2.88303709e+00],\n       [-3.12175751e+00],\n       [-3.19367313e+01],\n       [ 2.25645615e+02],\n       [ 2.27740356e+02],\n       [-4.62018681e+00],\n       [-1.60129917e+00],\n       [-6.64989758e+00],\n       [ 2.00073764e-01],\n       [ 1.85075092e+00],\n       [-5.08462763e+00],\n       [-1.78156605e+01],\n       [-1.54329163e+03],\n       [-1.72646141e+00],\n       [-4.04729795e+00],\n       [-1.65420742e+01],\n       [-2.72150574e+01],\n       [-3.46968746e+00],\n       [-4.36810963e-02],\n       [-6.47667980e+00],\n       [ 6.05041618e+01],\n       [ 6.74108887e+00],\n       [ 2.91921158e+01],\n       [ 5.79641879e-01],\n       [ 1.68278170e+00],\n       [ 2.20406866e+00],\n       [-2.80661106e+01],\n       [-1.29260727e+02],\n       [-4.96642342e+01],\n       [-1.01650932e+02],\n       [-8.75268841e+00],\n       [-7.38318145e-01],\n       [ 4.52210045e+00],\n       [-2.95197868e+00],\n       [-8.60794485e-01],\n       [-1.20741854e+01],\n       [-3.37635219e-01],\n       [ 4.11893272e+00],\n       [-3.05239964e+00],\n       [-3.01776154e+02],\n       [-2.34563446e+00],\n       [-1.60166046e+02],\n       [ 2.91436958e+00],\n       [ 4.47722740e+01],\n       [ 3.61651268e+01],\n       [ 3.13437521e-01],\n       [-3.79247189e+00],\n       [-2.56385555e+01],\n       [-2.07910872e+00],\n       [-1.55274391e+00],\n       [ 8.10547714e+01],\n       [-8.92992783e+00],\n       [-5.03591728e+00],\n       [-8.80062521e-01],\n       [-2.22363949e+00],\n       [-2.24087620e+00],\n       [-7.68681943e-01],\n       [ 1.83483170e+02],\n       [-5.71687317e+00],\n       [-9.57694948e-01],\n       [-5.73420143e+00],\n       [-2.08754532e+02],\n       [-1.06476183e+01],\n       [-2.72843099e+00],\n       [ 2.43864536e+00],\n       [ 5.34789753e+00],\n       [ 1.02928066e+00],\n       [-3.29680347e+00],\n       [-5.35017300e+00],\n       [ 9.13152754e-01],\n       [-2.40317535e+00],\n       [ 2.97034907e+00],\n       [-9.44918573e-01],\n       [ 1.16590004e+01],\n       [ 3.87364769e+00],\n       [-6.78022003e+00],\n       [-6.79508972e+01],\n       [-3.99888659e+00],\n       [-2.66358013e+01],\n       [-9.91384029e+00],\n       [-3.27114296e+00],\n       [-6.57486916e+00],\n       [-3.46982384e+01],\n       [ 4.50600100e+00],\n       [-5.28222179e+00],\n       [ 8.89915049e-01],\n       [ 1.29674625e+00],\n       [-3.44654877e+02],\n       [-3.80550446e+02],\n       [-9.06270599e+00],\n       [-1.27406378e+01],\n       [-8.05366993e+00],\n       [ 6.33374977e+01],\n       [-6.19507217e+00],\n       [ 4.52579880e+00],\n       [ 2.09753796e-01],\n       [-1.97065639e+00],\n       [ 5.13048267e+00],\n       [ 3.67536060e+03],\n       [-7.28202772e+00],\n       [-2.20895743e+00],\n       [-2.92253357e+02],\n       [ 2.87100773e+01],\n       [-6.42539062e+02],\n       [ 3.29142869e-01],\n       [-5.20618153e+00],\n       [-3.13245821e+00],\n       [-2.60106487e+01],\n       [ 3.26503754e+00],\n       [ 2.18832684e+00],\n       [-5.05985641e+00],\n       [-8.67913437e+00],\n       [-2.59767723e+00],\n       [-9.22307110e+00],\n       [ 4.28878021e+00],\n       [-6.19603920e+01],\n       [-1.35833178e+01],\n       [-3.72567225e+00],\n       [-2.00380182e+00],\n       [ 7.77724743e+00],\n       [-2.16206913e+01],\n       [-9.57884312e+00],\n       [-4.62078512e-01],\n       [-1.08173399e+01],\n       [-6.28382778e+00],\n       [-9.41453018e+01],\n       [-2.90155983e+01],\n       [-5.61621437e+01],\n       [-5.20971012e+00],\n       [-1.20483971e+00],\n       [-9.61836243e+00],\n       [-1.90575790e+00],\n       [-4.43779659e+00],\n       [-3.40085196e+00],\n       [-2.13183651e+01],\n       [ 3.91426735e+01],\n       [ 2.65760481e-01],\n       [-2.29075575e+00],\n       [ 8.23164508e-02],\n       [-4.98302746e+00],\n       [-3.75168276e+00],\n       [-9.93600750e+00],\n       [-1.00609407e+01],\n       [-1.68250580e+02],\n       [ 8.18276501e+00],\n       [-3.80602455e+00],\n       [ 1.41590897e+02],\n       [-6.20567417e+00],\n       [-3.58816109e+01],\n       [-1.26123190e+01],\n       [-3.96845131e+01],\n       [-4.66049385e+00],\n       [-7.94260406e+01],\n       [-8.50484314e+01],\n       [-5.88622510e-01],\n       [ 4.94838953e+00],\n       [-2.04939890e+00],\n       [-1.20928526e+00],\n       [-1.97942257e+00],\n       [ 1.09660201e-01],\n       [ 5.25259399e+00],\n       [ 1.42478728e+00],\n       [-1.07277498e+01],\n       [ 1.23457007e-01],\n       [-1.13903755e+02],\n       [-6.61754370e+00],\n       [ 2.60002251e+01],\n       [-2.20380688e+00],\n       [-7.25222766e-01],\n       [ 6.81360781e-01],\n       [-8.62257671e+00],\n       [ 9.03905213e-01],\n       [ 5.24903250e+00],\n       [ 1.84311523e+02],\n       [-9.82216309e+02],\n       [ 1.55573118e+00],\n       [-7.57122993e+00],\n       [ 1.91428852e+00],\n       [ 3.05069923e+00],\n       [-1.27614775e+01],\n       [-2.20179081e+00],\n       [-7.16749859e+00],\n       [ 2.23098993e+00],\n       [-9.48224354e+00],\n       [-8.31651166e-02],\n       [-4.72178698e+00],\n       [-5.94902933e-01],\n       [-7.31513441e-01],\n       [-2.35193777e+00],\n       [-2.31920662e+01],\n       [-8.23312187e+00],\n       [ 2.63954067e+00],\n       [-6.98857021e+00],\n       [-2.34323692e+01],\n       [-7.82771778e+00],\n       [-4.18579042e-01],\n       [ 7.63220581e+02],\n       [-1.09738159e+01],\n       [-3.48715246e-01],\n       [-4.37539148e+00],\n       [-5.28500271e+00],\n       [-1.57202959e+01],\n       [-4.46790619e+02],\n       [ 2.36423492e+00],\n       [-1.42105198e+00],\n       [-6.46274185e+00],\n       [ 3.50351095e+00],\n       [-3.29801083e+00],\n       [-8.11700012e+02],\n       [-4.15342522e+00],\n       [-4.79864540e+01],\n       [-8.80124760e+00],\n       [-6.31337585e+02],\n       [-6.96119499e+00],\n       [-2.78992538e+01],\n       [ 5.10059416e-01],\n       [-6.07079446e-01],\n       [-4.61957550e+00],\n       [ 1.99504301e-01],\n       [ 1.75421715e+00],\n       [-1.11073442e-01],\n       [-8.07616329e+00],\n       [-7.60106266e-01]], dtype=float32)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = load_model('model_ex')\n",
    "u_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "u_probs_1 = np.reshape(u_learner_e_x.predict(X_train),(len(X_train),))\n",
    "u_probs_0 = 1 - u_probs_1\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = load_model('model_25')\n",
    "u_learner_mu_x.fit(X_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test,Y_test),\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "u_learner_mu_x_predictions = np.reshape(u_learner_mu_x.predict(X_train),(len(X_train),))\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x_predictions)/(W_train - u_probs_1)\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = load_model('model_25')\n",
    "u_tau_hat_learner.fit(X_train,u_learner_residuals,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T16:11:31.651920Z",
     "start_time": "2023-06-14T16:11:25.130770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "74893.80292670502"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.reshape(u_tau_hats,(len(tau_test),)) - tau_test)**2).mean() # 74893.80 TODO: CHECK IF IT REALLY IS CORRECT!!!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T16:08:57.719623Z",
     "start_time": "2023-06-14T16:08:57.649293Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### STABLE ESTIMATOR? SINCE EXTREME VALUES FOR ESTIMATED PROBABILITIES!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Metalearner():\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class T Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):# TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1])\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w==0], y[w==0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w==1], y[w==1])\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0],\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           validation_data=(X_test_control, Y_test_control),\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1],\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           validation_data=(X_test_treatment, Y_test_treatment),\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x): # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x)\n",
    "            mu1_hats = self.mu1_model.predict(x)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x_poly_test)\n",
    "            mu1_hats = self.mu1_model.predict(x_poly_test)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            mu0_hats = self.mu0_model.predict(x, verbose=0)\n",
    "            mu1_hats = self.mu1_model.predict(x, verbose=0)\n",
    "            predictions = np.reshape(mu1_hats-mu0_hats,(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_nn = TLearner(method='lasso')\n",
    "t_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = t_nn.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 10.091322530886687\n",
    "# lasso: 5.583461099392904\n",
    "# nn: 3.1867804239471273"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class S Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_26')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w): # TODO: training process\n",
    "        x_w = np.concatenate((x,np.reshape(w,(len(w),1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting random forest for mu_x\")\n",
    "            self.mux_model.fit(x_w, y)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x_w)\n",
    "\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting lasso for mu_x\")\n",
    "            self.mux_model.fit(x_poly_train, y)\n",
    "\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_x\n",
    "            print(\"Training neural network for mu_x\")\n",
    "            self.mux_model.fit(x_w, y,\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x, w): # TODO:\n",
    "        x_0 = np.concatenate((x,np.zeros((len(x),1))), axis=1)\n",
    "        x_1 = np.concatenate((x,np.ones((len(x),1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_0)\n",
    "            mu1_hats = self.mux_model.predict(x_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_0 = self.poly.fit_transform(x_0)\n",
    "            x_poly_1 = self.poly.fit_transform(x_1)\n",
    "\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_poly_0)\n",
    "            mu1_hats = self.mux_model.predict(x_poly_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_0, verbose=0)\n",
    "            mu1_hats = self.mux_model.predict(x_1, verbose=0)\n",
    "            predictions = np.reshape(mu1_hats-mu0_hats,(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_nn = SLearner('lasso')\n",
    "s_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = s_nn.predict(X_test, W_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 18.134009488483855\n",
    "# lasso: 5.559126710289806\n",
    "# nn: 1.987529792077956"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class X Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class XLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.tau1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau0_model = load_model('model_25')\n",
    "            self.tau1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):# TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0])\n",
    "            imputed_1 = y[w==1] - self.mu0_model.predict(x[w==1])\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1])\n",
    "            imputed_0 = self.mu1_model.predict(x[w==0]) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w==0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w==1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x,w)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w==0], y[w==0])\n",
    "            imputed_1 = y[w==1] - self.mu0_model.predict(x_poly_train[w==1])\n",
    "\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w==1], y[w==1])\n",
    "            imputed_0 = self.mu1_model.predict(x_poly_train[w==0]) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x_poly_train[w==0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x_poly_train[w==1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None, # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_1 = y[w==1] - np.reshape(self.mu0_model.predict(x[w==1], verbose=0),(len(x[w==1]),))\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None, # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_0 = np.reshape(self.mu1_model.predict(x[w==0], verbose=0),(len(x[w==0]),)) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w==0], imputed_0,\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None, # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w==1], imputed_1,\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None, # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None, # include early stopping\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x): # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x)\n",
    "            tau_1_hats = self.tau1_model.predict(x)\n",
    "            # 2: probabilities\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            # 3: final predictions\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x_poly_test)\n",
    "            tau_1_hats = self.tau1_model.predict(x_poly_test)\n",
    "            probs = self.ex_model.predict_proba(x_poly_test)[:,1]\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = np.reshape(self.tau0_model.predict(x, verbose=0),(len(x),))\n",
    "            tau_1_hats = np.reshape(self.tau1_model.predict(x, verbose=0),(len(x),))\n",
    "            # 2: probabilities\n",
    "            logit = self.ex_model.predict(x, verbose=0)\n",
    "            probs = np.reshape(keras.activations.sigmoid(logit),(len(logit,)))\n",
    "            # 3: final predictions\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        predictions = probs * tau_0_hats + (1 - probs) * tau_1_hats\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_rf = XLearner('nn')\n",
    "x_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = x_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: # 3.1369636408859614 --> same (good)\n",
    "# lasso: # nn: 7.667219448077926 --> same (good)\n",
    "# nn: 3.161416602361538 --> same (good)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RLearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            self.mux_model.fit(x,y)\n",
    "\n",
    "            print('Fitting random forest for e_x')\n",
    "            # 2: fit ex\n",
    "            self.ex_model.fit(x,w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x)) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            print('Fitting random forest for tau_x')\n",
    "            # 4: fit tau\n",
    "            self.tau_model.fit(x,pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting lasso for mu_x')\n",
    "            self.mux_model.fit(x_poly_train,y)\n",
    "\n",
    "            # 2: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x_poly_train)) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Training NN for mu_x')\n",
    "            self.mux_model.fit(x,y,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "            # 2: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "            pseudo_outcomes = (y - np.reshape(self.mux_model.predict(x, verbose=0),(len(x),))) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                               sample_weight=weights,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_rf = RLearner('nn')\n",
    "r_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = r_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 17.722925118749608\n",
    "# lasso: 5.50038865455844\n",
    "# nn: 47.81939839016621"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DRLearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting random forest for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting random forest for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x,w)\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            neg_prob = self.ex_model.predict_proba(x)[:,0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x) + (1 - w) * self.mu0_model.predict(x)\n",
    "            pseudo_outcomes = (w - probs) / (probs * neg_prob) * (y - mu_w) + self.mu1_model.predict(x) - self.mu0_model.predict(x)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting lasso for mu_0')\n",
    "            self.mu0_model.fit(x_poly_train[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting lasso for mu_1')\n",
    "            self.mu1_model.fit(x_poly_train[w==1],y[w==1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x_poly_train) + (1 - w) * self.mu0_model.predict(x_poly_train)\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + self.mu1_model.predict(x_poly_train) - self.mu0_model.predict(x_poly_train)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Training NN for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Training NN for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x, verbose=0) + (1 - w) * self.mu0_model.predict(x, verbose=0)\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + self.mu1_model.predict(x, verbose=0) - self.mu0_model.predict(x, verbose=0)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                                   batch_size=100,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=None,\n",
    "                                   callbacks=None,\n",
    "                                   verbose=0\n",
    "                                   )\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dr_rf = DRLearner('rf')\n",
    "dr_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = dr_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 5.385491721300538 # why different??? ---> because if you take 1 - probs its not exactly the same as taking the [:,0] column!!\n",
    "# lasso: 6.252082321980517\n",
    "# nn: 8.35142898943478"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHECK THIS: CHANGE (1 - PROBS) TO [:,0] TO BE MORE EXACT!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class RALearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting random forest for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting random forest for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1])\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = w*(y - self.mu0_model.predict(x)) + (1 - w)*(self.mu1_model.predict(x) - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting lasso for mu_0')\n",
    "            self.mu0_model.fit(x_poly_train[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting lasso for mu_1')\n",
    "            self.mu1_model.fit(x_poly_train[w==1],y[w==1])\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = w*(y - self.mu0_model.predict(x_poly_train)) + (1 - w)*(self.mu1_model.predict(x_poly_train) - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Training NN for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Training NN for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu0_predictions = np.reshape(ra_learner_mu0.predict(x, verbose=0),(len(x),))\n",
    "            mu1_predictions = np.reshape(ra_learner_mu1.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "            pseudo_outcomes = w*(y - mu0_predictions) + (1 - w)*(mu1_predictions - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                                   batch_size=100,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=None,\n",
    "                                   callbacks=None,\n",
    "                                   verbose=0\n",
    "                                   )\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:18:20.753964Z",
     "start_time": "2023-06-14T15:18:20.627611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN for mu_0\n",
      "Training NN for mu_1\n",
      "Training NN for tau_x\n"
     ]
    },
    {
     "data": {
      "text/plain": "3.3973654461530867"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra_rf = RALearner('nn')\n",
    "ra_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = ra_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 5.355494017645751\n",
    "# lasso: 8.283890654355236\n",
    "# nn: 3.3973654461530867"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:19:59.472768Z",
     "start_time": "2023-06-14T15:19:55.193708Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class PWLearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x,w)\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            counter_probs = self.ex_model.predict_proba(x)[:,0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = ( w/probs - (1-w)/counter_probs )*y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "            counter_probs = self.ex_model.predict_proba(x_poly_train)[:,0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = ( w/probs - (1-w)/counter_probs )*y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "            counter_probs = 1 - probs\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = ( w/probs - (1-w)/counter_probs )*y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                                   batch_size=100,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=None,\n",
    "                                   callbacks=None,\n",
    "                                   verbose=0\n",
    "                                   )\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:36:32.286665Z",
     "start_time": "2023-06-14T15:36:32.268029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN for e_x\n",
      "Training NN for tau_x\n"
     ]
    },
    {
     "data": {
      "text/plain": "271.8425349295992"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_rf = PWLearner('nn')\n",
    "pw_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = pw_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 30.529802728890644\n",
    "# lasso: 16.03059004204301\n",
    "# nn: 271.8425349295992"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:37:16.222009Z",
     "start_time": "2023-06-14T15:37:12.922620Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class ULearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 2: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            self.mux_model.fit(x,y)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x,w)\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "\n",
    "            # calculate residuals\n",
    "            residuals = (y - self.mux_model.predict(x))/(w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x,residuals)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 2: fit mu_x\n",
    "            print('Fitting lasso for mu_x')\n",
    "            self.mux_model.fit(x_poly_train,y)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            residuals = (y - self.mux_model.predict(x_poly_train))/(w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,residuals)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Training NN for mu_x')\n",
    "            self.mux_model.fit(x,y,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            residuals = (y - self.mux_model.predict(x, verbose=0))/(w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,residuals,\n",
    "                                   batch_size=100,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=None,\n",
    "                                   callbacks=None,\n",
    "                                   verbose=1\n",
    "                                   )\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T16:09:21.041752Z",
     "start_time": "2023-06-14T16:09:20.911621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN for mu_x\n",
      "Training NN for e_x\n",
      "Training NN for tau_x\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5290.9863 - mean_squared_error: 5290.9863\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5267.7207 - mean_squared_error: 5267.7207\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5262.9185 - mean_squared_error: 5262.9185\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5261.6846 - mean_squared_error: 5261.6846\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5260.7236 - mean_squared_error: 5260.7236\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5260.0527 - mean_squared_error: 5260.0537\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5259.4658 - mean_squared_error: 5259.4658\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5258.9658 - mean_squared_error: 5258.9658\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5258.5239 - mean_squared_error: 5258.5239\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5258.1772 - mean_squared_error: 5258.1772\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5257.8755 - mean_squared_error: 5257.8760\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5257.6021 - mean_squared_error: 5257.6021\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5257.3730 - mean_squared_error: 5257.3730\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5257.2090 - mean_squared_error: 5257.2090\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5257.0566 - mean_squared_error: 5257.0566\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.9507 - mean_squared_error: 5256.9507\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.8960 - mean_squared_error: 5256.8960\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.8213 - mean_squared_error: 5256.8213\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.8184 - mean_squared_error: 5256.8184\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.7627 - mean_squared_error: 5256.7627\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.7349 - mean_squared_error: 5256.7349\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.7021 - mean_squared_error: 5256.7021\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6792 - mean_squared_error: 5256.6792\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6509 - mean_squared_error: 5256.6509\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6396 - mean_squared_error: 5256.6396\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6299 - mean_squared_error: 5256.6299\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6196 - mean_squared_error: 5256.6196\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6133 - mean_squared_error: 5256.6133\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6094 - mean_squared_error: 5256.6094\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.6040 - mean_squared_error: 5256.6040\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5991 - mean_squared_error: 5256.5991\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5977 - mean_squared_error: 5256.5972\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5962 - mean_squared_error: 5256.5962\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5962 - mean_squared_error: 5256.5962\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5928 - mean_squared_error: 5256.5928\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5898 - mean_squared_error: 5256.5903\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5898 - mean_squared_error: 5256.5898\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5898 - mean_squared_error: 5256.5898\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5898 - mean_squared_error: 5256.5898\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5859 - mean_squared_error: 5256.5859\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5854 - mean_squared_error: 5256.5854\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5850 - mean_squared_error: 5256.5850\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5850 - mean_squared_error: 5256.5850\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5840 - mean_squared_error: 5256.5840\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5869 - mean_squared_error: 5256.5869\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5850 - mean_squared_error: 5256.5850\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5830 - mean_squared_error: 5256.5830\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5859 - mean_squared_error: 5256.5859\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5850 - mean_squared_error: 5256.5850\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5811 - mean_squared_error: 5256.5811\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5796 - mean_squared_error: 5256.5801\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5806 - mean_squared_error: 5256.5806\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5806 - mean_squared_error: 5256.5806\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5806 - mean_squared_error: 5256.5806\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5791 - mean_squared_error: 5256.5791\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5776 - mean_squared_error: 5256.5776\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5791 - mean_squared_error: 5256.5786\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5806 - mean_squared_error: 5256.5806\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5811 - mean_squared_error: 5256.5811\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5840 - mean_squared_error: 5256.5840\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5786 - mean_squared_error: 5256.5776\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5801 - mean_squared_error: 5256.5801\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5791 - mean_squared_error: 5256.5791\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5791 - mean_squared_error: 5256.5791\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5781 - mean_squared_error: 5256.5781\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5806 - mean_squared_error: 5256.5806\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5825 - mean_squared_error: 5256.5825\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5845 - mean_squared_error: 5256.5845\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5786 - mean_squared_error: 5256.5786\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5757 - mean_squared_error: 5256.5757\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5781 - mean_squared_error: 5256.5781\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5737 - mean_squared_error: 5256.5737\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5737 - mean_squared_error: 5256.5737\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5742 - mean_squared_error: 5256.5742\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5752 - mean_squared_error: 5256.5752\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5747 - mean_squared_error: 5256.5747\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5728 - mean_squared_error: 5256.5728\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5728 - mean_squared_error: 5256.5723\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5742 - mean_squared_error: 5256.5742\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5752 - mean_squared_error: 5256.5752\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5762 - mean_squared_error: 5256.5762\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5796 - mean_squared_error: 5256.5796\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5767 - mean_squared_error: 5256.5767\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5752 - mean_squared_error: 5256.5752\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5723 - mean_squared_error: 5256.5723\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5728 - mean_squared_error: 5256.5728\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5737 - mean_squared_error: 5256.5737\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5737 - mean_squared_error: 5256.5737\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5737 - mean_squared_error: 5256.5737\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5713 - mean_squared_error: 5256.5713\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5713 - mean_squared_error: 5256.5713\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5708 - mean_squared_error: 5256.5708\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5713 - mean_squared_error: 5256.5713\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5718 - mean_squared_error: 5256.5718\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5256.5718 - mean_squared_error: 5256.5718\n"
     ]
    },
    {
     "data": {
      "text/plain": "23.375287707518837"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_rf = ULearner('nn')\n",
    "u_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = u_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 30.921286420155806\n",
    "# lasso: 7.6762472449663495\n",
    "# nn: 23.375287707518837 # de isch falsch OBE, da bide class isch korrekt ?????"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T16:10:00.614363Z",
     "start_time": "2023-06-14T16:09:55.278365Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
