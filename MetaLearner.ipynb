{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T02:08:12.309491Z",
     "start_time": "2023-07-24T02:08:12.272463Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.saving import load_model\n",
    "\n",
    "# from abc import ABC, abstractmethod\n",
    "\n",
    "from DefaultParameters import *\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make GPU available for keras\n",
    "# https://medium.com/mlearning-ai/install-tensorflow-on-mac-m1-m2-with-gpu-support-c404c6cfb580"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# set float64 as standard\n",
    "tf.keras.backend.set_floatx('float64')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:46.904907Z",
     "start_time": "2023-07-24T01:19:46.864952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Why 0 GPUs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:48.648500Z",
     "start_time": "2023-07-24T01:19:48.619753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# set global seed\n",
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:48.792873Z",
     "start_time": "2023-07-24T01:19:48.777744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:48.953050Z",
     "start_time": "2023-07-24T01:19:48.926124Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/arberimbibaj/dataset_example_indicatorCATE.csv', header=None, index_col=[0])\n",
    "data = data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:49.455663Z",
     "start_time": "2023-07-24T01:19:49.429349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "d = len(data[0, :]) - 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:49.614516Z",
     "start_time": "2023-07-24T01:19:49.597519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# train test split\n",
    "random.shuffle(data)\n",
    "training, test = data[:700, :], data[700:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:50.319529Z",
     "start_time": "2023-07-24T01:19:50.302959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# slice dataset by treatment status\n",
    "training_control = training[training[:, 26] == 0]\n",
    "training_treatment = training[training[:, 26] == 1]\n",
    "\n",
    "# slice test set by treatment status\n",
    "test_control = test[test[:, 26] == 0]\n",
    "test_treatment = test[test[:, 26] == 1]\n",
    "\n",
    "# Y_train by treatment status\n",
    "Y_train_control = training_control[:, 0]\n",
    "Y_train_treatment = training_treatment[:, 0]\n",
    "\n",
    "# Y_test by treatment status\n",
    "Y_test_control = test_control[:, 0]\n",
    "Y_test_treatment = test_treatment[:, 0]\n",
    "\n",
    "# X_train by treatment status\n",
    "X_train_control = training_control[:, 1:26]\n",
    "X_train_treatment = training_treatment[:, 1:26]\n",
    "\n",
    "# X_test by treatment status\n",
    "X_test_control = test_control[:, 1:26]\n",
    "X_test_treatment = test_treatment[:, 1:26]\n",
    "\n",
    "# X and Y test\n",
    "X_test = test[:, 1:26]\n",
    "Y_test = test[:, 0]\n",
    "\n",
    "# X_train and Y_train (no split by treatment status)\n",
    "X_train = training[:, 1:26]\n",
    "Y_train = training[:, 0]\n",
    "\n",
    "# W_train and W_test\n",
    "W_train = training[:, 26]\n",
    "W_test = test[:, 26]\n",
    "\n",
    "# tau_test\n",
    "tau_test = test[:, 27]\n",
    "tau_test_control = test_control[:, 27]\n",
    "tau_test_treatment = test_treatment[:, 27]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:50.838963Z",
     "start_time": "2023-07-24T01:19:50.825459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# set training and test features for the S-Learner (it views W as no different from other X's)\n",
    "X_W_train = training[:, 1:27]\n",
    "X_W_test = test[:, 1:27]\n",
    "X_test_0 = np.concatenate((test[:, 1:26], np.zeros((300, 1))), axis=1)\n",
    "X_test_1 = np.concatenate((test[:, 1:26], np.ones((300, 1))), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:55.716697Z",
     "start_time": "2023-07-24T01:19:55.689158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.417996, -0.258928,  0.439047, ...,  0.18676 , -0.512326,\n         0.334241],\n       [-0.755952, -0.264981,  0.466614, ..., -0.259811, -0.531267,\n        -0.302281],\n       [-0.169104,  0.508556, -0.68986 , ..., -0.861022, -0.363696,\n        -0.031585],\n       ...,\n       [ 0.711108,  0.216507, -0.645339, ...,  0.037564,  0.564271,\n         0.761275],\n       [-0.16064 ,  0.623831, -0.381564, ...,  1.014533, -0.082782,\n         0.045286],\n       [ 0.443016, -0.63676 ,  0.513939, ..., -0.686854,  1.083426,\n        -1.154605]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:19:56.541982Z",
     "start_time": "2023-07-24T01:19:56.517907Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "T-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu0.fit(X_train_control, Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu1.fit(X_train_treatment, Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((t_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "s_learner.fit(X_W_train, Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "s_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((s_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu0.fit(X_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu1.fit(X_train_treatment, Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_0_hat.fit(X_train_control, imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_1_hat.fit(X_train_treatment, imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "g_x_hat.fit(X_train, W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_test)\n",
    "probas_1 = probabilities[:, 1]\n",
    "probas_0 = probabilities[:, 0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_test) + probas_0 * x_tau_1_hat.predict(X_test)\n",
    "x_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### See g_x hat\n",
    "g_x_hat.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error (much lower here!)\n",
    "((x_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "ind = np.random.choice(len(X_train), int(len(X_train) / 2), replace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:08:12.032634Z",
     "start_time": "2023-07-17T23:08:11.982009Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = np.array((1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "lol"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:08:16.424181Z",
     "start_time": "2023-07-17T23:08:16.381141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "array([9, 0, 3, 4, 6])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind = np.random.choice(10, int(10 / 2), replace=False)\n",
    "train_ind"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:11:03.640494Z",
     "start_time": "2023-07-17T23:11:03.605764Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.zeros(len(X_train), dtype=bool)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:10:15.883879Z",
     "start_time": "2023-07-17T23:10:15.842926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "ind[train_ind] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:11:31.239533Z",
     "start_time": "2023-07-17T23:11:31.189849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True, False, False,  True,  True, False,  True, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:11:38.096176Z",
     "start_time": "2023-07-17T23:11:38.066227Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.417996, -0.258928,  0.439047,  0.114545,  0.202294,  0.232629,\n        -0.583582, -0.430972,  0.429963, -0.382148,  0.27617 ,  1.023249,\n         0.386307,  0.635114, -0.90664 , -1.018844,  0.542753, -0.759871,\n         0.600371, -0.063606,  0.325453,  0.172237,  0.18676 , -0.512326,\n         0.334241],\n       [ 0.40638 , -0.095792, -0.058921,  0.07669 ,  0.997712, -0.455101,\n         0.485145,  0.179103, -0.196513, -0.026966, -0.797527,  0.993493,\n        -0.46944 , -0.262035, -1.769498, -0.164981, -0.351169,  0.332267,\n         0.339417,  0.437438, -0.058943, -0.305766, -0.738398, -0.531641,\n         0.466363],\n       [-0.232977,  0.555448, -0.406086,  0.522266,  0.459783,  0.232973,\n        -0.043957, -0.447066, -0.490697,  0.567011, -0.004792,  1.000241,\n         0.182318, -0.14904 , -0.944393,  0.254892,  0.225088,  0.525903,\n        -1.276382, -0.506376,  0.482294,  0.422674,  0.318297, -0.981335,\n         0.915256],\n       [ 0.468949, -0.445755,  0.392202, -0.835586, -0.215265,  0.235904,\n         0.316909,  0.089419,  0.432548, -0.387784, -0.033511, -1.599416,\n        -0.394379, -0.9608  ,  1.89743 ,  0.975969, -0.502945, -0.057892,\n        -0.33056 ,  0.330216, -1.112766, -0.490608,  0.17439 ,  1.018791,\n        -1.1955  ],\n       [-0.368478,  0.59608 , -0.735811,  0.789282, -0.560681, -0.18187 ,\n        -0.145921,  0.448802, -0.536435,  0.582699,  0.182755, -0.328529,\n         0.381613,  0.635139, -0.084826, -0.63211 ,  0.250661,  0.766336,\n        -0.014227, -0.162542,  0.887049,  0.475692, -0.519485, -0.07647 ,\n        -0.021722]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[ind]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:13:23.727505Z",
     "start_time": "2023-07-17T23:13:23.694647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([False,  True,  True, False, False,  True, False,  True,  True,\n       False,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~ind"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:12:00.679005Z",
     "start_time": "2023-07-17T23:12:00.642796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([7, 4, 5, 1, 6])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[ind]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T22:54:29.109981Z",
     "start_time": "2023-07-17T22:54:29.069121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 4,  7,  6, 10,  5])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[~ind]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T22:56:08.687333Z",
     "start_time": "2023-07-17T22:56:08.642576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "~ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[ind]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1.72631505e+00,  3.64147650e+00,  2.33421522e-01,  2.00015216e-01,\n        3.57105421e+00, -1.98894219e+00,  4.66861967e+00, -1.18776198e+00,\n       -6.72853315e+00,  1.73509282e+00,  1.02315829e+00,  2.66358520e+00,\n        9.21810805e-01, -2.96307293e-01,  8.25681868e-01,  1.12836343e+00,\n        2.91106564e+00,  2.11549719e+00, -5.07782777e-01, -7.78415001e-01,\n        8.94690617e-01,  1.15673857e+00,  2.40012237e+00,  2.19039593e+00,\n        5.58543465e+00,  2.04966289e-01,  1.20332015e+00,  5.60686242e-01,\n       -2.70382222e-01,  6.08551160e+00, -8.98444513e-01, -6.06574142e-01,\n        2.67656515e-01,  1.18291908e+00,  4.71558254e+00,  1.33624346e+00,\n        3.39665587e+00,  1.13418876e+00,  4.24833897e-01,  5.11750884e-01,\n       -2.74338417e-01, -5.78563498e-01,  1.79635905e+00,  3.79477200e+00,\n        2.51367836e+00,  4.55615915e+00,  9.10703451e-01, -1.68875723e+00,\n        2.12238465e+00,  9.43687899e-01,  2.44106823e+00,  3.60689674e-01,\n        3.15639296e+00,  3.40908279e+00,  3.40892175e+00,  2.50774087e+00,\n        3.30892939e-01, -1.52504081e+00,  2.48648457e+00,  2.21288063e+00,\n        3.10895765e+00,  2.66358902e+00, -1.46143438e+00, -2.32882920e+00,\n        3.39350538e+00,  3.09786161e+00,  2.41115102e+00, -1.00360218e+00,\n        2.90771034e+00,  1.63367590e+00,  3.62307282e+00,  3.64738866e+00,\n        2.78700912e+00,  2.77385483e+00,  1.81569939e+00, -5.55911478e-02,\n        3.00838290e-01, -4.15303583e-02, -4.67397533e-01,  1.11921623e+00,\n        4.50440827e+00,  2.86141019e+00,  2.43776523e+00,  1.52588129e+00,\n       -6.52936450e-01,  1.83388090e+00, -1.23025518e+00, -7.82972696e-01,\n        3.33517782e+00,  3.05656410e+00,  2.61968162e+00,  2.11364857e+00,\n        3.28491115e+00,  2.15892937e+00,  1.69146372e-01,  7.35497841e-01,\n       -2.17987947e+00,  2.84661262e+00,  1.23339863e+00,  2.19851332e+00,\n        2.18518849e+00,  4.02497592e+00,  3.33957952e-01,  3.74232020e+00,\n        4.41588246e+00,  4.81178069e+00, -9.22187732e-01,  2.92761286e+00,\n        6.51168099e+00,  8.89099032e-01,  3.41745003e-01,  3.81766961e-01,\n        4.05640427e+00, -1.78036242e-01,  1.96778829e+00,  3.18254874e-01,\n        3.19373307e+00,  1.60532665e+00,  4.37612059e+00,  1.77565103e-01,\n        2.91260365e+00,  4.43694698e+00,  7.64671355e-01,  2.02010577e+00,\n        3.55529123e+00,  8.26336685e-01,  5.11043446e-01,  3.68215891e+00,\n        2.07803486e+00,  1.28429412e-01,  7.70566336e-01,  6.86905509e-01,\n        1.46660234e+00,  8.23393387e-01,  3.28519238e+00,  1.54471039e+00,\n       -3.65580349e+00,  6.09145175e-01, -1.59804722e+00, -2.93400767e+00,\n        8.96402315e-01,  1.71423218e+00,  3.68505010e+00,  6.56425763e-01,\n        1.69711686e-01, -7.86132331e-01,  1.23588415e+00, -2.81991445e-01,\n        8.79889412e-01, -1.88643422e+00,  3.60850833e+00,  7.31090772e-01,\n       -1.52690320e+00,  1.56407897e+00,  3.41674022e-01,  3.06121974e+00,\n        8.78558285e-01,  2.86955457e-01, -4.92367846e-01,  2.86692273e+00,\n        2.60128862e+00, -1.19240084e-01,  1.75220429e+00, -1.02345982e+00,\n        2.80538262e+00,  3.77279416e+00, -2.66671959e-01, -2.56290527e+00,\n       -3.59933443e-01,  4.18990073e+00,  2.13721813e+00,  4.27894054e-01,\n       -3.75349109e-01,  1.66425291e+00,  7.84074392e-02,  1.66586894e+00,\n       -2.06183961e+00,  5.56512666e+00,  3.99680015e+00,  4.07666019e+00,\n        7.92360528e-01,  1.70334718e+00,  5.58145817e-01,  4.66718651e+00,\n        3.43260814e+00, -1.64070640e+00,  1.63950763e-01,  1.64691484e+00,\n        1.37354314e+00, -1.74255876e-01, -1.82592104e+00,  5.77209505e+00,\n        3.08546065e+00,  3.01633086e+00, -2.64183541e-01,  1.35337795e+00,\n        1.17769541e+00,  4.17030091e+00,  1.48263863e-01,  2.32929236e-01,\n       -1.61285462e+00,  7.43578474e-01,  3.41068532e+00,  3.42962713e+00,\n        2.55585667e+00, -4.48804218e-01,  7.90272256e-01, -2.09358152e-01,\n        6.45384959e-01,  3.29598604e+00,  3.70127263e+00,  9.81912485e-01,\n        2.40007838e+00,  2.76487672e+00,  1.04638067e-01,  4.50530891e-01,\n       -9.67799506e-02,  2.45018193e-01,  2.49865075e+00,  2.06479938e+00,\n        3.20844182e+00,  1.74887218e+00,  4.33072035e-01,  2.35581078e+00,\n        4.17588916e-02,  3.32965784e+00,  1.71960890e+00,  3.17136334e+00,\n        2.32836195e+00,  5.85719959e+00,  3.48663542e+00,  1.66610770e+00,\n        5.78960705e+00,  1.04056802e+00,  6.16569243e-01,  3.33340124e+00,\n        2.87442815e+00,  1.51500121e+00,  2.11740251e+00,  1.79656014e+00,\n        2.66272793e+00,  4.42447542e+00,  1.99371299e+00,  4.40361535e-01,\n       -2.16894004e+00,  3.99846849e+00,  1.17404757e+00, -1.27856050e+00,\n        2.45715282e+00, -1.48642983e+00,  1.58529773e+00,  2.90653059e+00,\n        2.99738469e+00,  5.80047454e-01, -6.76703222e-01,  2.04189235e+00,\n       -2.41994635e-02,  3.48618489e-01, -9.53260738e-01,  1.14820951e+01,\n        4.57304383e+00,  8.76942649e-01,  3.22929747e+00,  2.43303290e-03,\n       -2.53951500e+00, -3.86371661e-01,  2.21973910e+00, -2.13188391e-01,\n        5.20151574e+00,  8.20898292e-01,  1.61551896e+00, -7.98396406e-01,\n       -4.87749230e-01,  4.47464203e+00,  1.07484369e+00,  3.43125798e-01,\n        6.04142307e-01, -6.64106037e-01,  3.58036434e+00,  2.98954790e+00,\n       -1.38650884e+00,  2.37150124e-01,  1.58316131e+00,  1.29708829e-01,\n        5.99428743e-01,  2.89293425e+00, -8.38207422e-01,  3.21125696e+00,\n        2.28231311e+00,  3.04762251e+00, -2.12158808e+00,  1.97002506e+00,\n        1.19857012e+00, -6.29740204e-01,  3.75410875e+00, -1.60659130e-01,\n       -1.61963550e+00,  3.53679362e+00,  2.95373587e+00,  3.03345100e+00])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### R-Learner\n",
    "# split for cross-fitting\n",
    "index = np.zeros(len(X_train), dtype=bool)\n",
    "train_ind = np.random.choice(len(X_train), int(len(X_train) / 2), replace=False)\n",
    "index[train_ind] = 1\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x_1 = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "r_learner_e_x_2 = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "r_learner_e_x_1.fit(X_train[index], W_train[index])\n",
    "r_learner_e_x_2.fit(X_train[~index], W_train[~index])\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas_1 = np.zeros(len(X_train), )\n",
    "r_probas_1[index] = r_learner_e_x_2.predict_proba(X_train[index])[:, 1]\n",
    "r_probas_1[~index] = r_learner_e_x_1.predict_proba(X_train[~index])[:, 1]\n",
    "\n",
    "#r_probas_0 = r_probas[:, 0]  # probabilities of W=0\n",
    "#r_probas_1 = r_probas[:, 1]  # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_mu_x.fit(X_train, Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1) ** 2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_tau.fit(X_train, r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)\n",
    "r_tau_hats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:19:41.810750Z",
     "start_time": "2023-07-17T23:19:40.215832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_probas[index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:18:52.820420Z",
     "start_time": "2023-07-17T23:18:52.790227Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_learner_e_x.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "13.714901338146138"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((r_tau_hats - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:19:46.767249Z",
     "start_time": "2023-07-17T23:19:46.714096Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.8469803 ,  8.06422269,  0.74636315,  3.46938656,  9.09528554,\n       -5.41001852, 10.61120148, -1.23555462, -3.1667259 ,  7.34959732,\n        0.37375337,  6.51004978,  3.90636817,  1.71642186, -0.96798373,\n        7.79938902,  2.94713074,  8.28080415, -3.57924635, -1.4321085 ,\n        1.94462515,  2.0366692 ,  9.11873476,  7.36213599,  8.75639886,\n        7.51676077, -0.50598641, -2.63898769,  0.12765948,  8.79954641,\n        0.59651539,  0.33033395,  0.48555466,  1.57145183, 11.22184961,\n        2.4211609 ,  7.90121481,  1.88173152,  1.56996062,  0.61934771,\n        1.21434896,  0.132631  ,  3.83851582,  5.25933493,  8.10620533,\n       10.94689164,  0.56418235, -4.02550191,  0.95639876,  1.19122792,\n        7.08042072,  1.22571084,  8.62864371,  7.22893055,  7.80239003,\n        8.80002186,  1.75150115, -2.06467558,  7.62897479,  1.65769294,\n        8.39736954,  3.99587509, -2.05487766, -4.95670679,  8.01155259,\n        7.58633606,  5.82328155, -2.86389204,  9.23833371,  7.13393383,\n       10.10321139,  9.93848715,  7.72788166,  4.89804815,  7.78743681,\n        1.52564725,  1.62940925,  1.51275156, -0.46421158,  4.87286925,\n        4.77501108,  8.49983021,  7.17130644,  6.75265302, -2.05811291,\n        4.24185397,  1.31946376, -3.79884357,  9.27976183,  7.1414544 ,\n        7.22703219,  7.68160496,  7.7053819 ,  3.98523557,  0.36710056,\n        1.0034189 , -0.28123104,  8.91095813,  3.82070105,  6.76863072,\n        2.32271978,  7.2079474 , -0.16397118,  8.78234329, 12.10513602,\n        8.64767911, -0.65433453,  3.67696406, 10.36847484,  0.85371788,\n        1.07842986,  4.08354721,  8.15241489, -0.4759884 ,  1.82774458,\n        1.6930199 ,  7.49973036,  2.72089801, 12.40604569,  1.87956346,\n        7.62027662,  8.89098345,  1.40185376,  3.26641617,  8.90736477,\n        1.87977534,  2.09984291,  7.83682045, -0.82731192,  0.24807534,\n        1.41093217,  0.98990768,  2.16807954,  0.60632358,  7.06954009,\n        8.57562095, -4.74650344,  2.75255137, -1.85012266, -7.28498064,\n        6.72153721,  2.64859423,  8.63187879,  3.69184901,  1.43199632,\n        1.46607347, -1.14600281, -2.7053598 ,  1.69897625, -0.18362787,\n        9.44059947,  0.95016839, -4.51944732,  7.5985786 ,  1.5526539 ,\n        7.35246338, -1.05192043,  1.66580565, -0.54375773,  6.16361185,\n        1.97093383,  0.26958463,  3.69651452, -1.74477032,  6.40938499,\n        8.62481452, -2.05271718, -4.56548166,  0.15351469,  7.52827535,\n        3.62690578, -0.56713607,  1.17603855,  0.85640508,  2.0934953 ,\n        3.63699602, -4.68081298, 12.58141067, 10.00208894,  6.24928314,\n        5.72509096,  1.51574229,  0.65688531,  7.09239344, 10.19417628,\n       -2.41286316, -0.67120725,  3.67203923, -0.9812024 ,  0.39037617,\n       -5.5397037 ,  9.16551629,  6.97157219,  8.07141478,  1.61984933,\n        2.03808093,  3.81849598,  8.27840387,  0.4610143 ,  1.20645788,\n       -1.86083618,  0.79997913,  5.41726373,  6.38031435,  8.11811123,\n       -0.95891724,  2.42010791,  0.39241392,  0.80816481,  7.89672536,\n        8.19754509,  7.6076921 ,  8.37719208,  7.80977331,  1.7500487 ,\n        1.79709236, -0.23823989, -0.20333862,  6.43402486,  0.23306019,\n        8.76688305,  1.48797297,  2.58560426,  2.94808071, -0.81265556,\n        9.52403853,  8.20419364,  9.29683722,  8.3545264 , 11.97481221,\n        8.81016706,  2.14509445, 10.85758209,  8.06266025,  1.68626684,\n        8.08933821,  8.07073589,  7.8785909 ,  8.09877331,  4.00339975,\n        1.88671795, 10.73243919,  4.30231808,  2.43486141, -5.72600924,\n        8.49096898,  1.64691894,  3.54687815,  7.77508504, -1.94894436,\n        3.89608355, 10.87704782,  7.77224239, -0.91084068, -0.02376878,\n       -0.09446218,  8.08534249,  0.21470782,  0.05104656,  9.37855751,\n       10.15218286,  0.46661333,  8.38986254,  2.97986713, -3.7481927 ,\n       -0.48909984,  5.77907839, -1.88639561,  8.23759816, -0.45744631,\n        7.01711911, -1.99348823,  0.2196328 ,  7.76390094,  6.64708927,\n        1.13971154,  0.40244715, -0.57485326,  9.12486117,  8.86423394,\n        0.49229974,  2.36521851,  3.78975175,  1.15036578,  2.37745891,\n        8.84286405, -3.63406286,  7.99651526,  5.70313456,  9.89484538,\n       -4.932646  ,  8.03624093, -0.49401259,  1.77844646,  6.18214895,\n       -0.0526304 ,  2.13548308,  3.30879787,  7.87054906,  8.35261874])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DR-Learner\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_train)\n",
    "dr_probas_0 = dr_probas[:, 0]  # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:, 1]  # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_mu_0.fit(X_train_control, Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_mu_1.fit(X_train_treatment, Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_train) + (1 - W_train) * dr_learner_mu_0.predict(\n",
    "    X_train)  # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(\n",
    "    X_train) - dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_train, dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "dr_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:12.203387Z",
     "start_time": "2023-07-17T23:20:10.678569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "5.371503418814479"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((dr_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:12.986441Z",
     "start_time": "2023-07-17T23:20:12.930736Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.91490677,  8.26301955,  0.97106317,  3.5528225 ,  8.67564974,\n       -4.68236172, 10.6688935 , -1.17413595, -2.63692004,  7.44641532,\n        0.2918939 ,  6.43137586,  3.73575211,  2.59984884, -0.86203239,\n        7.66755618,  2.51321221,  8.82989392, -4.41247407, -0.60076672,\n        2.64336168,  1.99061936,  8.96845941,  7.7862815 ,  8.21040191,\n        7.55267284, -1.14354599, -1.6653916 ,  1.25465375,  8.52964283,\n       -0.46274891, -0.08576136,  0.63671972,  1.27520691, 11.51410645,\n        2.00179904,  8.06064817,  0.79332432,  1.88817362,  0.45527881,\n        1.06693595, -0.11435547,  2.77432839,  4.61230113,  8.27338701,\n       10.53841517,  0.61909964, -4.58885619, -0.24858074,  1.83739071,\n        7.30205278,  0.52086065,  9.27698339,  7.95976077,  7.9325154 ,\n        8.00236788,  2.13039965, -2.13439482,  7.9438079 ,  1.6070221 ,\n        7.90002368,  2.89252177, -2.14836991, -4.90215694,  8.36905704,\n        7.70217215,  5.54715161, -1.83924182,  9.59718485,  6.96651919,\n        9.97623643, 10.50304114,  8.46248293,  5.44752101,  8.23284585,\n        1.87035848,  1.76253894,  1.096323  , -0.77281902,  4.65886033,\n        4.08693553,  9.06629475,  7.2846707 ,  6.91463276, -2.45478032,\n        4.29423858,  1.047026  , -4.00246238,  8.45749121,  7.3308703 ,\n        7.48653022,  8.34125527,  7.69065389,  4.52825058,  1.28770055,\n        0.63088139,  0.09525266,  8.75691332,  4.47194583,  7.23818097,\n        2.40536132,  6.45496018, -0.09073443,  8.38050831, 11.67956658,\n        9.05400165, -1.35824746,  3.67146358,  9.19640487,  1.58340767,\n        1.1739344 ,  3.73147696,  8.57807127, -0.7306568 ,  2.10871377,\n        1.93744957,  6.98260567,  2.12583702, 11.66317864,  1.92150873,\n        7.28413872,  8.8332668 ,  0.97069832,  3.13148742,  9.16374074,\n        1.1991766 ,  2.7466835 ,  7.87338095, -0.53782637,  0.16073633,\n        2.20942804,  1.5098778 ,  1.71295282,  0.97727523,  6.83324377,\n        8.6180895 , -3.92330714,  2.82209429, -2.0266383 , -7.59311155,\n        6.3671048 ,  3.15615404,  7.98434222,  3.33001691,  2.74047294,\n        1.03254886, -1.25837108, -2.92810248,  0.9371009 , -0.22489178,\n        9.36536038,  1.15156331, -4.86577529,  7.69639877,  1.66780274,\n        8.11559443, -1.14733722,  2.34846067, -1.13603509,  7.02301746,\n        2.05258472, -0.34833145,  3.93935158, -1.75590286,  7.08446459,\n        9.12730268, -1.34887892, -4.68200697,  0.03928503,  7.59127757,\n        3.1800284 , -0.60252759,  1.82650608,  0.70264057,  2.97982366,\n        3.89388756, -3.84804754, 13.27910146, 10.12442684,  6.17366687,\n        5.04829352,  1.67815604,  0.15533602,  6.07207105,  9.50717836,\n       -1.17030761, -0.39387043,  2.84623984, -1.77919363,  0.19379624,\n       -5.57453544,  9.40441703,  7.15254168,  7.4551914 ,  1.93933243,\n        1.54918812,  3.41283043,  8.42730052, -0.06615665,  0.59497905,\n       -2.35644412,  0.65944959,  5.69666181,  5.94491705,  8.09355056,\n       -1.49469063,  2.48439797,  0.37201801,  1.78266505,  8.01401948,\n        7.76224435,  8.2296767 ,  8.63921695,  7.83134102,  1.62885603,\n        1.22704062, -0.21580536, -0.27625405,  5.9063476 , -0.25489677,\n        8.93256632,  1.53016088,  3.33282188,  2.74638323, -0.91148359,\n        9.66951246,  7.62377911,  9.3824503 ,  8.49887363, 11.88052343,\n        8.72153874,  2.43572754, 10.23488754,  7.25785079,  1.52223446,\n        7.04445635,  7.69460576,  7.84594312,  8.70958812,  3.68954893,\n        1.27426381,  9.95993074,  3.9941588 ,  3.07839905, -4.39324739,\n        8.34635162,  2.14669136,  3.56857981,  8.43191879, -2.48142331,\n        3.85694975, 10.29505727,  8.20617483, -0.48670216,  0.34770498,\n        0.11407752,  7.6853113 ,  0.69642855,  0.05512238,  9.49577312,\n        9.80476658,  0.55308993,  7.7286017 ,  3.0569524 , -4.27417536,\n       -0.54798416,  5.77410651, -1.91132226,  8.28758938, -0.65651156,\n        7.00550478, -2.82245468,  0.10368094,  7.29295899,  5.76176052,\n        0.99843677,  0.38747159, -0.50173003,  9.4128133 ,  7.91981657,\n        0.61867981,  3.71024214,  3.4696848 ,  1.25762509,  2.14036319,\n        8.57665685, -3.8086749 ,  7.58413691,  5.01934423,  9.18869017,\n       -5.33839562,  7.69921217, -0.1654332 ,  2.41413054,  6.41967436,\n       -0.30457117,  1.75865245,  3.39697429,  8.06744271,  8.0756532 ])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu0.fit(X_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu1.fit(X_train_treatment, Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "ra_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train * (Y_train - ra_learner_mu0.predict(X_train)) + (1 - W_train) * (\n",
    "        ra_learner_mu1.predict(X_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:16.900972Z",
     "start_time": "2023-07-17T23:20:15.542225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "5.355494017645751"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((ra_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:18.152389Z",
     "start_time": "2023-07-17T23:20:18.095728Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1.69335751e+00,  5.51435777e+00,  6.49317088e-01,  1.38362166e+00,\n        6.18713897e+00, -7.55770863e+00,  2.65565760e+01, -2.88704592e+00,\n       -1.46108193e+01,  8.69275972e+00, -7.73108024e+00,  5.79727300e+00,\n       -3.34877109e+00,  9.08688721e+00, -6.88004021e+00,  1.33255598e+01,\n        5.65555888e+00,  7.26496292e+00,  5.59304347e+00, -5.16473555e+00,\n        4.53800163e+00, -2.70582499e-01,  1.54480767e+01,  8.60462234e+00,\n        7.97849246e+00,  2.72901141e+00, -3.33299618e+00, -1.05528872e+01,\n        7.24175812e-01,  6.29505882e+00, -2.00675881e+00,  2.92123016e+00,\n       -3.33797351e+00,  5.04550345e+00, -1.81037455e+00,  4.78954307e-01,\n        1.46588815e+01,  3.02715507e+00,  2.93444155e+00, -1.53125973e+00,\n       -6.05593583e+00, -5.40691703e+00,  2.93908936e+00,  9.35490041e+00,\n        4.68076226e+00,  7.51259325e+01, -4.81573023e+00, -2.51313844e+01,\n        5.96131991e-01,  1.81198608e+00,  1.07100715e+01, -1.31322532e+01,\n        1.06334147e+01,  1.01695299e+00,  7.05957944e+00,  1.71341908e+01,\n        5.80862462e-01,  6.31926319e+00,  7.51584936e+00,  1.00046398e+01,\n        1.60063893e+01,  9.08114935e+00,  4.63327350e+00, -9.16678378e+00,\n        1.04253847e+01,  9.45827673e+00,  7.42628121e+00, -3.39927515e+00,\n        7.10658714e+00,  1.09106524e+01,  1.95009305e+01,  9.15955624e+00,\n        1.56383526e+01,  4.05826169e+00,  5.41169757e+00, -1.77924808e+01,\n       -1.67395808e-01,  4.30007143e+00,  9.22027336e-01, -8.00072486e+00,\n       -1.68392638e+00, -5.94099815e+00,  1.52482133e+01,  2.90352693e+00,\n        1.30401690e+00,  3.32396964e+00,  5.03974252e+00, -4.37800286e+00,\n        3.27991655e+01,  2.46924811e+01,  9.29782729e+00,  8.02734903e+00,\n        2.34030820e+01, -1.40529139e+01,  2.83033777e+00,  9.25475848e+00,\n       -7.74441139e-01,  1.48505376e+01, -5.78486026e+00,  5.62998472e+00,\n        4.59930131e+00,  8.34313698e+00,  3.13601233e+00,  1.10858123e+01,\n        4.60825816e+01,  1.13651812e+01, -2.71635496e+00,  1.14057035e+01,\n        1.14423933e+01,  7.39242083e+00,  3.77120074e+00,  5.02399762e+00,\n        4.77309961e+00, -3.50916602e+00,  3.51963610e+00,  4.41179321e+00,\n        9.53011269e+00,  5.09796875e+00,  3.42936106e+01, -7.07584331e+00,\n        8.23632868e+00,  9.38661423e+00, -2.36432104e+00,  9.84442739e+00,\n        9.81673063e-01,  1.95771441e+00,  5.27015632e+00,  9.55681446e+00,\n       -4.35741342e+00,  6.38141628e+00,  1.22135057e+01, -4.25705422e-01,\n        7.36927114e+00,  3.56805184e-01,  3.31416741e+00,  1.72042107e+01,\n        1.58968025e+01, -3.25462553e-01, -8.33614244e+00, -4.17601775e-01,\n        4.39781661e+00, -9.43273826e+00,  1.12570123e+01, -6.09933672e+00,\n        7.91230649e+00,  3.78913407e+00, -7.49771326e+00, -2.24974032e-01,\n        2.11920338e+00,  1.94995653e+00,  1.27799531e+01, -9.48158637e+00,\n       -3.33749315e+00, -9.31622271e-01,  5.64862895e+00,  7.91748636e+00,\n       -5.96754609e+00, -5.50411085e+00,  1.75175489e+00,  9.92744956e+00,\n       -4.66542963e+00,  4.38964107e-01,  6.54731347e+00,  5.85297621e-01,\n        1.92466120e+01,  6.59837684e+00, -4.37253314e+00, -4.77430722e+00,\n        7.92112236e+00,  1.00059337e+01,  1.14562125e+01,  4.59437190e+00,\n       -3.78063677e+00,  4.51311392e-01, -3.54635790e+01,  2.46015942e+01,\n        1.28713605e+00,  3.73431606e+01,  3.26549530e+01,  6.88986125e+00,\n        2.79843987e+01,  5.67567210e-01, -2.03743087e+00,  1.80152536e+01,\n        1.02135672e+01, -8.51623120e+00, -7.52529853e-01,  3.44184401e+00,\n        4.31272593e+00,  4.59457326e+00, -5.28099470e+00,  1.45038933e+01,\n        4.23428804e+00,  6.46561978e+00,  4.00288760e+00,  3.07762519e+00,\n        9.19841420e+00,  1.10884382e+01,  5.80348841e+00,  1.47602050e+00,\n        6.01540030e+00, -4.07779374e+00,  5.39498129e+00,  4.78301457e+00,\n        1.22277537e+01,  9.08894968e+00,  2.62696585e+00, -5.77625212e+00,\n       -9.32576987e-01,  1.79737091e+01,  9.14652181e+00,  4.20318716e+00,\n        1.66449723e+01,  5.37270439e+00,  1.55030388e+00,  6.86654174e+00,\n        1.45239106e+00,  4.77168797e+00,  9.72610985e+00, -3.09803824e+00,\n        8.74857872e+00,  1.06550942e-01, -2.03544298e+00,  1.32309238e+01,\n       -1.73683813e+01,  2.33909586e+00,  1.13684031e+01,  1.07080139e+01,\n        1.76635943e+01,  4.88415774e+00,  3.68243018e+00,  4.77887683e+00,\n        1.84813968e+01,  1.73058211e+01,  8.00704694e+00,  6.69035079e+00,\n        4.69966032e-01,  8.28313807e+00,  7.55515437e+00,  1.69863302e+01,\n        2.92908651e+00,  5.01313013e+01, -6.57243705e+00,  2.84867876e+00,\n       -6.81410838e+00,  8.26538689e+00,  7.20454621e-01,  6.23491771e+00,\n        6.61315091e+00, -1.29700609e+01,  8.64368490e+00,  4.32879807e+01,\n        1.98815910e+01, -1.16835858e+00,  1.54346773e-01, -3.15426554e+00,\n        1.17477184e+01, -2.97294325e-01, -2.69738825e+00,  1.12306875e+01,\n        9.62544774e+00,  1.41793150e+00,  1.66124852e-01,  1.34026156e+01,\n       -2.81247634e+01, -2.21214804e+00,  1.66830995e+01, -3.80880565e+00,\n        6.77396488e+00, -3.30956290e-02,  1.04778929e+01,  1.85902343e+00,\n        2.91340458e+00,  8.67790427e+00, -1.63861327e+00, -9.39886635e+00,\n        3.51514021e+00, -3.98182693e+00,  3.15586435e+01,  2.68174525e+01,\n       -9.37875419e+00,  1.47016067e+01, -1.12004441e+01,  7.34660192e+00,\n        2.81203297e+00,  1.59120514e+01, -3.43898871e-01,  1.22161880e+01,\n        2.31519359e+00,  1.19796805e+01, -1.00656903e+01, -4.22104805e+00,\n        9.43756996e-01, -1.07476382e+01,  5.95348697e+00,  5.60755308e+00,\n        8.13732265e+00,  7.82383004e-01,  9.23308111e+00,  8.23538797e+00])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ACHTUNG: TRIED TO INCLUDE CROSS-FITTING!\n",
    "### PW-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0_1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu0_2 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu0_1.fit(X_train_control, Y_train_control)\n",
    "pw_learner_mu0_2.fit(X_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1_1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu1_2 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu1_1.fit(X_train_treatment, Y_train_treatment)\n",
    "pw_learner_mu1_2.fit(X_train_treatment, Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "# split for cross-fitting\n",
    "index = np.zeros(len(X_train), dtype=bool)\n",
    "train_ind = np.random.choice(len(X_train), int(len(X_train) / 2), replace=False)\n",
    "index[train_ind] = 1\n",
    "\n",
    "pw_learner_e_x_1 = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "pw_learner_e_x_2 = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "pw_learner_e_x_1.fit(X_train[index], W_train[index])\n",
    "pw_learner_e_x_2.fit(X_train[~index], W_train[~index])\n",
    "\n",
    "pw_probas_1 = np.zeros(len(X_train), )\n",
    "pw_probas_1[index] = pw_learner_e_x_2.predict_proba(X_train[index])[:, 1]\n",
    "pw_probas_1[~index] = pw_learner_e_x_1.predict_proba(X_train[~index])[:, 1]\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train / pw_probas_1 - (1 - W_train) / (1 - pw_probas_1)) * Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:25:41.678915Z",
     "start_time": "2023-07-17T23:25:39.592621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "94.43780635665614"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((pw_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:25:43.752321Z",
     "start_time": "2023-07-17T23:25:43.704250Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2.85250904e+00,  1.00547688e+01, -4.92574582e-01,  1.91452713e+00,\n        7.92620400e+00, -1.16463139e+01,  1.31921917e+01, -2.11709957e+00,\n       -7.16250459e+00,  6.00462123e+00,  2.72981362e+00,  7.66322441e+00,\n        2.45976887e+00,  1.18104536e+00,  9.14107486e-01,  6.16511795e+00,\n        1.08497420e+01,  1.01644602e+01, -2.16899051e+00, -1.81475606e+00,\n        1.34111458e+00,  3.41936913e+00,  1.04168301e+01,  7.57884238e+00,\n        9.50578740e+00,  2.09883177e+00,  1.69555973e+00,  2.79744550e+00,\n        4.16811513e-03,  1.50075375e+01, -4.28915305e+00, -8.77925502e-01,\n       -2.78016855e+00,  3.32467561e+00,  1.06499625e+01,  3.00485772e+00,\n        8.31998895e+00,  4.89600153e+00,  6.81894459e-02,  2.19437557e-02,\n       -1.49448971e+00, -1.36872242e+00,  4.83187150e+00,  9.35912173e+00,\n        6.47748569e+00,  1.37123644e+01,  4.08149426e+00, -2.59808756e+00,\n        3.33981472e+00,  1.33195860e+00,  7.00985818e+00,  2.88766820e+00,\n        1.12951611e+01,  9.49361535e+00,  8.27421488e+00,  9.65030646e+00,\n        1.66799120e+00, -4.27047407e+00,  5.46663229e+00,  5.45520384e+00,\n        5.67327631e+00,  3.40459522e+00, -3.44338015e+00, -6.12381311e+00,\n        9.49446434e+00,  7.70510149e+00,  6.20623224e+00, -5.47830310e+00,\n        8.40988433e+00,  1.97504134e+00,  5.41909400e+00,  8.66600078e+00,\n        7.82765671e+00,  7.11545164e+00,  2.04529673e+01,  4.40529511e-01,\n        3.37877850e+00, -3.56197473e-03, -6.86873419e-02,  4.57507701e+00,\n        6.59528519e+00,  9.10911278e+00,  9.41461153e+00,  6.89354467e+00,\n       -1.47101875e+00,  4.48008331e+00, -2.54218086e+00, -3.59854074e+00,\n        8.98837471e+00,  1.03377520e+01,  6.94540158e+00,  6.49344661e+00,\n        7.70222005e+00,  5.04733032e+00,  6.24781139e-01,  3.07909370e+00,\n       -3.12769196e+00,  1.49029864e+01,  6.29949225e+00,  6.63651138e+00,\n        4.61421670e+00,  3.67322564e+00,  1.43165296e+00,  9.89797870e+00,\n        1.31450812e+01,  8.97206431e+00, -2.22841353e+00,  7.85190017e+00,\n        2.80039878e+01,  9.66392220e+00, -6.51791900e-01,  3.97786190e+00,\n        1.23782826e+01, -7.80540663e-01,  5.26144751e+00,  3.36665356e+00,\n        5.86354648e+00,  3.64687801e+00,  1.12314667e+01,  1.76252990e+00,\n        8.47935090e+00,  1.83555436e+01,  5.96450093e+00,  4.13233759e+00,\n        7.77252980e+00,  1.37507557e+00,  4.46851179e+00,  9.77506013e+00,\n        1.80642718e+00,  2.41555820e+00,  2.20732427e+00,  3.84527284e+00,\n        3.66475808e+00,  2.32778550e+00,  9.16702510e+00, -5.83256253e-01,\n       -1.04964039e+01, -8.16031335e-01, -6.20261281e+00, -1.43077006e+01,\n        3.31902372e+00,  9.46399611e+00,  2.87005749e+01,  6.86013081e-01,\n        1.45515077e+00,  3.46689925e-01, -6.48229240e-01, -5.61947813e-01,\n        2.78998828e+00, -8.03682461e+00,  9.12770739e+00, -5.75156238e-01,\n       -4.43598381e+00, -7.19008038e-02,  1.26663628e+00,  6.86145562e+00,\n        1.93697961e-01,  9.07254794e-01, -2.72445241e-01,  1.07944844e+01,\n        6.39337734e+00,  1.33017331e-01,  5.62652197e+00, -3.19763361e+00,\n        8.48954009e+00,  1.81307848e+01,  5.39347373e-01, -9.28889636e+00,\n       -5.51078616e-01,  1.02733607e+01,  7.31211334e+00,  3.85022172e-01,\n        4.48516976e-01,  2.22105207e+00, -6.48613734e+00,  4.35275483e+00,\n       -8.76228628e+00,  1.39775752e+01,  1.01309095e+01,  8.73729369e+00,\n        3.97215982e+00,  6.02264572e+00,  2.56328305e+00,  1.01349625e+01,\n        1.19917344e+01, -6.67790460e+00,  8.26548189e-02,  2.74261297e+00,\n        2.85720624e+00,  5.97073093e-01, -8.41997318e+00,  2.41724609e+01,\n        7.21191612e+00,  8.46737936e+00, -9.19178491e-01,  4.70694059e+00,\n        3.64573241e+00,  1.05688983e+01, -3.23810244e+00,  2.02691448e+00,\n       -7.93244530e+00,  5.44136528e+00,  8.64528575e+00,  9.03190285e+00,\n        8.33858429e+00,  2.60432708e+00,  2.31530191e+00,  7.20192325e-01,\n       -4.21964337e-02,  8.18908829e+00,  7.28568702e+00,  8.34612959e+00,\n        1.15142092e+01,  5.76128388e+00, -3.34582334e-01,  2.19462888e+00,\n        2.09085698e-01,  2.82480677e-01,  6.60714368e+00,  6.77216767e+00,\n        7.15360305e+00,  3.96479096e+00, -1.64606847e-01, -4.82386291e+00,\n        3.52695177e+00,  9.71360470e+00,  9.27205676e+00,  9.74753554e+00,\n        8.15734382e+00,  1.54316967e+01,  5.88685109e+00,  4.06011379e+00,\n        1.09673740e+01,  3.15390269e+00,  1.74028967e+00,  5.78397121e+00,\n        7.59080327e+00, -2.44058883e+00,  7.11314551e+00,  1.91227419e+01,\n        5.37158902e+00,  1.25067094e+01,  5.16746027e+00,  1.39687565e+00,\n       -1.07915642e+01,  1.17879854e+01,  3.57076284e+00, -1.76982812e+00,\n        3.71536540e+00, -2.98371938e+00, -2.95070740e+01,  7.56902650e+00,\n        8.34883584e+00,  4.38851979e+00, -2.80794330e+00, -2.98553571e-01,\n       -2.16876572e+00, -7.97058936e-01, -2.66855112e+00,  3.40190650e+01,\n        1.78093237e+01,  7.58489102e-01,  8.41063862e+00,  8.89314259e-01,\n       -3.68213887e+00, -2.34205409e+00,  8.21394882e+00,  1.65366763e-01,\n        2.35475500e+01,  1.17073606e+00, -3.90064380e-01, -4.29804364e+00,\n       -9.21310752e-01,  1.39088594e+01,  2.09249398e+00,  2.63959978e-01,\n        1.10591085e+00, -8.29732626e-01,  1.07572133e+01,  8.17239651e+00,\n       -1.84561323e+00,  1.81579044e+00,  4.39918375e+00, -6.40101573e-01,\n        7.69861706e-01,  3.91485286e+01, -5.07151018e+00,  7.43479396e+00,\n        5.86257661e+00,  3.36854123e+00, -7.76981926e+00,  8.78801234e+00,\n        5.74807320e+00,  3.05998382e+00,  1.05205518e+01, -6.46393362e-01,\n       -1.93027465e+00,  5.24818207e+00,  8.14458904e+00,  6.16366838e+00])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "u_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_learner_mu_x.fit(X_train, Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_train)) / (\n",
    "        W_train - u_learner_e_x.predict_proba(X_train)[:, 1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_tau_hat_learner.fit(X_train, u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:26.068576Z",
     "start_time": "2023-07-17T23:20:24.619382Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "30.921286420155806"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error\n",
    "((u_tau_hats - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:26.068796Z",
     "start_time": "2023-07-17T23:20:26.021545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "                0\ncount  700.000000\nmean     0.500714\nstd      0.327341\nmin      0.010000\n25%      0.180000\n50%      0.635000\n75%      0.830000\nmax      0.960000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>700.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.500714</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.327341</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.180000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.635000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.830000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.960000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(u_learner_e_x.predict_proba(X_train)[:, 1]).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:26.115632Z",
     "start_time": "2023-07-17T23:20:26.029431Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-7.53570215e+00,  4.98723100e+00,  1.27991161e+00, -9.53500778e+00,\n        9.61888400e+00,  1.06705232e+01,  9.07106653e+00,  5.32108474e+00,\n        1.25167032e+01,  2.77165694e+00, -7.52731027e+00,  3.07867939e+00,\n       -6.34744625e-01, -1.10509041e+01,  6.39476036e+00, -5.51503950e+00,\n        8.12170286e-01, -8.40279786e+00,  1.28387357e+01,  2.72450608e+00,\n        2.12960700e+00,  6.32594490e+00,  2.78297020e+01,  1.20707445e+01,\n        1.25844545e+01,  1.54171109e+01, -8.19509583e+00,  1.49761160e+01,\n       -2.62166063e+00, -1.10321900e+00,  1.81592264e+01,  4.08048964e+00,\n       -1.03011748e+01,  1.84751838e+00,  1.15022312e+00,  4.26358376e+00,\n       -9.46753062e-01,  3.23614317e+00,  1.25883081e+01,  1.92386464e+01,\n       -8.86369172e+00,  4.26094588e-01, -1.44925474e-01, -5.89680377e+00,\n       -1.57057073e+00,  8.44735270e+00,  2.92545955e+00,  5.66846345e+00,\n       -4.83778690e+00,  5.80575091e+00,  1.35073998e+01,  6.02018300e+00,\n        1.32363581e+01,  2.67540760e+01,  1.12928505e+01,  2.26984222e+01,\n       -7.93762725e+00,  7.49714421e+00,  1.63174373e+01, -2.51221917e+01,\n        1.91380031e+01,  4.79704048e+00,  5.90180585e+00,  1.79904633e+00,\n        4.99708991e+00,  4.46355748e+00,  5.27216012e+00,  2.37931189e+01,\n        1.29857620e+01, -1.04810777e+00,  1.50117808e+01,  6.39392814e+00,\n        1.77983682e+00, -4.03343747e+00,  9.75555105e+00,  7.47949242e+00,\n        1.22306604e+01,  2.69283944e+00,  6.12149229e+00,  1.98671736e+01,\n        9.29845626e+00,  6.14362621e+00, -3.33504550e-01,  2.34584766e+01,\n        9.34229333e-01,  2.96929979e+00,  3.79984942e+00,  2.34292391e+00,\n       -2.67105625e+01,  1.29889630e+00,  2.75207541e+01, -3.26358883e+00,\n       -7.68604567e+00,  6.86020619e-01, -4.26380244e+00,  2.86558212e+00,\n        3.50087135e+00,  1.91215319e+01,  3.26960200e+00,  9.05220692e+00,\n        9.86069940e+00, -1.32555384e+00,  1.46477870e+00, -1.65370230e+01,\n        1.75204281e+01, -5.64988724e+00,  7.15339163e+00,  1.36640650e+01,\n        1.05197764e+01,  3.48434709e+00, -1.25163824e+01,  1.27458069e+01,\n        1.15969515e+01,  1.86666777e+00,  5.27270057e+00,  1.89143109e+01,\n        5.94289508e+00, -2.14181418e+01,  1.03534205e+01, -1.63412932e+01,\n        1.37228247e+01,  1.28689693e+01, -1.80145023e+00,  2.64210286e+00,\n        1.08958320e+01,  1.07626915e+01, -6.15109100e+00, -1.05700000e+00,\n       -3.59609695e+00,  1.75781522e+01,  1.00126108e+00,  2.66442213e+01,\n        1.02989490e+01, -2.63105759e+00,  1.44437114e+00, -6.24553128e+00,\n       -6.03377353e+00,  1.46589281e+01, -5.56073719e+00, -2.29328944e+01,\n        1.30987038e+01, -8.31447674e+00,  1.62270866e+01,  1.36840200e+00,\n        9.08353131e+00,  6.85896683e+00,  2.75232610e+00,  1.84958889e+01,\n        2.84518496e+00, -1.42147223e+00, -3.58603250e+00, -1.37600021e+01,\n        1.70045646e+01, -3.69347969e+00,  4.62854542e-01,  3.38004916e+00,\n        1.79206391e-01,  2.51246899e+01, -5.94631644e+00, -1.38318918e+00,\n       -2.57445717e+00,  5.37036015e+00, -1.43579384e+01, -2.78687667e+00,\n       -8.03261992e+00,  1.33084647e+01,  7.34352135e+00,  3.96903862e+01,\n        5.28227448e+00, -1.20988938e+01,  1.26077171e+01,  3.47263806e+00,\n        3.78686579e+00,  1.07329522e+01,  9.66287043e+00, -4.43802691e+00,\n        6.92222361e+00,  1.31266677e+01, -7.79428263e+00,  2.98369940e+01,\n        7.90640491e+00, -4.50895707e+00,  1.37475797e+01,  3.65804505e+00,\n       -2.89512154e+00,  6.81615788e+00,  3.27893900e+00, -1.51561896e+01,\n       -7.51430327e+00,  1.13098022e+01, -9.61812880e+00, -9.67851282e+00,\n        4.53787400e+00,  1.25987357e+01,  5.79116822e+00,  2.22496503e+00,\n        1.69787769e+01, -2.07787375e-01, -2.76433794e+00, -1.33384617e+02,\n       -1.44014455e+01,  6.00444650e+00,  1.54644676e+00,  3.90989367e+00,\n       -4.31283932e+00,  5.95523130e-01,  2.58839558e+01, -6.45482360e+00,\n        1.15623351e+01,  1.03503408e+01, -2.25740656e+00,  9.10546531e+00,\n       -1.22621653e+00,  4.77049467e+00, -7.45583700e+00, -1.61775888e+01,\n       -5.60335900e+00,  8.57758080e+00,  5.44062067e+00, -9.32127500e-01,\n        5.03506026e+00,  1.48771591e+00,  2.19252219e+01, -3.05218718e+00,\n       -5.59293222e+00,  3.19799983e+00,  1.03381566e+01,  4.13520875e+00,\n       -1.04158035e+01,  9.57761373e+00,  5.95159162e+00,  1.24468149e+01,\n        2.03925212e+01,  1.31195809e+01, -5.42034520e+00,  4.01836702e+01,\n        1.72491958e+01,  9.03342090e+00,  5.28189300e+00,  4.19683263e-01,\n       -8.44971000e-01,  1.15901409e+01, -2.34174962e+01,  1.42100699e+01,\n        2.52044443e+00, -1.30208075e+01, -2.84242706e-01,  8.56404425e+00,\n        1.36950645e+01, -1.05159805e+01,  7.44499748e+01,  1.30329491e+01,\n       -9.94017575e+00, -1.06714267e+00, -1.09675097e+01,  1.63818870e+01,\n        1.20024585e+01, -1.44388178e+00, -3.50950078e+00,  1.05264925e+00,\n        5.48064870e+00, -1.38652374e+01, -9.86322656e+00,  3.72222588e+00,\n        1.40596242e+01,  1.33453069e+01, -2.29474444e+00,  1.39629782e+00,\n        3.53306700e+00, -1.41000908e+00,  6.76992570e+00, -8.78836652e-01,\n       -4.31814996e+01, -9.68247300e+00,  1.11972026e+01,  5.93480387e+00,\n        1.18677569e+01,  1.63198445e+01,  8.17523733e-01, -7.54184935e+00,\n       -7.42389091e-01,  3.03594842e+00,  1.11415933e+01, -4.18104279e+00,\n        9.79610533e+00,  3.63153625e-01,  6.74469179e+00, -2.75131207e+01,\n        1.30393712e+01, -2.57867020e+01,  9.90310537e+00,  2.99645712e+00,\n        2.72050620e+01,  3.35617671e+00,  6.80439688e+00, -7.52963522e+00,\n        3.40043805e+00, -2.77493520e+00, -1.13726795e+01,  5.68109808e+00,\n        8.82436640e+00,  1.33139876e+01, -3.29239078e+00,  1.36283032e+01,\n        9.05352650e+00, -4.17458407e+00, -9.06868480e-01, -7.98853922e+00,\n        8.77831176e-01, -4.66950277e+00,  1.16602666e+01,  1.99437238e+01,\n        6.01339112e+00,  4.42867688e+00,  2.46784074e+00,  1.24231357e+01,\n        6.51417379e+00,  1.32719574e+00,  2.66381544e+01,  5.49530233e+00,\n       -4.06385564e+00, -5.80585108e+00,  3.86860176e-01,  4.16869548e+00,\n        1.17515950e-01,  1.73501395e+00,  2.79601095e+01,  1.30647792e+01,\n       -5.75532018e+00,  1.54633408e+01,  2.51503301e+01, -1.68673631e+00,\n        5.21344424e+00, -6.02091105e+00, -9.01890015e+00,  1.26534575e+01,\n        7.12684979e+00,  1.82754287e+01, -7.77614973e+00, -1.28314555e+00,\n        6.56226684e-01, -1.03179454e+01,  5.12354547e+00, -3.16439403e+00,\n        8.34049456e+00, -2.98744033e+00, -3.17901434e+01, -4.81518843e+00,\n        6.37462533e+00, -6.92878518e+00,  5.86209073e+00,  1.66904106e+01,\n        7.76272786e+00, -2.93871798e+01,  5.79131312e+00,  5.52529731e+00,\n       -2.44604380e+00,  8.74882333e+00, -4.09893386e+00, -8.99192025e+00,\n        3.35426900e+00,  1.07597730e+00,  2.06565938e+01, -1.42557680e+01,\n        6.56449380e+00,  5.83500800e+00, -6.94608667e+00, -4.17581082e+00,\n        8.27015977e+00,  1.04667375e+01,  2.48985269e+00,  1.02799191e+01,\n        7.97690935e+00,  3.65953315e+00, -1.06436578e+01,  2.29087234e+00,\n        1.93663439e+01,  2.13543732e+01,  1.32080989e+00,  7.99387813e+00,\n        2.18498579e+01,  7.13601779e+00, -3.70420193e+00, -1.23562327e+00,\n        3.02120965e+00, -3.71243281e+00, -8.31820179e+00,  1.71094402e+01,\n       -1.29760461e+01,  1.85621426e+01,  1.56390386e+00, -5.43792115e+00,\n       -2.51158410e+01,  5.93629152e+00,  9.31403722e+00,  1.26493889e+00,\n        5.33068981e+00,  2.03816710e+01, -6.33620767e+00, -1.12180444e+01,\n        9.26449536e+00,  4.42174210e+00, -1.01558378e+00, -2.03595214e+00,\n       -6.04562185e+00, -2.69111052e+00,  1.64425954e+02,  1.40269632e+01,\n       -1.34506316e+01, -6.96461440e+00,  6.12370814e+00,  8.18027647e-01,\n        1.72405948e+01,  9.70356400e+00, -8.83682800e-01, -1.34822326e+00,\n        7.53837584e+00,  1.03570611e+00,  1.36410386e+01,  1.02438186e+01,\n        1.10591990e+01, -6.55777518e+00,  1.17791992e+01,  2.97243286e+00,\n        6.17090138e+00,  9.23711333e-01,  9.47226820e+00, -5.29914669e+00,\n        7.80742185e+00, -1.28325094e+01,  4.95168450e+00,  3.02237943e+00,\n       -3.54883626e+00,  2.27869009e+00,  2.07512842e+01,  7.07641364e-01,\n        7.34335426e+00,  1.39879907e+01,  1.36732748e+01, -6.02132273e-01,\n        4.30639995e+00, -1.03443902e+01,  5.56529029e+00, -1.00703820e+01,\n        2.26044374e+01,  4.40218311e+00,  5.78186086e+00,  1.89889433e+00,\n       -9.14243083e+00,  3.85751644e+00,  1.24066837e+01,  8.81731646e+00,\n        1.12993318e+01,  2.23651123e+01,  2.85819961e+00, -9.63877320e+00,\n       -7.18534804e+00,  4.76262533e+00,  1.55369233e+01,  1.60531330e+00,\n        2.31833892e+00,  3.76500805e+00,  1.39093492e+01,  5.03736643e-01,\n        1.52880050e+00,  6.22239692e+00, -1.23267177e+01,  2.49261871e+01,\n       -4.42755433e+00,  8.53343546e+00,  5.14002420e+00,  2.09734542e+01,\n        1.63237499e+01,  1.83100058e+01,  1.14923053e+01,  3.40929133e-01,\n        1.02714858e+01, -1.65245733e+01,  6.31559444e-01,  1.01136951e+01,\n        2.03540008e+01,  1.72799850e+00,  1.84339415e+01,  5.99277843e+00,\n        2.30018682e+01,  1.05610509e+00,  1.18755582e+01,  1.31852731e+01,\n        7.79235300e+00,  5.54317728e+00,  1.03671393e+01, -3.38826713e+00,\n        2.14600202e+01, -4.69172238e-01, -1.30480455e+00,  1.77028618e+01,\n       -1.93902883e+01,  1.05840284e+01,  7.75248141e+00, -1.19589116e+01,\n       -9.68842933e-01, -3.70439368e+00, -1.34637402e+01,  3.38424965e+00,\n       -6.96704429e+00, -8.93499111e+00, -9.68924740e+00,  7.37169625e+00,\n        2.73913980e+00,  1.42103858e+01,  2.37226166e+01,  1.02861687e+01,\n        9.67015412e-01, -1.43661190e+01, -7.13874115e+00,  1.05399366e+01,\n        5.95260655e+00, -1.66711067e+00,  1.46092495e+01,  8.65823300e+00,\n        1.40783481e+01, -9.15483327e+00, -1.45983695e+01, -9.22492950e+00,\n       -5.42674938e-01,  2.97892879e+00,  6.44991775e+00,  1.93482481e+01,\n       -3.78607525e+00,  8.65542723e+00, -1.35901551e+01,  1.02160571e+01,\n        4.07938495e+01, -2.70215200e+00, -4.97974785e+00,  4.26589000e+00,\n        2.62701242e+01, -1.20289161e+00,  6.32145665e+00, -3.10785396e+00,\n        1.06413834e+01, -1.99448481e+00,  7.42937877e+00,  9.94099542e+00,\n       -1.26450008e+01,  2.38402172e-01, -2.59714295e+00,  9.28006583e-01,\n       -2.83877762e+01,  3.96848659e+00,  1.48023231e+01,  3.36408161e+00,\n        7.30133724e+00, -2.08445421e+01,  3.58637460e+00,  7.66101305e+00,\n        1.35094176e+01,  1.32659943e+01, -8.79703385e-01,  6.23809823e+00,\n        3.20415188e+00,  1.87087268e+01,  6.80758528e+00, -7.02804526e+00,\n        1.03036328e+01,  2.19607208e+00,  1.21464217e+01,  7.48185500e+00,\n       -2.45733194e+00, -2.65015889e-01, -2.82214220e+00,  1.17761007e+01,\n       -2.64185343e+01,  1.07533968e+01, -1.88508053e+00,  1.09395421e+01,\n        3.20221618e+01,  1.06259850e+00,  2.74160079e+01, -1.40911721e+01,\n        1.22809119e+01, -1.80812647e-01,  6.20014313e+00, -2.52619932e+00,\n       -1.69364417e+01, -8.71142200e+00,  5.16382500e+00, -7.96097029e+00,\n        8.25369433e+00,  2.12508944e+01,  3.05434976e+01, -3.25760415e+01,\n        1.37613233e+01, -3.27080783e-01,  1.45227108e+01,  3.65165707e+00,\n        3.20864008e+01, -2.53936623e+00,  1.03243072e+01,  1.77160122e+01,\n       -3.99919020e+00, -6.34493588e-01, -2.07704871e+00, -2.97553554e+01,\n       -6.73786623e+00, -1.08520125e+01, -1.30099376e+01,  5.53118220e+00,\n       -5.73191500e+00,  1.52289438e+01,  5.59454400e+00,  7.75384948e+00,\n       -1.06424768e+00,  3.44777386e+00,  1.05266244e+01,  3.25787774e+00,\n       -5.79793000e-01,  6.56458608e+00,  1.61760109e+01, -2.84339226e+00,\n        1.48541841e+01,  7.79019797e+00, -5.46829714e-01,  1.57653597e+01,\n        4.79931667e-01,  1.08871998e+01, -3.55558240e+00,  2.08042632e+00,\n        1.75899094e+00,  8.68846415e+00,  6.27591725e+00, -7.06964018e+00,\n        7.94041900e+00,  8.05580587e+00, -2.41972912e+01,  9.78093587e+00,\n        1.48634013e+00,  8.68661279e+00,  2.01509378e+00, -3.84456746e+00,\n        2.20849379e+01,  7.61686979e+00, -1.88173033e+00,  1.02640230e+01,\n        4.60434411e+00,  1.04937079e+01,  3.05956305e+01,  5.03470006e+00,\n       -1.19932383e+00,  6.70896393e+00,  5.99767055e+00,  4.15904553e+00,\n        9.41434111e+00, -2.37000072e+00, -2.27837315e+00,  1.05134862e+01,\n        6.91481229e+00, -9.34049819e+00,  1.59404611e-01, -1.34783085e+01,\n       -1.94594790e+00, -2.56723395e+00,  9.26144354e+00,  1.39402094e+01,\n        1.39245118e+01, -2.70992191e+01,  1.74919649e+01,  8.71617947e-01,\n        1.72196580e+01, -5.92945082e+00,  6.12259750e+00,  1.12279159e+01,\n        6.50383273e+00, -5.44661650e-01,  2.22429800e-01, -1.05385485e+01,\n        4.38049273e-01,  1.73185735e+01, -4.53400904e+00,  1.53558559e+01,\n        4.05036220e+00,  1.78302749e+01, -2.37289765e+00,  2.83916647e-01,\n       -2.67624647e+00,  1.12534830e+01, -1.86730126e+01,  7.97322207e+00,\n        5.13046128e+01,  2.08629317e+01, -1.01361400e+00,  3.09923400e+00,\n        2.56048789e+00,  1.91502642e+01, -3.60155476e+01,  6.00467710e+00,\n        1.02968793e+01, -1.94376100e+00,  5.10757427e+00, -6.49181280e+00])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_learner_residuals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T23:20:26.141128Z",
     "start_time": "2023-07-17T23:20:26.038996Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lasso (or L1-loss for logistic regression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess for lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_train = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "poly_train_treatment = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_treatment = poly_train_treatment.fit_transform(X_train_treatment)\n",
    "poly_train_control = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_control = poly_train_treatment.fit_transform(X_train_control)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "xw_poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_W_poly_train = poly_train_treatment.fit_transform(X_W_train)\n",
    "\n",
    "xw_poly_test_0 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_0 = xw_poly_test_0.fit_transform(X_test_0)\n",
    "\n",
    "xw_poly_test_1 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_1 = xw_poly_test_0.fit_transform(X_test_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "T-learner (Lasso)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Lasso)\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu0.fit(X_poly_train_control, Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_poly_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu1.fit(X_poly_train_treatment, Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_poly_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((t_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "S-learner (Lasso)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "tic = time.perf_counter()\n",
    "# mu_x\n",
    "s_learner = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "s_learner.fit(X_W_poly_train, Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_poly_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_poly_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "print(f'Time for computation: {toc - tic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((s_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X-learner (Lasso (or l1-penalty))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu0.fit(X_poly_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu1.fit(X_poly_train_treatment, Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_poly_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_poly_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_0_hat.fit(X_poly_train_control, imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_1_hat.fit(X_poly_train_treatment, imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "g_x_hat.fit(X_poly_train, W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_poly_test)\n",
    "probas_1 = probabilities[:, 1]\n",
    "probas_0 = probabilities[:, 0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_poly_test) + probas_0 * x_tau_1_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic}')  # 127 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((x_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R-learner (Lasso (or l1-penalty))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "r_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_poly_train)\n",
    "r_probas_0 = r_probas[:, 0]  # probabilities of W=0\n",
    "r_probas_1 = r_probas[:, 1]  # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_mu_x.fit(X_poly_train, Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_poly_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1) ** 2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_tau.fit(X_poly_train, r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_poly_test)\n",
    "r_tau_hats\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic} seconds')  # 98 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DR-learner (Lasso (l1-penalty))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "dr_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_poly_train)\n",
    "dr_probas_0 = dr_probas[:, 0]  # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:, 1]  # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_0.fit(X_poly_train_control, Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_1.fit(X_poly_train_treatment, Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_poly_train) + (1 - W_train) * dr_learner_mu_0.predict(\n",
    "    X_poly_train)  # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(\n",
    "    X_poly_train) - dr_learner_mu_0.predict(X_poly_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_poly_train, dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc - tic} seconds')  # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RA-learner (Lasso)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu0.fit(X_poly_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu1.fit(X_poly_train_treatment, Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "ra_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train * (Y_train - ra_learner_mu0.predict(X_poly_train)) + (1 - W_train) * (\n",
    "        ra_learner_mu1.predict(X_poly_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_poly_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic} seconds.')  # 121 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((ra_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PW-learner (Lasso)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu0.fit(X_poly_train_control, Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu1.fit(X_poly_train_treatment, Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "pw_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train / pw_learner_e_x.predict_proba(X_poly_train)[:, 1] - (1 - W_train) / (\n",
    "    pw_learner_e_x.predict_proba(X_poly_train)[:, 0])) * Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_poly_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic} seconds.')  # 117 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((pw_tau_hat - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U-learner (Lasso)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "u_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "u_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_learner_mu_x.fit(X_poly_train, Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_poly_train)) / (\n",
    "        W_train - u_learner_e_x.predict_proba(X_poly_train)[:, 1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_tau_hat_learner.fit(X_poly_train, u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic} seconds.')  # 98 seconds\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((u_tau_hats - tau_test) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "T-Learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = load_model('model_25')\n",
    "print('Training mu0')\n",
    "t_learner_mu0.fit(X_train_control, Y_train_control,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test, Y_test),\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = load_model('model_25')\n",
    "print('Training mu1')\n",
    "t_learner_mu1.fit(X_train_treatment, Y_train_treatment,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test, Y_test),\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc - tic} seconds.')  # 3 seconds\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(t_tau_hat, (300,)) - tau_test) ** 2).mean()  # 3.18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "S-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = load_model('model_26')\n",
    "s_learner.fit(X_W_train, Y_train,\n",
    "              batch_size=100,\n",
    "              epochs=100,\n",
    "              validation_data=(X_W_test, Y_test),\n",
    "              callbacks=None  # include early stopping\n",
    "              )\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(s_tau_hat, (300,)) - tau_test) ** 2).mean()  # 1.98"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = load_model('model_25')\n",
    "x_learner_mu0.fit(X_train_control, Y_train_control,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test_control, Y_test_control),\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - np.reshape(x_learner_mu0.predict(X_train_treatment), (len(Y_train_treatment),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = load_model('model_25')\n",
    "x_learner_mu1.fit(X_train_treatment, Y_train_treatment,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test_treatment, Y_test_treatment),\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "\n",
    "# d_0\n",
    "imputed_0 = np.reshape(x_learner_mu1.predict(X_train_control), (len(Y_train_control),)) - Y_train_control\n",
    "\n",
    "# regress imputed on X\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = load_model('model_25')\n",
    "x_tau_1_hat.fit(X_train_treatment, imputed_1,\n",
    "                batch_size=100,\n",
    "                epochs=100,\n",
    "                validation_data=(X_test_treatment, tau_test_treatment),\n",
    "                callbacks=None  # include early stopping\n",
    "                )\n",
    "\n",
    "x_tau_1_hat_predicts = np.reshape(x_tau_1_hat.predict(X_test), (len(X_test),))\n",
    "\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = load_model('model_25')\n",
    "x_tau_0_hat.fit(X_train_control, imputed_0,\n",
    "                batch_size=100,\n",
    "                epochs=100,\n",
    "                validation_data=(X_test_control, tau_test_control),\n",
    "                callbacks=None  # include early stopping\n",
    "                )\n",
    "\n",
    "x_tau_0_hat_predicts = np.reshape(x_tau_0_hat.predict(X_test), (len(X_test),))\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = load_model('model_ex')\n",
    "g_x_hat.fit(X_train, W_train,\n",
    "            batch_size=100,\n",
    "            epochs=100,\n",
    "            validation_data=(X_test, W_test),\n",
    "            callbacks=None  # include early stopping\n",
    "            )\n",
    "x_probabilities = g_x_hat.predict(X_test)\n",
    "x_probs_1 = np.reshape(keras.activations.sigmoid(x_probabilities), (len(x_probabilities, )))\n",
    "x_probs_0 = 1 - x_probs_1\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = x_probs_1 * x_tau_0_hat_predicts + x_probs_0 * x_tau_1_hat_predicts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(x_tau_hat, (300,)) - tau_test) ** 2).mean()  # 3.1614 with smoothing of 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = load_model('model_ex')\n",
    "r_learner_e_x.fit(X_train, W_train,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test, W_test),\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "\n",
    "# get e_x predictions\n",
    "r_probabilities = np.reshape(keras.activations.sigmoid(r_learner_e_x.predict(X_train)), len(X_train, ))\n",
    "r_probas_1 = r_probabilities  # probabilities of W=1\n",
    "r_probas_0 = 1 - r_probabilities  # probabilities of W=0\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = load_model('model_25')\n",
    "r_learner_mu_x.fit(X_train, Y_train,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test, Y_test),\n",
    "                   callbacks=None  # include early stopping\n",
    "                   )\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - np.reshape(r_learner_mu_x.predict(X_train), (len(X_train),))) / (\n",
    "        W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1) ** 2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = load_model('model_25')\n",
    "r_learner_tau.fit(X_train, r_learner_pseudo_outcomes,\n",
    "                  sample_weight=r_learner_weights,\n",
    "                  batch_size=100,\n",
    "                  epochs=100,\n",
    "                  validation_data=None,\n",
    "                  callbacks=None  # include early stopping\n",
    "                  )\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(r_tau_hats, (len(X_test))) - tau_test) ** 2).mean()  #47.81"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DR-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = load_model('model_ex')\n",
    "dr_learner_e_x.fit(X_train, W_train,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test, W_test),\n",
    "                   callbacks=None  # include early stopping\n",
    "                   )\n",
    "\n",
    "dr_probabilities = np.reshape(keras.activations.sigmoid(dr_learner_e_x.predict(X_train)), len(X_train, ))\n",
    "dr_probas_0 = 1 - dr_probabilities  # probabilities of W=0\n",
    "dr_probas_1 = dr_probabilities  # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = load_model('model_25')\n",
    "dr_learner_mu_0.fit(X_train_control, Y_train_control,\n",
    "                    batch_size=100,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test_control, Y_test_control),\n",
    "                    callbacks=None  # include early stopping\n",
    "                    )\n",
    "\n",
    "dr_learner_mu_0_predictions = dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = load_model('model_25')\n",
    "dr_learner_mu_1.fit(X_train_treatment, Y_train_treatment,\n",
    "                    batch_size=100,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "                    callbacks=None  # include early stopping\n",
    "                    )\n",
    "\n",
    "dr_learner_mu_1_predictions = dr_learner_mu_1.predict(X_train)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1_predictions + (\n",
    "        1 - W_train) * dr_learner_mu_0_predictions  # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (\n",
    "        Y_train - mu_w) + dr_learner_mu_1_predictions - dr_learner_mu_0_predictions\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = load_model('model_25')\n",
    "dr_learner_tau_hat.fit(X_train, dr_pseudo_outcomes,\n",
    "                       batch_size=100,\n",
    "                       epochs=100,\n",
    "                       validation_data=None,\n",
    "                       callbacks=None  # include early stopping\n",
    "                       )\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc - tic} seconds')  # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(dr_tau_hat, (len(tau_test),)) - tau_test) ** 2).mean()  # 8.3514"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RA-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = load_model('model_25')\n",
    "ra_learner_mu0.fit(X_train_control, Y_train_control,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test_control, Y_test_control),\n",
    "                   callbacks=None  # include early stopping\n",
    "                   )\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu0_predictions = np.reshape(ra_learner_mu0.predict(X_train), (len(X_train),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = load_model('model_25')\n",
    "ra_learner_mu1.fit(X_train_treatment, Y_train_treatment,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test_treatment, Y_test_treatment),\n",
    "                   callbacks=None  # include early stopping\n",
    "                   )\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu1_predictions = np.reshape(ra_learner_mu1.predict(X_train), (len(X_train),))\n",
    "\n",
    "# e_x TODO: IS IT NEEDED?\n",
    "\"\"\"ra_learner_e_x = load_model('model_ex')\n",
    "ra_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train * (Y_train - ra_learner_mu0_predictions) + (1 - W_train) * (\n",
    "        ra_learner_mu1_predictions - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = load_model('model_25')\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome,\n",
    "                       batch_size=100,\n",
    "                       epochs=100,\n",
    "                       validation_data=None,\n",
    "                       callbacks=None  # include early stopping\n",
    "                       )\n",
    "\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(ra_tau_hat, (len(tau_test),)) - tau_test) ** 2).mean()  # 3.397"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PW-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = load_model('model_ex')\n",
    "pw_learner_e_x.fit(X_train, W_train,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test, W_test),\n",
    "                   callbacks=None  # include early stopping\n",
    "                   )\n",
    "\n",
    "pw_probabilities = np.reshape(keras.activations.sigmoid(pw_learner_e_x.predict(X_train)), len(X_train, ))\n",
    "pw_probs_1 = pw_probabilities\n",
    "pw_probs_0 = 1 - pw_probabilities\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train / pw_probs_1 - (1 - W_train) / pw_probs_0) * Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = load_model('model_25')\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome,\n",
    "                       batch_size=100,\n",
    "                       epochs=100,\n",
    "                       validation_data=None,\n",
    "                       callbacks=None  # include early stopping\n",
    "                       )\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(pw_tau_hat, (len(tau_test),)) - tau_test) ** 2).mean()  # 271.842 TODO: CHECK IF IT REALLY IS CORRECT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# see why so bad\n",
    "pw_probabilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U-learner (NN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "\n",
    "u_learner_e_x = load_model('model_ex')\n",
    "u_learner_e_x.fit(X_train, W_train,\n",
    "                  batch_size=100,\n",
    "                  epochs=1000,\n",
    "                  validation_split=0.3,\n",
    "                  callbacks=callback  # include early stopping\n",
    "                  )\n",
    "\n",
    "u_probs_1 = np.reshape(keras.activations.sigmoid(u_learner_e_x.predict(X_train)), (len(X_train),))\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = load_model('model_25')\n",
    "u_learner_mu_x.fit(X_train, Y_train,\n",
    "                   batch_size=100,\n",
    "                   epochs=100,\n",
    "                   validation_data=None,\n",
    "                   callbacks=None\n",
    "                   )\n",
    "\n",
    "u_learner_mu_x_predictions = np.reshape(u_learner_mu_x.predict(X_train), (len(X_train),))\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x_predictions) / (W_train - u_probs_1)\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = load_model('model_25')\n",
    "u_tau_hat_learner.fit(X_train, u_learner_residuals,\n",
    "                      batch_size=100,\n",
    "                      epochs=1000,\n",
    "                      callbacks=callback,\n",
    "                      validation_split=0.3\n",
    "                      )\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(u_tau_hats, (len(tau_test),)) - tau_test) ** 2).mean()  # 51.102 TODO: CHECK IF IT REALLY IS CORRECT!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "u_learner_residuals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(u_probs_1).describe()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Metalearner:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class T Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TLearner:  # TODO: comment what is what.\n",
    "    def __init__(self, method):  # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=1000, max_depth=99, random_state=0, max_features=0.66)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=1000, max_depth=99, random_state=0, max_features=0.66)\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):  # TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1])\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w == 1], y[w == 1])\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0],\n",
    "                               batch_size=100,\n",
    "                               epochs=10000,\n",
    "                               callbacks=callback,\n",
    "                               validation_split=0.3,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1],\n",
    "                               batch_size=100,\n",
    "                               epochs=10000,\n",
    "                               callbacks=callback,  # include early stopping\n",
    "                               validation_split=0.3,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self,\n",
    "                x):  # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x)\n",
    "            mu1_hats = self.mu1_model.predict(x)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x_poly_test)\n",
    "            mu1_hats = self.mu1_model.predict(x_poly_test)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            mu0_hats = self.mu0_model(x)\n",
    "            mu1_hats = self.mu1_model(x)\n",
    "            predictions = np.reshape(mu1_hats - mu0_hats, (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_nn = TLearner(method='nn')\n",
    "t_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = t_nn.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 10.091322530886687 / 7.359776707186481 (max_features=0.66)\n",
    "# lasso: 5.583461099392904\n",
    "# nn: 3.1867804239471273"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "stratified = StratifiedKFold(n_splits=CF_FOLDS, shuffle=True, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T16:48:40.418248Z",
     "start_time": "2023-07-22T16:48:40.381874Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for (train_index, test_index) in stratified.split(X_train, W_train):\n",
    "    temp_model = load_model('model_25')\n",
    "    print('New Fold')\n",
    "    temp_model.fit(X_train[train_index], Y_train[train_index], epochs=30)\n",
    "    print(temp_model.predict(X_train[test_index]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class S Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SLearner:  # TODO: comment what is what.\n",
    "    def __init__(self, method):  # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=2000, max_depth=100, random_state=0, max_features=0.66)\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_26')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):  # TODO: training process\n",
    "        x_w = np.concatenate((x, np.reshape(w, (len(w), 1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting random forest for mu_x\")\n",
    "            self.mux_model.fit(x_w, y)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x_w)\n",
    "\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting lasso for mu_x\")\n",
    "            self.mux_model.fit(x_poly_train, y)\n",
    "\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x_w = tf.convert_to_tensor(x_w)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "\n",
    "            # 1: train mu_x\n",
    "            print(\"Training neural network for mu_x\")\n",
    "            self.mux_model.fit(x_w, y,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,  # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self,\n",
    "                x):  # TODO:\n",
    "        x_0 = np.concatenate((x, np.zeros((len(x), 1))), axis=1)\n",
    "        x_1 = np.concatenate((x, np.ones((len(x), 1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_0)\n",
    "            mu1_hats = self.mux_model.predict(x_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_0 = self.poly.fit_transform(x_0)\n",
    "            x_poly_1 = self.poly.fit_transform(x_1)\n",
    "\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_poly_0)\n",
    "            mu1_hats = self.mux_model.predict(x_poly_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x_0 = tf.convert_to_tensor(x_0)\n",
    "            x_1 = tf.convert_to_tensor(x_1)\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model(x_0)\n",
    "            mu1_hats = self.mux_model(x_1)\n",
    "            predictions = np.reshape(mu1_hats - mu0_hats, (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_nn = SLearner('nn')\n",
    "s_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = s_nn.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 18.134009488483855\n",
    "# lasso: 5.559126710289806\n",
    "# nn: 1.987529792077956"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class X Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class XLearner:  # TODO: comment what is what.\n",
    "    def __init__(self, method):  # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.tau1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau0_model = load_model('model_25')\n",
    "            self.tau1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):  # TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0])\n",
    "            imputed_1 = y[w == 1] - self.mu0_model.predict(x[w == 1])\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1])\n",
    "            imputed_0 = self.mu1_model.predict(x[w == 0]) - y[w == 0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w == 0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w == 1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x, w)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w == 0], y[w == 0])\n",
    "            imputed_1 = y[w == 1] - self.mu0_model.predict(x_poly_train[w == 1])\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w == 1], y[w == 1])\n",
    "            imputed_0 = self.mu1_model.predict(x_poly_train[w == 0]) - y[w == 0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x_poly_train[w == 0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x_poly_train[w == 1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x_poly_train, w)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            imputed_0 = np.empty(len(x), )\n",
    "            imputed_1 = np.empty(len(x), )\n",
    "\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            for train_index, test_index in stratified.split(x, w):\n",
    "                index = np.zeros(len(x), dtype=bool)\n",
    "                index[test_index] = 1\n",
    "\n",
    "                temp_model = load_model('model_25')\n",
    "\n",
    "                x_train = x[~index]\n",
    "                x_test = x[index]\n",
    "                w_train = w[~index]\n",
    "                w_test = w[index]\n",
    "                y_train = y[~index]\n",
    "                y_test = y[index]\n",
    "\n",
    "                temp_model.fit(x_train[w_train == 0], y_train[w_train == 0],\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "                imputed_1[index][w_test == 1] = y_test[w_test == 1] - np.reshape(temp_model(x_test[w_test == 1]),\n",
    "                                                                                 (len(x_test[w_test == 1]),))\n",
    "\n",
    "            imputed_1 = tf.convert_to_tensor(imputed_1)\n",
    "\n",
    "            \"\"\"self.mu0_model.fit(x[w == 0], y[w == 0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,  # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_1 = y[w == 1] - np.reshape(self.mu0_model(x[w == 1]), (len(x[w == 1]),))\"\"\"\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "\n",
    "            for train_index, test_index in stratified.split(x, w):\n",
    "                index = np.zeros(len(x), dtype=bool)\n",
    "                index[test_index] = 1\n",
    "\n",
    "                temp_model = load_model('model_25')\n",
    "                x_train = x[~index]\n",
    "                x_test = x[index]\n",
    "                w_train = w[~index]\n",
    "                w_test = w[index]\n",
    "                y_train = y[~index]\n",
    "\n",
    "                temp_model.fit(x_train[w_train == 1], y_train[w_train == 1],\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "                imputed_0[index][w_test == 0] = np.array(temp_model(x_test[w_test == 0])).squeeze() - y_test[\n",
    "                    w_test == 0]\n",
    "\n",
    "            imputed_0 = tf.convert_to_tensor(imputed_0)\n",
    "\n",
    "            \"\"\"self.mu1_model.fit(x[w == 1], y[w == 1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,  # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_0 = np.reshape(self.mu1_model(x[w == 0]), (len(x[w == 0]),)) - y[w == 0]\"\"\"\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w == 0], imputed_0[w == 0],\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None,  # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w == 1], imputed_1[w == 1],\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None,  # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x, w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,  # include early stopping\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self,\n",
    "                x):  # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x)\n",
    "            tau_1_hats = self.tau1_model.predict(x)\n",
    "            # 2: probabilities\n",
    "            probs = self.ex_model.predict_proba(x)[:, 1]\n",
    "            # 3: final predictions\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x_poly_test)\n",
    "            tau_1_hats = self.tau1_model.predict(x_poly_test)\n",
    "            probs = self.ex_model.predict_proba(x_poly_test)[:, 1]\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = np.reshape(self.tau0_model(x), (len(x),))\n",
    "            tau_1_hats = np.reshape(self.tau1_model(x), (len(x),))\n",
    "            # 2: probabilities\n",
    "            logit = self.ex_model(x)\n",
    "            probs = np.reshape(keras.activations.sigmoid(logit), (len(logit, )))\n",
    "            # 3: final predictions\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        predictions = probs * tau_0_hats + (1 - probs) * tau_1_hats\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:00:53.332769Z",
     "start_time": "2023-07-22T17:00:53.297925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network for mu_0\n",
      "Training neural network for mu_1\n",
      "Fitting random forest for tau_0\n",
      "Fitting random forest for tau_1\n",
      "Fitting random forest for e_x\n"
     ]
    },
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rf = XLearner('nn')\n",
    "x_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = x_rf.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: # 3.1369636408859614 --> same (good)\n",
    "# lasso: # nn: 7.667219448077926 --> same (good)\n",
    "# nn: 3.161416602361538 --> same (good)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:01:02.868448Z",
     "start_time": "2023-07-22T17:00:53.655049Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T17:01:10.578155Z",
     "start_time": "2023-07-22T17:01:10.543911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imputed_1 = np.empty(len(X_train), )\n",
    "\n",
    "for train_index, test_index in stratified.split(X_train, W_train):\n",
    "    index = np.zeros(len(X_train), dtype=bool)\n",
    "    index[test_index] = 1\n",
    "\n",
    "    temp_model = load_model('model_25')\n",
    "\n",
    "    x_train = X_train[~index]\n",
    "    x_test = X_train[index]\n",
    "    w_train = W_train[~index]\n",
    "    w_test = W_train[index]\n",
    "    y_train = Y_train[~index]\n",
    "    y_test = Y_train[index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model(x_test[w_test == 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test[w_test == 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = np.zeros(len(X_train), dtype=bool)\n",
    "for train_index, test_index in stratified.split(X_train, W_train):\n",
    "    index[test_index] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stratified = StratifiedKFold(n_splits=CF_FOLDS, shuffle=True, random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for (train_index, test_index) in stratified.split(X_train, W_train):\n",
    "    temp_model = load_model('model_25')\n",
    "    print('New Fold')\n",
    "    temp_model.fit(X_train[train_index], Y_train[train_index], epochs=30)\n",
    "    print(temp_model.predict(X_train[test_index]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.experimental.numpy.empty(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RLearner:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            self.mux_model.fit(x, y)\n",
    "\n",
    "            print('Fitting random forest for e_x')\n",
    "            # 2: fit ex\n",
    "            self.ex_model.fit(x, w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x)[:, 1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x)) / (w - probs)\n",
    "            weights = (w - probs) ** 2\n",
    "\n",
    "            print('Fitting random forest for tau_x')\n",
    "            # 4: fit tau\n",
    "            self.tau_model.fit(x, pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting lasso for mu_x')\n",
    "            self.mux_model.fit(x_poly_train, y)\n",
    "\n",
    "            # 2: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train, w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:, 1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x_poly_train)) / (w - probs)\n",
    "            weights = (w - probs) ** 2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train, pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Training NN for mu_x')\n",
    "            self.mux_model.fit(x, y,\n",
    "                               batch_size=100,\n",
    "                               epochs=400,\n",
    "                               callbacks=callback,\n",
    "                               validation_split=0.3,\n",
    "                               verbose=0\n",
    "                               )\n",
    "            # 2: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x, w,\n",
    "                              batch_size=100,\n",
    "                              epochs=400,\n",
    "                              callbacks=callback,\n",
    "                              validation_split=0.3,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model(x)), len(x, ))\n",
    "            pseudo_outcomes = (y - np.reshape(self.mux_model(x), (len(x),))) / (w - probs)\n",
    "            weights = (w - probs) ** 2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes,\n",
    "                               sample_weight=weights,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=callback,\n",
    "                               validation_split=0.3,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            predictions = np.reshape(self.tau_model(x), (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_rf = RLearner('nn')\n",
    "r_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = r_rf.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 17.722925118749608\n",
    "# lasso: 5.50038865455844\n",
    "# nn: 47.81939839016621"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DRLearner:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting random forest for mu_0')\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting random forest for mu_1')\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x, w)\n",
    "            probs = self.ex_model.predict_proba(x)[:, 1]\n",
    "            neg_prob = self.ex_model.predict_proba(x)[:, 0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x) + (1 - w) * self.mu0_model.predict(x)\n",
    "            pseudo_outcomes = (w - probs) / (probs * neg_prob) * (y - mu_w) + self.mu1_model.predict(\n",
    "                x) - self.mu0_model.predict(x)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting lasso for mu_0')\n",
    "            self.mu0_model.fit(x_poly_train[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting lasso for mu_1')\n",
    "            self.mu1_model.fit(x_poly_train[w == 1], y[w == 1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train, w)\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:, 1]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x_poly_train) + (1 - w) * self.mu0_model.predict(x_poly_train)\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + self.mu1_model.predict(\n",
    "                x_poly_train) - self.mu0_model.predict(x_poly_train)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Training NN for mu_0')\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Training NN for mu_1')\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x, w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            probs = tf.reshape(keras.activations.sigmoid(self.ex_model(x)), len(x, ))\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_0_hats = self.mu0_model(x)\n",
    "            mu_1_hats = self.mu1_model(x)\n",
    "\n",
    "            mu_w = w * mu_1_hats + (1 - w) * mu_0_hats\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + mu_1_hats - mu_0_hats\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            # predict\n",
    "            predictions = np.reshape(self.tau_model(x), (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dr_rf = DRLearner('nn')\n",
    "dr_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = dr_rf.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 5.385491721300538 # why different??? ---> because if you take 1 - probs its not exactly the same as taking the [:,0] column!!\n",
    "# lasso: 6.252082321980517\n",
    "# nn: 8.35142898943478"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHECK THIS: CHANGE (1 - PROBS) TO [:,0] TO BE MORE EXACT!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RALearner:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting random forest for mu_0')\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting random forest for mu_1')\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1])\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = w * (y - self.mu0_model.predict(x)) + (1 - w) * (self.mu1_model.predict(x) - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting lasso for mu_0')\n",
    "            self.mu0_model.fit(x_poly_train[w == 0], y[w == 0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting lasso for mu_1')\n",
    "            self.mu1_model.fit(x_poly_train[w == 1], y[w == 1])\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = w * (y - self.mu0_model.predict(x_poly_train)) + (1 - w) * (\n",
    "                    self.mu1_model.predict(x_poly_train) - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Training NN for mu_0')\n",
    "            self.mu0_model.fit(x[w == 0], y[w == 0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Training NN for mu_1')\n",
    "            self.mu1_model.fit(x[w == 1], y[w == 1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu0_predictions = np.reshape(self.mu0_model(x), (len(x),))\n",
    "            mu1_predictions = np.reshape(self.mu1_model(x), (len(x),))\n",
    "\n",
    "            pseudo_outcomes = w * (y - mu0_predictions) + (1 - w) * (mu1_predictions - y)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            # predict\n",
    "            predictions = np.reshape(self.tau_model(x), (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ra_rf = RALearner('nn')\n",
    "ra_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = ra_rf.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 5.355494017645751\n",
    "# lasso: 8.283890654355236\n",
    "# nn: 3.3973654461530867\n",
    "\n",
    "# 3.353439795888397\n",
    "# 3.3534397958883955"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class PWLearner:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.ex_1_model = load_model('model_ex')\n",
    "            self.ex_2_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x, w)\n",
    "            probs = self.ex_model.predict_proba(x)[:, 1]\n",
    "            counter_probs = self.ex_model.predict_proba(x)[:, 0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = (w / probs - (1 - w) / counter_probs) * y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train, w)\n",
    "\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:, 1]\n",
    "            counter_probs = self.ex_model.predict_proba(x_poly_train)[:, 0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = (w / probs - (1 - w) / counter_probs) * y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train, pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # cross-fitting\n",
    "            # split for cross-fitting\n",
    "            index = np.zeros(len(x), dtype=bool)\n",
    "            train_ind = np.random.choice(len(x), int(len(x) / 2), replace=False)\n",
    "            index[train_ind] = 1\n",
    "\n",
    "            probs = np.zeros(len(x), )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "\n",
    "            self.ex_1_model.fit(x[index], w[index],\n",
    "                                batch_size=100,\n",
    "                                epochs=50,\n",
    "                                callbacks=None,\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            self.ex_2_model.fit(x[~index], w[~index],\n",
    "                                batch_size=100,\n",
    "                                epochs=50,\n",
    "                                callbacks=None,\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            probs[index] = tf.squeeze(keras.activations.sigmoid(self.ex_1_model(x[index])))\n",
    "            probs[~index] = tf.squeeze(keras.activations.sigmoid(self.ex_2_model(x[~index])))\n",
    "\n",
    "            # probs = tf.squeeze(keras.activations.sigmoid(self.ex_model(x)))\n",
    "            counter_probs = 1 - probs\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            pseudo_outcomes = (w / probs - (1 - w) / counter_probs) * y\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x, pseudo_outcomes,\n",
    "                               batch_size=100,\n",
    "                               epochs=50,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            predictions = np.array(self.tau_model(x)).squeeze()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T23:30:34.511437Z",
     "start_time": "2023-07-18T23:30:34.491111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting lasso for e_x\n",
      "Fitting lasso for tau_x\n"
     ]
    },
    {
     "data": {
      "text/plain": "16.03059004204301"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_rf = PWLearner('lasso')\n",
    "pw_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = pw_rf.predict(X_test)\n",
    "((predictions - tau_test) ** 2).mean()\n",
    "# rf: 30.529802728890644\n",
    "# lasso: 16.03059004204301\n",
    "# nn: 271.8425349295992\n",
    "\n",
    "# 238.41087366274616"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T23:31:20.453447Z",
     "start_time": "2023-07-18T23:31:11.393973Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class ULearner:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mux_1_model = load_model('model_25')\n",
    "            self.mux_2_model = load_model('model_25')\n",
    "            self.ex_1_model = load_model('model_ex')\n",
    "            self.ex_2_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def compute_hats(self, x_train, y_train, w_train, x_test):\n",
    "        # 1: fit mu_x\n",
    "        print('Fitting random forest for mu_x')\n",
    "        temp_mux = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "        temp_mux.fit(x_train, y_train)\n",
    "        # 2: fit ex\n",
    "        temp_ex = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "        temp_ex.fit(x_train, w_train)\n",
    "        # residuals\n",
    "        probs = temp_ex.predict_proba(x_test)[:, 1]\n",
    "        mux_hat = temp_mux.predict(x_test)\n",
    "        return mux_hat, probs\n",
    "\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            if CF_FOLDS == 1:\n",
    "                mux_hat, probs = self.compute_hats(x, y, w, x)\n",
    "\n",
    "            else:\n",
    "                # initialize\n",
    "                mux_hat = np.zeros(700)\n",
    "                probs = np.zeros(700)\n",
    "                # cross-fitting\n",
    "                stratified = StratifiedKFold(n_splits=2, shuffle=True, random_state=0)\n",
    "                for train_index, test_index in stratified.split(x, w):\n",
    "                    index = np.zeros(700, dtype=bool)\n",
    "                    index[test_index] = 1\n",
    "                    print('Fitting Classifier')\n",
    "                    \"\"\"\n",
    "                    temp_ex = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "                    temp_ex.fit(x[index], w[index])\n",
    "                    temp_mux = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "                    temp_mux.fit(x[index], y[index])\n",
    "                    probs[~index] = temp_ex.predict_proba(x[~index])[:, 1]\n",
    "                    mux_hat[~index] = temp_mux.predict(x[~index])\n",
    "                    \"\"\"\n",
    "                    mux_hat[~index], probs[~index] =  self.compute_hats(x[index], y[index], w[index], x[~index] )\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            temp_mux = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            temp_mux.fit(x, y)\n",
    "            # 2: fit ex\n",
    "            temp_ex = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            temp_ex.fit(x, w)\n",
    "            # residuals\n",
    "            probs = temp_ex.predict_proba(x)[:, 1]\n",
    "            mux_hat = temp_mux.predict(x)\n",
    "            \"\"\"\n",
    "            residuals = (y - mux_hat) / (w - probs)\n",
    "            # 3: fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x, residuals)\n",
    "\n",
    "            \"\"\"\n",
    "            # 2: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            self.mux_model.fit(x, y)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x, w)\n",
    "            probs = self.ex_model.predict_proba(x)[:, 1]\n",
    "\n",
    "            # calculate residuals\n",
    "            residuals = (y - self.mux_model.predict(x)) / (w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x, residuals)\n",
    "            \"\"\"\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 2: fit mu_x\n",
    "            print('Fitting lasso for mu_x')\n",
    "            self.mux_model.fit(x_poly_train, y)\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train, w)\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:, 1]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            residuals = (y - self.mux_model.predict(x_poly_train)) / (w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train, residuals)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            y = tf.convert_to_tensor(y)\n",
    "            w = tf.convert_to_tensor(w)\n",
    "\n",
    "            # cross-fitting\n",
    "            index = np.zeros(len(x), dtype=bool)\n",
    "            train_ind = np.random.choice(len(x), int(len(x) / 2), replace=False)\n",
    "            index[train_ind] = 1\n",
    "\n",
    "            probs = np.zeros(len(x), )\n",
    "            mu_x_predictions = np.zeros(len(x), )\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Training NN for mu_x')\n",
    "            self.mux_1_model.fit(x[index], y[index],\n",
    "                                 batch_size=100,\n",
    "                                 epochs=50,\n",
    "                                 callbacks=None,\n",
    "                                 verbose=0\n",
    "                                 )\n",
    "\n",
    "            self.mux_2_model.fit(x[~index], y[~index],\n",
    "                                 batch_size=100,\n",
    "                                 epochs=50,\n",
    "                                 callbacks=None,\n",
    "                                 verbose=0\n",
    "                                 )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_1_model.fit(x[index], w[index],\n",
    "                                batch_size=100,\n",
    "                                epochs=50,\n",
    "                                callbacks=None,\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            self.ex_2_model.fit(x[~index], w[~index],\n",
    "                                batch_size=100,\n",
    "                                epochs=50,\n",
    "                                callbacks=None,\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            probs[~index] = tf.reshape(keras.activations.sigmoid(self.ex_2_model(x[~index])), len(x[~index], ))\n",
    "            probs[index] = tf.reshape(keras.activations.sigmoid(self.ex_1_model(x[index])), len(x[index], ))\n",
    "\n",
    "            mu_x_predictions[index] = tf.reshape(self.mux_1_model(x[index]), (len(x[index]),))\n",
    "            mu_x_predictions[~index] = tf.reshape(self.mux_2_model(x[~index]), (len(x[~index]),))\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            probs = tf.reshape(keras.activations.sigmoid(self.ex_model(x)), len(x, ))\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_x_predictions = tf.reshape(self.mux_model(x), (len(x),))\n",
    "\n",
    "            \"\"\"\n",
    "            residuals = (y - mu_x_predictions) / (w - probs)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x, residuals,\n",
    "                               batch_size=100,\n",
    "                               epochs=50,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # to tensor\n",
    "            x = tf.convert_to_tensor(x)\n",
    "            # predict\n",
    "            predictions = np.reshape(self.tau_model(x), (len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T02:09:00.126772Z",
     "start_time": "2023-07-24T02:09:00.096563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN for mu_x\n",
      "Training NN for e_x\n",
      "Training NN for tau_x\n"
     ]
    },
    {
     "data": {
      "text/plain": "2.086929293352275"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_rf = ULearner('nn')\n",
    "u_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = u_rf.predict(X_test)\n",
    "np.sqrt((((predictions - tau_test) ** 2).mean()))\n",
    "# rf: 30.921286420155806\n",
    "# lasso: 7.6762472449663495\n",
    "# nn: 51.10236987028663\n",
    "\n",
    "# 31.033895213307428\n",
    "# 31.033895213307698"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T02:12:00.034328Z",
     "start_time": "2023-07-24T02:11:55.714890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "CF_FOLDS = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T02:09:05.478789Z",
     "start_time": "2023-07-24T02:09:05.449223Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "try stratifiedfolds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Classifier\n",
      "Fitting Classifier\n",
      "Fitting Classifier\n",
      "Fitting Classifier\n"
     ]
    }
   ],
   "source": [
    "# TODO: THATS IT!!!!!\n",
    "w_hats = np.zeros(700)\n",
    "\n",
    "stratified = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "for train_index, test_index in stratified.split(X_train, W_train):\n",
    "    index = np.zeros(700, dtype=bool)\n",
    "    index[test_index] = 1\n",
    "    print('Fitting Classifier')\n",
    "    temp_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "    temp_model.fit(X_train[index], W_train[index])\n",
    "    w_hats[~index] = temp_model.predict_proba(X_train[~index])[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:48:40.703063Z",
     "start_time": "2023-07-24T01:48:40.436063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True False  True  True False  True  True False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False  True  True False False  True False False False\n",
      " False False False False False False False  True  True  True False False\n",
      "  True  True  True False False False False False  True False False False\n",
      " False  True False False False False  True  True  True False  True False\n",
      " False False False False False  True False  True False False False False\n",
      " False  True False  True False False False False False False False False\n",
      " False False False False  True False False False False  True  True  True\n",
      " False False False  True  True  True  True False False  True False False\n",
      " False False False  True False False False  True False False  True  True\n",
      "  True False False False False False False False False False  True False\n",
      " False False  True False False False  True False False False False False\n",
      "  True  True False False False False False  True False False False False\n",
      " False False  True  True False False  True  True False False False  True\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True False  True  True False False False  True\n",
      "  True  True False  True  True False False False False False False False\n",
      " False False  True False False False False False False False False  True\n",
      "  True False  True False False False False False False  True False False\n",
      "  True False False  True  True False  True False False False False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False  True False False False False False  True  True False False  True\n",
      "  True False  True  True False False False False False False False False\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False  True False False False False False False False False\n",
      " False False False  True  True  True False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      "  True  True False False False  True  True False False  True False False\n",
      " False False False False  True False False  True False False False False\n",
      "  True  True False False False False False  True False  True False False\n",
      " False False False False False  True False False  True False False  True\n",
      " False  True False False  True False False False False False False False\n",
      " False  True False False False False False  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False  True False False False False False False\n",
      " False False  True False False False False False  True  True False  True\n",
      " False False False False False False False  True  True False False  True\n",
      " False False False False False False False  True  True  True False  True\n",
      " False False  True  True  True False  True False  True False  True  True\n",
      " False False  True False False False False False False False False False\n",
      " False False False False  True False False False False  True  True False\n",
      " False False  True  True  True False  True False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      "  True False False False False False  True False  True False False False\n",
      " False False  True False False  True False False  True False  True False\n",
      " False False False  True  True False False False False False False False\n",
      " False  True False  True False  True False False False False False False\n",
      " False  True  True False False False False False  True False  True False\n",
      " False False False False  True False False False False False False  True\n",
      " False False False False False  True  True False False False False  True\n",
      " False False False False False False  True  True False False False False\n",
      "  True False False False  True  True False  True False False False False\n",
      " False  True False False False  True  True  True False False False False\n",
      " False False False False  True False False False False  True False False\n",
      " False  True False False]\n",
      "[False False False  True False False  True False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True False False  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False False False  True  True\n",
      " False False False  True  True  True  True  True False  True  True  True\n",
      "  True False  True  True  True  True False False False  True False  True\n",
      "  True  True  True  True  True False  True False  True  True  True  True\n",
      "  True False  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True False False False\n",
      "  True  True  True False False False False  True  True False  True  True\n",
      "  True  True  True False  True  True  True False  True  True False False\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True  True  True False  True  True  True  True  True\n",
      " False False  True  True  True  True  True False  True  True  True  True\n",
      "  True  True False False  True  True False False  True  True  True False\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False  True False False  True  True  True False\n",
      " False False  True False False  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True False\n",
      " False  True False  True  True  True  True  True  True False  True  True\n",
      " False  True  True False False  True False  True  True  True  True  True\n",
      "  True  True False False  True  True False  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False False  True  True False\n",
      " False  True False False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True  True  True False\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True  True False False  True  True False  True  True\n",
      "  True  True  True  True False  True  True False  True  True  True  True\n",
      " False False  True  True  True  True  True False  True False  True  True\n",
      "  True  True  True  True  True False  True  True False  True  True False\n",
      "  True False  True  True False  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True False False  True False\n",
      "  True  True  True  True  True  True  True False False  True  True False\n",
      "  True  True  True  True  True  True  True False False False  True False\n",
      "  True  True False False False  True False  True False  True False False\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True False False  True\n",
      "  True  True False False False  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True False  True False  True  True  True\n",
      "  True  True False  True  True False  True  True False  True False  True\n",
      "  True  True  True False False  True  True  True  True  True  True  True\n",
      "  True False  True False  True False  True  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True False  True False  True\n",
      "  True  True  True  True False  True  True  True  True  True  True False\n",
      "  True  True  True  True  True False False  True  True  True  True False\n",
      "  True  True  True  True  True  True False False  True  True  True  True\n",
      " False  True  True  True False False  True False  True  True  True  True\n",
      "  True False  True  True  True False False False  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True False  True  True\n",
      "  True False  True  True]\n",
      "tf.Tensor(\n",
      "[ True  True  True False  True  True False  True  True False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False  True  True False False  True False False False\n",
      " False False False False False False False  True  True  True False False\n",
      "  True  True  True False False False False False  True False False False\n",
      " False  True False False False False  True  True  True False  True False\n",
      " False False False False False  True False  True False False False False\n",
      " False  True False  True False False False False False False False False\n",
      " False False False False  True False False False False  True  True  True\n",
      " False False False  True  True  True  True False False  True False False\n",
      " False False False  True False False False  True False False  True  True\n",
      "  True False False False False False False False False False  True False\n",
      " False False  True False False False  True False False False False False\n",
      "  True  True False False False False False  True False False False False\n",
      " False False  True  True False False  True  True False False False  True\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True False  True  True False False False  True\n",
      "  True  True False  True  True False False False False False False False\n",
      " False False  True False False False False False False False False  True\n",
      "  True False  True False False False False False False  True False False\n",
      "  True False False  True  True False  True False False False False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False  True False False False False False  True  True False False  True\n",
      "  True False  True  True False False False False False False False False\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False  True False False False False False False False False\n",
      " False False False  True  True  True False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      "  True  True False False False  True  True False False  True False False\n",
      " False False False False  True False False  True False False False False\n",
      "  True  True False False False False False  True False  True False False\n",
      " False False False False False  True False False  True False False  True\n",
      " False  True False False  True False False False False False False False\n",
      " False  True False False False False False  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False  True False False False False False False\n",
      " False False  True False False False False False  True  True False  True\n",
      " False False False False False False False  True  True False False  True\n",
      " False False False False False False False  True  True  True False  True\n",
      " False False  True  True  True False  True False  True False  True  True\n",
      " False False  True False False False False False False False False False\n",
      " False False False False  True False False False False  True  True False\n",
      " False False  True  True  True False  True False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      "  True False False False False False  True False  True False False False\n",
      " False False  True False False  True False False  True False  True False\n",
      " False False False  True  True False False False False False False False\n",
      " False  True False  True False  True False False False False False False\n",
      " False  True  True False False False False False  True False  True False\n",
      " False False False False  True False False False False False False  True\n",
      " False False False False False  True  True False False False False  True\n",
      " False False False False False False  True  True False False False False\n",
      "  True False False False  True  True False  True False False False False\n",
      " False  True False False False  True  True  True False False False False\n",
      " False False False False  True False False False False  True False False\n",
      " False  True False False], shape=(700,), dtype=bool)\n",
      "[False False False False False False False False False  True False False\n",
      " False  True False  True False  True False False False False False  True\n",
      " False False  True  True False False  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True  True False False False  True False  True\n",
      " False  True  True False False False False False False False  True False\n",
      " False False False False  True  True False  True False False False False\n",
      " False False False  True False False False  True False False False False\n",
      " False False False False False False False  True  True False False  True\n",
      " False  True False False False False  True False False False False False\n",
      " False  True False False  True False False False False  True False  True\n",
      "  True  True False False  True False False  True  True False  True False\n",
      " False False False False False  True  True False False  True  True False\n",
      "  True False False False  True False False False False False  True False\n",
      " False False False False  True False False False False False  True  True\n",
      "  True False False  True False False False False False False  True False\n",
      " False False False False False  True False False  True False False False\n",
      " False False  True False False False False False False False False False\n",
      "  True False False False  True False False False False  True False False\n",
      " False False False False False  True False False False False  True  True\n",
      " False False False False False False False False False False False False\n",
      "  True  True False False False False False  True False False False False\n",
      "  True False  True False  True False False False False False  True False\n",
      " False False False False False False False False  True False False False\n",
      " False False False  True  True False False False  True False False False\n",
      "  True False False False  True False  True  True  True False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False  True False False  True  True  True False  True False False\n",
      " False False  True False  True False False False False False False False\n",
      " False False False False False False  True False False  True  True  True\n",
      " False False False False  True False False False False False False  True\n",
      "  True  True  True  True False False False False False  True  True False\n",
      " False False False False False  True False  True False False False  True\n",
      " False False  True False  True False False False False False False False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False  True  True False False False False False  True False False\n",
      " False False False  True False False  True  True False False False False\n",
      "  True False False False  True False False False False False False False\n",
      " False False  True False  True False False False False False  True False\n",
      "  True False False False False False False False False False False False\n",
      "  True False False False False False False False False  True  True  True\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False False False  True False  True False False  True False\n",
      " False False  True False  True  True  True False False  True False False\n",
      " False False  True  True False False False False  True False False  True\n",
      " False  True  True  True False False False  True False False False False\n",
      " False False  True False False False False  True False  True False False\n",
      "  True False False False False False  True False False  True False False\n",
      " False  True False False False  True False False False  True False  True\n",
      " False False  True False False False False False  True  True False  True\n",
      "  True False False False False  True False False False  True False False\n",
      "  True False  True  True False False False False  True False False False\n",
      "  True False False False  True False False False False  True False False\n",
      "  True False False  True False False False False False False False False\n",
      " False False  True False False False  True False  True False  True  True\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False  True False False False  True False\n",
      "  True False  True  True]\n",
      "[ True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True False  True False  True False  True  True  True  True  True False\n",
      "  True  True False False  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True False False  True  True  True False  True False\n",
      "  True False False  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False False  True False  True  True  True  True\n",
      "  True  True  True False  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True False\n",
      "  True False  True  True  True  True False  True  True  True  True  True\n",
      "  True False  True  True False  True  True  True  True False  True False\n",
      " False False  True  True False  True  True False False  True False  True\n",
      "  True  True  True  True  True False False  True  True False False  True\n",
      " False  True  True  True False  True  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True  True  True  True False False\n",
      " False  True  True False  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True False  True  True False  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True  True  True  True False  True  True  True  True\n",
      " False  True False  True False  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True False False  True  True  True False  True  True  True\n",
      " False  True  True  True False  True False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True False  True  True False False False  True False  True  True\n",
      "  True  True False  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True False False False\n",
      "  True  True  True  True False  True  True  True  True  True  True False\n",
      " False False False False  True  True  True  True  True False False  True\n",
      "  True  True  True  True  True False  True False  True  True  True False\n",
      "  True  True False  True False  True  True  True  True  True  True  True\n",
      "  True False  True  True  True False False  True  True  True  True  True\n",
      "  True  True False False  True  True  True  True  True False  True  True\n",
      "  True  True  True False  True  True False False  True  True  True  True\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True False  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True False False False\n",
      " False  True  True  True  True  True  True False  True  True  True False\n",
      "  True  True  True  True  True False  True False  True  True False  True\n",
      "  True  True False  True False False False  True  True False  True  True\n",
      "  True  True False False  True  True  True  True False  True  True False\n",
      "  True False False False  True  True  True False  True  True  True  True\n",
      "  True  True False  True  True  True  True False  True False  True  True\n",
      " False  True  True  True  True  True False  True  True False  True  True\n",
      "  True False  True  True  True False  True  True  True False  True False\n",
      "  True  True False  True  True  True  True  True False False  True False\n",
      " False  True  True  True  True False  True  True  True False  True  True\n",
      " False  True False False  True  True  True  True False  True  True  True\n",
      " False  True  True  True False  True  True  True  True False  True  True\n",
      " False  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True False  True False  True False False\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True False  True\n",
      " False  True False False]\n",
      "tf.Tensor(\n",
      "[False False False False False False False False False  True False False\n",
      " False  True False  True False  True False False False False False  True\n",
      " False False  True  True False False  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True  True False False False  True False  True\n",
      " False  True  True False False False False False False False  True False\n",
      " False False False False  True  True False  True False False False False\n",
      " False False False  True False False False  True False False False False\n",
      " False False False False False False False  True  True False False  True\n",
      " False  True False False False False  True False False False False False\n",
      " False  True False False  True False False False False  True False  True\n",
      "  True  True False False  True False False  True  True False  True False\n",
      " False False False False False  True  True False False  True  True False\n",
      "  True False False False  True False False False False False  True False\n",
      " False False False False  True False False False False False  True  True\n",
      "  True False False  True False False False False False False  True False\n",
      " False False False False False  True False False  True False False False\n",
      " False False  True False False False False False False False False False\n",
      "  True False False False  True False False False False  True False False\n",
      " False False False False False  True False False False False  True  True\n",
      " False False False False False False False False False False False False\n",
      "  True  True False False False False False  True False False False False\n",
      "  True False  True False  True False False False False False  True False\n",
      " False False False False False False False False  True False False False\n",
      " False False False  True  True False False False  True False False False\n",
      "  True False False False  True False  True  True  True False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False  True False False  True  True  True False  True False False\n",
      " False False  True False  True False False False False False False False\n",
      " False False False False False False  True False False  True  True  True\n",
      " False False False False  True False False False False False False  True\n",
      "  True  True  True  True False False False False False  True  True False\n",
      " False False False False False  True False  True False False False  True\n",
      " False False  True False  True False False False False False False False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False  True  True False False False False False  True False False\n",
      " False False False  True False False  True  True False False False False\n",
      "  True False False False  True False False False False False False False\n",
      " False False  True False  True False False False False False  True False\n",
      "  True False False False False False False False False False False False\n",
      "  True False False False False False False False False  True  True  True\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False False False  True False  True False False  True False\n",
      " False False  True False  True  True  True False False  True False False\n",
      " False False  True  True False False False False  True False False  True\n",
      " False  True  True  True False False False  True False False False False\n",
      " False False  True False False False False  True False  True False False\n",
      "  True False False False False False  True False False  True False False\n",
      " False  True False False False  True False False False  True False  True\n",
      " False False  True False False False False False  True  True False  True\n",
      "  True False False False False  True False False False  True False False\n",
      "  True False  True  True False False False False  True False False False\n",
      "  True False False False  True False False False False  True False False\n",
      "  True False False  True False False False False False False False False\n",
      " False False  True False False False  True False  True False  True  True\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False  True False False False  True False\n",
      "  True False  True  True], shape=(700,), dtype=bool)\n",
      "[False False False  True False False  True False False False  True  True\n",
      "  True False  True False False False  True False False False  True False\n",
      "  True False False False False False False False False  True False  True\n",
      " False False  True  True False  True False False False False False  True\n",
      " False False False  True False False  True  True False False False  True\n",
      "  True False  True False False False False False False False False False\n",
      "  True False False  True  True False  True False False False False  True\n",
      "  True False  True False False False  True False False False  True False\n",
      " False False  True False False False  True False False False False False\n",
      "  True  True  True False False False False False False False  True False\n",
      " False False  True False  True  True False False  True  True False False\n",
      " False False  True False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False  True False False  True  True False False\n",
      " False  True  True  True False False False False False False False False\n",
      " False  True  True False  True  True False  True  True  True False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False  True  True False  True False False False\n",
      " False False False False  True False  True False  True False False False\n",
      " False  True  True False False False False  True  True False  True  True\n",
      " False False False False False False False False False  True  True False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False  True False  True  True False False False False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False  True False False  True False False False False False False\n",
      "  True  True  True False False False  True False  True  True False False\n",
      "  True False False  True  True False False False  True False False False\n",
      " False False False False False False False  True  True False  True False\n",
      " False False False False False False False False  True False False False\n",
      " False False  True False False  True  True False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False  True False False  True False False  True False False\n",
      "  True False False False False False  True False False False  True False\n",
      "  True False  True  True False False False  True False  True  True False\n",
      " False False False False False False  True  True  True False False False\n",
      "  True False False False  True False False False False False False False\n",
      " False  True  True  True False False  True False False False  True False\n",
      " False False False  True False False  True False False False False False\n",
      " False  True False False False False False  True False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False  True False False False False  True False  True False False False\n",
      " False  True False False False False False False  True False False  True\n",
      " False  True False False False False False False  True False  True False\n",
      " False False False False  True  True  True  True False False  True False\n",
      " False False False False  True  True False False False False  True  True\n",
      " False  True False False False  True False False False False False False\n",
      " False False False  True False False False  True False False False False\n",
      " False False  True False False False  True  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False  True False False  True  True False False False False\n",
      " False  True False False False  True False False False False False False\n",
      " False False  True False False False False False False False  True False\n",
      " False  True False False  True  True False False  True False  True False\n",
      " False  True False False False False False False False False False False\n",
      " False False  True False False False False False False False  True False\n",
      "  True False False  True False  True False  True False False False  True\n",
      " False False False False]\n",
      "[ True  True  True False  True  True False  True  True  True False False\n",
      " False  True False  True  True  True False  True  True  True False  True\n",
      " False  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True False False  True False  True  True  True  True  True False\n",
      "  True  True  True False  True  True False False  True  True  True False\n",
      " False  True False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True False False  True False  True  True  True  True False\n",
      " False  True False  True  True  True False  True  True  True False  True\n",
      "  True  True False  True  True  True False  True  True  True  True  True\n",
      " False False False  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True False False  True  True False False  True  True\n",
      "  True  True False  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True False False  True  True\n",
      "  True False False False  True  True  True  True  True  True  True  True\n",
      "  True False False  True False False  True False False False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False False  True False  True  True  True\n",
      "  True  True  True  True False  True False  True False  True  True  True\n",
      "  True False False  True  True  True  True False False  True False False\n",
      "  True  True  True  True  True  True  True  True  True False False  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True False  True  True False  True False False  True  True  True  True\n",
      "  True False  True  True  True False False  True  True  True  True  True\n",
      "  True  True False  True  True False  True  True  True  True  True  True\n",
      " False False False  True  True  True False  True False False  True  True\n",
      " False  True  True False False  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True False  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True False  True  True False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      " False  True  True False  True  True False  True  True False  True  True\n",
      " False  True  True  True  True  True False  True  True  True False  True\n",
      " False  True False False  True  True  True False  True False False  True\n",
      "  True  True  True  True  True  True False False False  True  True  True\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      "  True False False False  True  True False  True  True  True False  True\n",
      "  True  True  True False  True  True False  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True False  True False  True  True  True\n",
      "  True False  True  True  True  True  True  True False  True  True False\n",
      "  True False  True  True  True  True  True  True False  True False  True\n",
      "  True  True  True  True False False False False  True  True False  True\n",
      "  True  True  True  True False False  True  True  True  True False False\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True False  True  True  True  True\n",
      "  True  True False  True  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True False  True  True False False  True  True  True  True\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True False  True\n",
      "  True False  True  True False False  True  True False  True False  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True False  True\n",
      " False  True  True False  True False  True False  True  True  True False\n",
      "  True  True  True  True]\n",
      "tf.Tensor(\n",
      "[False False False  True False False  True False False False  True  True\n",
      "  True False  True False False False  True False False False  True False\n",
      "  True False False False False False False False False  True False  True\n",
      " False False  True  True False  True False False False False False  True\n",
      " False False False  True False False  True  True False False False  True\n",
      "  True False  True False False False False False False False False False\n",
      "  True False False  True  True False  True False False False False  True\n",
      "  True False  True False False False  True False False False  True False\n",
      " False False  True False False False  True False False False False False\n",
      "  True  True  True False False False False False False False  True False\n",
      " False False  True False  True  True False False  True  True False False\n",
      " False False  True False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False  True False False  True  True False False\n",
      " False  True  True  True False False False False False False False False\n",
      " False  True  True False  True  True False  True  True  True False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False  True  True False  True False False False\n",
      " False False False False  True False  True False  True False False False\n",
      " False  True  True False False False False  True  True False  True  True\n",
      " False False False False False False False False False  True  True False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False  True False  True  True False False False False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False  True False False  True False False False False False False\n",
      "  True  True  True False False False  True False  True  True False False\n",
      "  True False False  True  True False False False  True False False False\n",
      " False False False False False False False  True  True False  True False\n",
      " False False False False False False False False  True False False False\n",
      " False False  True False False  True  True False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False  True False False  True False False  True False False\n",
      "  True False False False False False  True False False False  True False\n",
      "  True False  True  True False False False  True False  True  True False\n",
      " False False False False False False  True  True  True False False False\n",
      "  True False False False  True False False False False False False False\n",
      " False  True  True  True False False  True False False False  True False\n",
      " False False False  True False False  True False False False False False\n",
      " False  True False False False False False  True False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False  True False False False False  True False  True False False False\n",
      " False  True False False False False False False  True False False  True\n",
      " False  True False False False False False False  True False  True False\n",
      " False False False False  True  True  True  True False False  True False\n",
      " False False False False  True  True False False False False  True  True\n",
      " False  True False False False  True False False False False False False\n",
      " False False False  True False False False  True False False False False\n",
      " False False  True False False False  True  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False  True False False  True  True False False False False\n",
      " False  True False False False  True False False False False False False\n",
      " False False  True False False False False False False False  True False\n",
      " False  True False False  True  True False False  True False  True False\n",
      " False  True False False False False False False False False False False\n",
      " False False  True False False False False False False False  True False\n",
      "  True False False  True False  True False  True False False False  True\n",
      " False False False False], shape=(700,), dtype=bool)\n",
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False  True  True False False False\n",
      " False  True False False False False False False False False  True False\n",
      "  True  True False False  True False  True False False False  True False\n",
      " False False False False  True False False False False  True  True False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False False False False False False False False  True  True False  True\n",
      "  True  True False False False  True False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False  True False  True  True  True False False False False\n",
      " False False False  True False  True False False False  True False  True\n",
      " False False False  True  True False False False  True False False  True\n",
      " False  True False False False False False False False False False False\n",
      "  True False False False False  True  True  True False  True False False\n",
      " False False False False False False  True False False False False False\n",
      "  True  True  True False False False False False False  True  True False\n",
      " False False False False False  True  True  True  True False  True  True\n",
      " False  True False  True False False False  True False False  True False\n",
      " False  True False  True False False False  True False False False False\n",
      " False False False False False  True False False False  True False False\n",
      " False False False False  True  True False False  True False False  True\n",
      " False False False  True False  True False False False  True False False\n",
      " False False False False False  True False False False  True  True  True\n",
      " False False  True False False False False False False  True  True False\n",
      " False  True False False False False False False False  True  True  True\n",
      " False False False False False False False  True False False False False\n",
      " False  True False False False False False False False False  True  True\n",
      " False False False  True False False False False False False False  True\n",
      "  True  True  True  True False  True False False False False False False\n",
      " False False False  True False False False False  True False  True False\n",
      " False False False False  True False  True False False False False False\n",
      " False False  True False False False False False  True False  True False\n",
      " False False False  True False  True False False False False False  True\n",
      " False False False False  True False False False  True False False  True\n",
      " False  True False False  True False False False False False  True  True\n",
      " False  True False False False  True False False False False  True False\n",
      " False False False False False  True False False False  True False False\n",
      "  True  True False False False  True False False False False False False\n",
      " False False False False False  True False False False  True False False\n",
      " False  True False False  True  True  True  True  True False False False\n",
      " False False  True  True False  True False False False False False False\n",
      "  True False False False False False False False False  True False False\n",
      "  True False False  True False False False  True False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False  True False  True  True False False\n",
      " False False False  True  True False False False False False  True  True\n",
      " False  True False False  True False False False False False False  True\n",
      "  True False False False False False False False  True False  True False\n",
      "  True False False False  True False False  True False False  True False\n",
      " False False False False  True False False False False False False  True\n",
      " False False False False False False  True  True False  True  True False\n",
      " False  True False  True False False False  True  True False False False\n",
      " False False  True False False False False False False  True False  True\n",
      " False False False  True False False False False False  True False False\n",
      "  True False False  True False False False False  True  True False  True\n",
      " False  True  True False False False False False  True False False False\n",
      " False False False False]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True False False  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False  True\n",
      " False False  True  True False  True False  True  True  True False  True\n",
      "  True  True  True  True False  True  True  True  True False False  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False  True  True\n",
      "  True  True  True  True  True  True  True  True False False  True False\n",
      " False False  True  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True False False False  True  True  True  True\n",
      "  True  True  True False  True False  True  True  True False  True False\n",
      "  True  True  True False False  True  True  True False  True  True False\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False False False  True False  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      " False False False  True  True  True  True  True  True False False  True\n",
      "  True  True  True  True  True False False False False  True False False\n",
      "  True False  True False  True  True  True False  True  True False  True\n",
      "  True False  True False  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True False  True  True\n",
      "  True  True  True  True False False  True  True False  True  True False\n",
      "  True  True  True False  True False  True  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True  True False False False\n",
      "  True  True False  True  True  True  True  True  True False False  True\n",
      "  True False  True  True  True  True  True  True  True False False False\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False False\n",
      "  True  True  True False  True  True  True  True  True  True  True False\n",
      " False False False False  True False  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True False  True False  True\n",
      "  True  True  True  True False  True False  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True False  True False  True\n",
      "  True  True  True False  True False  True  True  True  True  True False\n",
      "  True  True  True  True False  True  True  True False  True  True False\n",
      "  True False  True  True False  True  True  True  True  True False False\n",
      "  True False  True  True  True False  True  True  True  True False  True\n",
      "  True  True  True  True  True False  True  True  True False  True  True\n",
      " False False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True False  True  True\n",
      "  True False  True  True False False False False False  True  True  True\n",
      "  True  True False False  True False  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True False  True  True\n",
      " False  True  True False  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True False  True False False  True  True\n",
      "  True  True  True False False  True  True  True  True  True False False\n",
      "  True False  True  True False  True  True  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True False  True False  True\n",
      " False  True  True  True False  True  True False  True  True False  True\n",
      "  True  True  True  True False  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True False False  True False False  True\n",
      "  True False  True False  True  True  True False False  True  True  True\n",
      "  True  True False  True  True  True  True  True  True False  True False\n",
      "  True  True  True False  True  True  True  True  True False  True  True\n",
      " False  True  True False  True  True  True  True False False  True False\n",
      "  True False False  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True]\n",
      "tf.Tensor(\n",
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False  True  True False False False\n",
      " False  True False False False False False False False False  True False\n",
      "  True  True False False  True False  True False False False  True False\n",
      " False False False False  True False False False False  True  True False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False False False False False False False False  True  True False  True\n",
      "  True  True False False False  True False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False  True False  True  True  True False False False False\n",
      " False False False  True False  True False False False  True False  True\n",
      " False False False  True  True False False False  True False False  True\n",
      " False  True False False False False False False False False False False\n",
      "  True False False False False  True  True  True False  True False False\n",
      " False False False False False False  True False False False False False\n",
      "  True  True  True False False False False False False  True  True False\n",
      " False False False False False  True  True  True  True False  True  True\n",
      " False  True False  True False False False  True False False  True False\n",
      " False  True False  True False False False  True False False False False\n",
      " False False False False False  True False False False  True False False\n",
      " False False False False  True  True False False  True False False  True\n",
      " False False False  True False  True False False False  True False False\n",
      " False False False False False  True False False False  True  True  True\n",
      " False False  True False False False False False False  True  True False\n",
      " False  True False False False False False False False  True  True  True\n",
      " False False False False False False False  True False False False False\n",
      " False  True False False False False False False False False  True  True\n",
      " False False False  True False False False False False False False  True\n",
      "  True  True  True  True False  True False False False False False False\n",
      " False False False  True False False False False  True False  True False\n",
      " False False False False  True False  True False False False False False\n",
      " False False  True False False False False False  True False  True False\n",
      " False False False  True False  True False False False False False  True\n",
      " False False False False  True False False False  True False False  True\n",
      " False  True False False  True False False False False False  True  True\n",
      " False  True False False False  True False False False False  True False\n",
      " False False False False False  True False False False  True False False\n",
      "  True  True False False False  True False False False False False False\n",
      " False False False False False  True False False False  True False False\n",
      " False  True False False  True  True  True  True  True False False False\n",
      " False False  True  True False  True False False False False False False\n",
      "  True False False False False False False False False  True False False\n",
      "  True False False  True False False False  True False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False  True False  True  True False False\n",
      " False False False  True  True False False False False False  True  True\n",
      " False  True False False  True False False False False False False  True\n",
      "  True False False False False False False False  True False  True False\n",
      "  True False False False  True False False  True False False  True False\n",
      " False False False False  True False False False False False False  True\n",
      " False False False False False False  True  True False  True  True False\n",
      " False  True False  True False False False  True  True False False False\n",
      " False False  True False False False False False False  True False  True\n",
      " False False False  True False False False False False  True False False\n",
      "  True False False  True False False False False  True  True False  True\n",
      " False  True  True False False False False False  True False False False\n",
      " False False False False], shape=(700,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "stratified = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "for train_index, test_index in stratified.split(X_train, W_train):\n",
    "    index = np.zeros(700, dtype=bool)\n",
    "    index[test_index] = 1\n",
    "    print(index)\n",
    "    print(~index)\n",
    "    print(tf.convert_to_tensor(index))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:50:16.777997Z",
     "start_time": "2023-07-24T01:50:16.755136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.51, 0.32, 0.79, 0.46, 0.74, 0.38, 0.52, 0.54, 0.36, 0.58, 0.63,\n       0.29, 0.51, 0.37, 0.57, 0.21, 0.74, 0.4 , 0.55, 0.56, 0.77, 0.5 ,\n       0.31, 0.38, 0.6 , 0.14, 0.35, 0.43, 0.43, 0.62, 0.45, 0.72, 0.54,\n       0.46, 0.68, 0.52, 0.57, 0.39, 0.76, 0.38, 0.54, 0.37, 0.49, 0.46,\n       0.44, 0.28, 0.46, 0.51, 0.31, 0.56, 0.49, 0.54, 0.41, 0.53, 0.41,\n       0.48, 0.81, 0.32, 0.48, 0.41, 0.37, 0.35, 0.74, 0.38, 0.47, 0.64,\n       0.5 , 0.61, 0.74, 0.73, 0.54, 0.26, 0.38, 0.59, 0.36, 0.51, 0.61,\n       0.48, 0.48, 0.39, 0.53, 0.56, 0.36, 0.29, 0.69, 0.47, 0.54, 0.38,\n       0.35, 0.54, 0.33, 0.4 , 0.29, 0.48, 0.56, 0.33, 0.58, 0.29, 0.31,\n       0.36, 0.24, 0.44, 0.51, 0.64, 0.26, 0.61, 0.43, 0.55, 0.28, 0.63,\n       0.51, 0.43, 0.27, 0.64, 0.59, 0.53, 0.64, 0.34, 0.52, 0.63, 0.26,\n       0.64, 0.43, 0.52, 0.48, 0.73, 0.47, 0.61, 0.54, 0.5 , 0.35, 0.53,\n       0.37, 0.53, 0.34, 0.11, 0.36, 0.41, 0.74, 0.58, 0.37, 0.37, 0.41,\n       0.41, 0.55, 0.3 , 0.36, 0.32, 0.55, 0.61, 0.55, 0.46, 0.5 , 0.7 ,\n       0.46, 0.62, 0.65, 0.39, 0.61, 0.71, 0.56, 0.53, 0.33, 0.54, 0.4 ,\n       0.54, 0.44, 0.83, 0.43, 0.39, 0.39, 0.66, 0.22, 0.32, 0.5 , 0.55,\n       0.69, 0.49, 0.57, 0.51, 0.28, 0.46, 0.49, 0.34, 0.62, 0.42, 0.41,\n       0.66, 0.54, 0.65, 0.8 , 0.38, 0.56, 0.41, 0.49, 0.54, 0.42, 0.41,\n       0.51, 0.25, 0.28, 0.66, 0.49, 0.52, 0.46, 0.15, 0.8 , 0.4 , 0.39,\n       0.49, 0.5 , 0.59, 0.52, 0.43, 0.74, 0.2 , 0.8 , 0.58, 0.58, 0.46,\n       0.59, 0.44, 0.41, 0.18, 0.29, 0.34, 0.66, 0.8 , 0.54, 0.41, 0.33,\n       0.39, 0.56, 0.45, 0.35, 0.5 , 0.5 , 0.33, 0.36, 0.79, 0.38, 0.27,\n       0.68, 0.69, 0.55, 0.33, 0.54, 0.57, 0.42, 0.6 , 0.37, 0.5 , 0.38,\n       0.32, 0.41, 0.45, 0.41, 0.75, 0.76, 0.5 , 0.76, 0.42, 0.87, 0.41,\n       0.61, 0.28, 0.46, 0.5 , 0.35, 0.49, 0.41, 0.34, 0.52, 0.37, 0.39,\n       0.39, 0.52, 0.45, 0.45, 0.62, 0.61, 0.37, 0.5 , 0.44, 0.47, 0.39,\n       0.4 , 0.82, 0.52, 0.84, 0.37, 0.49, 0.15, 0.48, 0.47, 0.73, 0.33,\n       0.44, 0.55, 0.49, 0.6 , 0.53, 0.51, 0.36, 0.67, 0.45, 0.37, 0.63,\n       0.79, 0.28, 0.83, 0.24, 0.45, 0.56, 0.42, 0.39, 0.46, 0.44, 0.3 ,\n       0.55, 0.61, 0.58, 0.47, 0.66, 0.55, 0.68, 0.37, 0.38, 0.65, 0.48,\n       0.47, 0.69, 0.46, 0.56, 0.72, 0.53, 0.65, 0.36, 0.33, 0.34, 0.44,\n       0.56, 0.48, 0.48, 0.53, 0.3 , 0.75, 0.34, 0.27, 0.39, 0.57, 0.36,\n       0.54, 0.32, 0.34, 0.57, 0.46, 0.28, 0.51, 0.76, 0.28, 0.46, 0.58,\n       0.33, 0.41, 0.67, 0.36, 0.36, 0.37, 0.28, 0.45, 0.47, 0.48, 0.5 ,\n       0.52, 0.54, 0.46, 0.32, 0.27, 0.5 , 0.45, 0.54, 0.34, 0.37, 0.5 ,\n       0.44, 0.5 , 0.52, 0.73, 0.42, 0.42, 0.56, 0.73, 0.39, 0.52, 0.3 ,\n       0.49, 0.53, 0.47, 0.43, 0.59, 0.35, 0.48, 0.52, 0.49, 0.49, 0.16,\n       0.52, 0.64, 0.75, 0.43, 0.62, 0.52, 0.52, 0.28, 0.39, 0.76, 0.56,\n       0.38, 0.8 , 0.39, 0.7 , 0.48, 0.46, 0.32, 0.61, 0.45, 0.24, 0.47,\n       0.48, 0.4 , 0.61, 0.42, 0.43, 0.44, 0.17, 0.6 , 0.44, 0.42, 0.75,\n       0.66, 0.44, 0.75, 0.49, 0.6 , 0.32, 0.5 , 0.55, 0.48, 0.26, 0.46,\n       0.55, 0.53, 0.57, 0.46, 0.49, 0.81, 0.41, 0.55, 0.54, 0.54, 0.44,\n       0.65, 0.31, 0.66, 0.63, 0.33, 0.82, 0.42, 0.52, 0.31, 0.5 , 0.41,\n       0.69, 0.45, 0.56, 0.36, 0.63, 0.62, 0.28, 0.39, 0.23, 0.58, 0.57,\n       0.61, 0.34, 0.42, 0.39, 0.61, 0.65, 0.57, 0.48, 0.69, 0.55, 0.63,\n       0.45, 0.71, 0.37, 0.39, 0.49, 0.21, 0.63, 0.39, 0.55, 0.46, 0.59,\n       0.54, 0.65, 0.59, 0.3 , 0.51, 0.37, 0.29, 0.45, 0.39, 0.37, 0.55,\n       0.59, 0.58, 0.15, 0.58, 0.48, 0.63, 0.49, 0.59, 0.43, 0.54, 0.46,\n       0.53, 0.64, 0.47, 0.56, 0.19, 0.62, 0.33, 0.28, 0.34, 0.35, 0.64,\n       0.48, 0.61, 0.56, 0.44, 0.41, 0.67, 0.56, 0.48, 0.55, 0.51, 0.52,\n       0.57, 0.55, 0.41, 0.51, 0.34, 0.61, 0.35, 0.44, 0.66, 0.54, 0.6 ,\n       0.36, 0.48, 0.52, 0.3 , 0.46, 0.46, 0.47, 0.68, 0.54, 0.68, 0.48,\n       0.72, 0.43, 0.51, 0.46, 0.47, 0.55, 0.31, 0.45, 0.63, 0.53, 0.44,\n       0.48, 0.61, 0.57, 0.66, 0.62, 0.58, 0.34, 0.36, 0.57, 0.65, 0.68,\n       0.49, 0.56, 0.3 , 0.48, 0.66, 0.45, 0.8 , 0.41, 0.37, 0.33, 0.47,\n       0.58, 0.5 , 0.69, 0.5 , 0.36, 0.41, 0.6 , 0.23, 0.53, 0.58, 0.5 ,\n       0.6 , 0.5 , 0.25, 0.52, 0.47, 0.27, 0.46, 0.24, 0.59, 0.58, 0.41,\n       0.63, 0.53, 0.36, 0.57, 0.52, 0.41, 0.55, 0.38, 0.57, 0.37, 0.18,\n       0.57, 0.67, 0.49, 0.69, 0.6 , 0.37, 0.41, 0.82, 0.59, 0.36, 0.34,\n       0.5 , 0.58, 0.57, 0.54, 0.64, 0.48, 0.48, 0.19, 0.35, 0.7 , 0.56,\n       0.51, 0.57, 0.35, 0.22, 0.3 , 0.31, 0.47, 0.51, 0.46, 0.32, 0.73,\n       0.4 , 0.57, 0.36, 0.51, 0.31, 0.43, 0.3 , 0.58, 0.49, 0.35, 0.24,\n       0.76, 0.34, 0.23, 0.36, 0.15, 0.27, 0.57, 0.47, 0.45, 0.44, 0.66,\n       0.33, 0.51, 0.58, 0.65, 0.4 , 0.38, 0.56])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:48:51.358466Z",
     "start_time": "2023-07-24T01:48:51.340612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.56, 0.46, 0.68, 0.7 , 0.71, 0.31, 0.75, 0.54, 0.43, 0.43, 0.4 ,\n       0.69, 0.32, 0.53, 0.57, 0.26, 0.59, 0.35, 0.64, 0.45, 0.56, 0.71,\n       0.64, 0.3 , 0.47, 0.44, 0.32, 0.48, 0.46, 0.68, 0.54, 0.56, 0.43,\n       0.53, 0.35, 0.46, 0.52, 0.58, 0.32, 0.36, 0.31, 0.41, 0.76, 0.39,\n       0.51, 0.37, 0.62, 0.74, 0.53, 0.58, 0.41, 0.32, 0.61, 0.58, 0.44,\n       0.22, 0.73, 0.68, 0.29, 0.89, 0.32, 0.6 , 0.3 , 0.35, 0.34, 0.76,\n       0.74, 0.68, 0.54, 0.65, 0.49, 0.39, 0.55, 0.36, 0.37, 0.44, 0.47,\n       0.6 , 0.41, 0.62, 0.42, 0.41, 0.43, 0.41, 0.39, 0.36, 0.33, 0.51,\n       0.48, 0.36, 0.33, 0.55, 0.61, 0.43, 0.48, 0.53, 0.56, 0.47, 0.58,\n       0.38, 0.25, 0.6 , 0.43, 0.35, 0.45, 0.81, 0.67, 0.68, 0.32, 0.34,\n       0.76, 0.41, 0.28, 0.62, 0.39, 0.46, 0.74, 0.42, 0.51, 0.82, 0.28,\n       0.69, 0.27, 0.41, 0.58, 0.55, 0.48, 0.56, 0.47, 0.61, 0.26, 0.59,\n       0.57, 0.55, 0.39, 0.49, 0.54, 0.48, 0.54, 0.45, 0.56, 0.52, 0.78,\n       0.45, 0.7 , 0.46, 0.29, 0.49, 0.43, 0.53, 0.53, 0.38, 0.68, 0.68,\n       0.6 , 0.48, 0.49, 0.47, 0.69, 0.43, 0.5 , 0.56, 0.6 , 0.53, 0.64,\n       0.54, 0.67, 0.69, 0.55, 0.63, 0.52, 0.63, 0.27, 0.61, 0.62, 0.43,\n       0.57, 0.43, 0.34, 0.47, 0.42, 0.6 , 0.34, 0.57, 0.61, 0.62, 0.5 ,\n       0.6 , 0.64, 0.56, 0.7 , 0.58, 0.65, 0.5 , 0.69, 0.41, 0.58, 0.58,\n       0.51, 0.24, 0.49, 0.54, 0.41, 0.41, 0.45, 0.51, 0.8 , 0.52, 0.62,\n       0.39, 0.58, 0.52, 0.43, 0.62, 0.74, 0.38, 0.72, 0.68, 0.62, 0.47,\n       0.45, 0.55, 0.39, 0.59, 0.73, 0.48, 0.63, 0.46, 0.56, 0.56, 0.45,\n       0.47, 0.51, 0.58, 0.69, 0.79, 0.68, 0.35, 0.52, 0.73, 0.41, 0.38,\n       0.44, 0.53, 0.46, 0.21, 0.84, 0.36, 0.47, 0.71, 0.55, 0.36, 0.46,\n       0.3 , 0.62, 0.54, 0.45, 0.61, 0.86, 0.62, 0.55, 0.52, 0.56, 0.48,\n       0.63, 0.49, 0.46, 0.68, 0.73, 0.38, 0.29, 0.57, 0.25, 0.27, 0.56,\n       0.49, 0.45, 0.51, 0.38, 0.54, 0.51, 0.41, 0.49, 0.47, 0.56, 0.78,\n       0.49, 0.67, 0.56, 0.77, 0.41, 0.54, 0.29, 0.38, 0.5 , 0.54, 0.46,\n       0.46, 0.3 , 0.28, 0.56, 0.46, 0.3 , 0.31, 0.63, 0.58, 0.35, 0.55,\n       0.67, 0.68, 0.58, 0.3 , 0.69, 0.6 , 0.3 , 0.37, 0.41, 0.57, 0.51,\n       0.59, 0.71, 0.34, 0.66, 0.74, 0.61, 0.65, 0.64, 0.53, 0.49, 0.62,\n       0.65, 0.43, 0.63, 0.6 , 0.67, 0.78, 0.55, 0.39, 0.53, 0.47, 0.55,\n       0.48, 0.61, 0.31, 0.66, 0.33, 0.68, 0.5 , 0.45, 0.4 , 0.55, 0.61,\n       0.42, 0.48, 0.43, 0.46, 0.57, 0.51, 0.38, 0.51, 0.47, 0.52, 0.7 ,\n       0.63, 0.46, 0.51, 0.32, 0.52, 0.54, 0.29, 0.52, 0.47, 0.61, 0.5 ,\n       0.4 , 0.28, 0.54, 0.73, 0.66, 0.56, 0.22, 0.62, 0.37, 0.42, 0.55,\n       0.35, 0.55, 0.61, 0.46, 0.65, 0.54, 0.46, 0.77, 0.39, 0.49, 0.31,\n       0.48, 0.44, 0.26, 0.5 , 0.64, 0.48, 0.53, 0.4 , 0.53, 0.59, 0.19,\n       0.53, 0.77, 0.48, 0.54, 0.69, 0.7 , 0.62, 0.73, 0.35, 0.6 , 0.8 ,\n       0.42, 0.55, 0.45, 0.59, 0.6 , 0.34, 0.18, 0.56, 0.69, 0.56, 0.34,\n       0.61, 0.55, 0.6 , 0.35, 0.38, 0.53, 0.38, 0.79, 0.53, 0.36, 0.69,\n       0.66, 0.57, 0.42, 0.73, 0.51, 0.43, 0.47, 0.64, 0.4 , 0.53, 0.52,\n       0.52, 0.78, 0.59, 0.37, 0.72, 0.74, 0.61, 0.41, 0.66, 0.59, 0.56,\n       0.41, 0.37, 0.57, 0.53, 0.45, 0.9 , 0.55, 0.58, 0.49, 0.63, 0.54,\n       0.34, 0.38, 0.45, 0.34, 0.54, 0.78, 0.23, 0.38, 0.31, 0.59, 0.49,\n       0.49, 0.61, 0.48, 0.51, 0.63, 0.48, 0.55, 0.49, 0.56, 0.41, 0.68,\n       0.43, 0.43, 0.47, 0.42, 0.66, 0.55, 0.5 , 0.46, 0.69, 0.56, 0.38,\n       0.51, 0.7 , 0.62, 0.41, 0.5 , 0.54, 0.7 , 0.59, 0.32, 0.31, 0.46,\n       0.33, 0.73, 0.2 , 0.52, 0.46, 0.61, 0.48, 0.69, 0.56, 0.66, 0.54,\n       0.56, 0.58, 0.45, 0.57, 0.17, 0.66, 0.48, 0.56, 0.37, 0.51, 0.46,\n       0.43, 0.46, 0.52, 0.45, 0.4 , 0.4 , 0.31, 0.28, 0.43, 0.61, 0.51,\n       0.47, 0.61, 0.4 , 0.53, 0.32, 0.52, 0.36, 0.46, 0.5 , 0.47, 0.27,\n       0.49, 0.37, 0.5 , 0.33, 0.53, 0.44, 0.69, 0.64, 0.29, 0.61, 0.54,\n       0.62, 0.59, 0.38, 0.61, 0.34, 0.38, 0.33, 0.63, 0.68, 0.42, 0.71,\n       0.5 , 0.74, 0.57, 0.71, 0.85, 0.52, 0.28, 0.46, 0.72, 0.65, 0.75,\n       0.39, 0.59, 0.43, 0.47, 0.47, 0.53, 0.33, 0.51, 0.58, 0.47, 0.61,\n       0.64, 0.72, 0.66, 0.7 , 0.56, 0.62, 0.5 , 0.32, 0.7 , 0.53, 0.39,\n       0.66, 0.35, 0.51, 0.62, 0.64, 0.36, 0.49, 0.34, 0.54, 0.62, 0.44,\n       0.41, 0.45, 0.39, 0.41, 0.53, 0.26, 0.5 , 0.5 , 0.42, 0.39, 0.19,\n       0.59, 0.37, 0.39, 0.57, 0.49, 0.32, 0.53, 0.7 , 0.72, 0.62, 0.26,\n       0.52, 0.24, 0.52, 0.68, 0.49, 0.66, 0.47, 0.6 , 0.65, 0.57, 0.32,\n       0.58, 0.46, 0.38, 0.56, 0.52, 0.33, 0.65, 0.49, 0.5 , 0.74, 0.64,\n       0.34, 0.53, 0.35, 0.58, 0.73, 0.53, 0.48, 0.68, 0.47, 0.43, 0.61,\n       0.54, 0.66, 0.47, 0.28, 0.32, 0.52, 0.59, 0.48, 0.42, 0.57, 0.76,\n       0.26, 0.47, 0.47, 0.54, 0.6 , 0.55, 0.74])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:47:23.196629Z",
     "start_time": "2023-07-24T01:47:23.170418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False,  True, False, False,  True, False, False,\n       False,  True,  True,  True, False,  True, False,  True, False,\n        True,  True,  True, False,  True, False,  True,  True, False,\n       False, False, False, False, False, False,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True, False, False,\n       False,  True,  True, False, False, False,  True,  True, False,\n        True,  True, False,  True,  True,  True,  True, False,  True,\n        True, False, False, False, False, False, False, False, False,\n        True, False, False,  True,  True, False,  True, False,  True,\n        True, False,  True,  True, False,  True, False, False, False,\n        True, False,  True,  True,  True,  True,  True,  True,  True,\n       False, False,  True,  True, False,  True, False, False, False,\n        True,  True,  True, False, False, False, False, False, False,\n       False,  True, False,  True, False,  True, False,  True,  True,\n       False, False,  True,  True, False, False, False, False,  True,\n        True, False,  True,  True,  True,  True, False, False, False,\n       False, False, False,  True, False,  True, False, False, False,\n        True, False,  True, False, False,  True,  True,  True, False,\n       False, False,  True, False, False,  True, False,  True, False,\n       False, False,  True, False, False,  True,  True, False, False,\n        True,  True,  True,  True, False,  True,  True,  True, False,\n        True, False, False, False,  True,  True, False,  True,  True,\n        True,  True,  True,  True, False,  True,  True,  True,  True,\n       False, False, False, False, False, False,  True,  True, False,\n       False, False, False, False, False,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True, False,  True,\n        True,  True,  True, False,  True, False, False,  True, False,\n        True,  True, False,  True,  True,  True, False, False, False,\n       False,  True,  True, False, False,  True, False,  True,  True,\n        True,  True,  True, False, False, False, False,  True,  True,\n       False, False,  True,  True,  True,  True, False, False, False,\n        True, False,  True,  True, False, False,  True, False, False,\n       False,  True, False, False,  True,  True,  True,  True, False,\n        True,  True,  True, False,  True,  True, False, False,  True,\n        True, False, False,  True,  True, False, False,  True,  True,\n       False, False,  True, False, False, False,  True,  True,  True,\n        True,  True,  True, False, False, False,  True,  True,  True,\n        True, False, False,  True,  True, False,  True,  True, False,\n       False, False,  True, False,  True,  True, False, False, False,\n        True, False, False, False,  True,  True, False,  True,  True,\n        True,  True,  True,  True, False,  True, False, False,  True,\n       False, False, False, False, False,  True,  True, False,  True,\n        True, False,  True, False,  True, False, False, False, False,\n       False,  True, False,  True,  True, False, False, False, False,\n        True, False,  True,  True, False, False,  True, False,  True,\n        True,  True, False,  True, False, False,  True, False,  True,\n        True, False, False, False,  True,  True,  True, False,  True,\n        True,  True, False, False,  True,  True,  True,  True,  True,\n       False,  True, False, False,  True, False,  True,  True,  True,\n       False,  True,  True,  True,  True, False, False,  True,  True,\n       False, False, False, False,  True, False, False,  True,  True,\n        True, False,  True,  True, False, False,  True,  True, False,\n        True,  True, False,  True, False,  True,  True, False, False,\n       False, False, False, False,  True, False, False, False,  True,\n       False,  True, False,  True, False, False, False,  True, False,\n        True,  True,  True,  True,  True,  True, False, False, False,\n       False,  True,  True,  True, False,  True,  True, False,  True,\n       False, False, False,  True,  True, False, False, False, False,\n       False, False,  True,  True, False,  True,  True,  True, False,\n        True, False, False, False,  True,  True, False,  True, False,\n       False, False, False, False,  True,  True,  True,  True, False,\n        True,  True, False, False, False, False, False,  True,  True,\n        True, False,  True,  True,  True,  True, False,  True, False,\n        True,  True,  True, False, False, False, False,  True,  True,\n       False,  True, False,  True,  True, False, False,  True, False,\n       False, False,  True,  True, False,  True, False, False, False,\n        True,  True,  True, False,  True, False,  True, False, False,\n       False,  True, False,  True,  True, False, False,  True, False,\n       False, False, False,  True,  True, False,  True,  True, False,\n       False, False,  True, False,  True, False, False, False,  True,\n        True,  True, False,  True,  True, False, False,  True,  True,\n        True, False, False, False,  True,  True, False,  True, False,\n       False,  True,  True, False,  True,  True, False, False,  True,\n        True,  True,  True, False,  True, False,  True, False, False,\n       False, False, False,  True, False, False,  True, False,  True,\n        True, False, False, False, False,  True,  True,  True,  True,\n        True,  True,  True,  True, False,  True, False,  True,  True,\n       False, False,  True, False, False, False, False])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:42:50.012806Z",
     "start_time": "2023-07-24T01:42:49.972670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.zeros(700, dtype=bool)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T01:41:35.824255Z",
     "start_time": "2023-07-24T01:41:35.807168Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = np.zeros((700,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lol = load_model('model_ex')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lol.fit(X_train, w, batch_size=100, epochs=400, callbacks=callback, validation_split=0.3, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keras.activations.sigmoid(lol.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try out some stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, max_features=0.3, random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier.fit(X_train, W_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier.predict_proba(X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CROSS-FITTING WILL PROBABLY SOLVE THE PROBLEM!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier.predict(X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.ones(100, dtype=bool)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_mask = torch.zeros(100, dtype=bool)\n",
    "pred_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "~pred_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hasattr(rf, 'predict')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_class = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hasattr(rf_class, 'train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso = LassoCV()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hasattr(lasso, 'predict_proba')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MyModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dimension=FEATURE_DIMENSION,\n",
    "            n_layers_1=3,\n",
    "            n_layers_2=2,\n",
    "            n_units_1=N_UNITS_FIRST_PART,\n",
    "            n_units_2=N_UNITS_SECOND_PART,\n",
    "            activation=NON_LINEARITY,\n",
    "            regularizer=regularizers.L2(1e-4)):\n",
    "        super().__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.n_layers_1 = n_layers_1\n",
    "        self.n_layers_2 = n_layers_2\n",
    "        self.n_units_1 = n_units_1\n",
    "        self.n_units_2 = n_units_2\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "        self.dense1 = layers.Dense(units=self.n_units_1, activation=self.activation, name=\"layer1\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "        self.dense2 = layers.Dense(units=self.n_units_1, activation=self.activation, name=\"layer2\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "        self.dense3 = layers.Dense(units=self.n_units_1, activation=self.activation, name=\"layer3\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "        self.dense4 = layers.Dense(units=self.n_units_2, activation=self.activation, name=\"layer4\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "        self.dense5 = layers.Dense(units=self.n_units_2, activation=self.activation, name=\"layer5\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "        self.dense6 = layers.Dense(units=1, activation='linear', name=\"layer6\",\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dense6(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MyModel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              # List of metrics to monitor\n",
    "              metrics=[keras.metrics.MeanSquaredError()],\n",
    "              # weighted metrics\n",
    "              weighted_metrics=[]\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear\n",
    "# activation)\n",
    "model1 = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"model_25\")\n",
    "\n",
    "# compile the model\n",
    "model1.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    "    # weighted metrics\n",
    "    weighted_metrics=[]\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.clone_model(model1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1.fit(X_train, Y_train, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    "    # weighted metrics\n",
    "    weighted_metrics=[]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.fit(X_train, Y_train, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clone_model_regression(model):\n",
    "    cloned_model = tf.keras.models.clone_model(model)\n",
    "    cloned_model.compile(\n",
    "        optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),\n",
    "        # Loss function to minimize\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # List of metrics to monitor\n",
    "        metrics=[keras.metrics.MeanSquaredError()],\n",
    "        # weighted metrics\n",
    "        weighted_metrics=[]\n",
    "    )\n",
    "    return cloned_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cloned_model = clone_model_regression(model1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cloned_model.fit(X_train, Y_train, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clone_model_classification(model):\n",
    "    cloned_model = tf.keras.models.clone_model(model)\n",
    "    cloned_model.compile(\n",
    "        optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),\n",
    "        # Loss function to minimize\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # List of metrics to monitor\n",
    "        metrics=[keras.metrics.MeanSquaredError()],\n",
    "        # weighted metrics\n",
    "        weighted_metrics=[]\n",
    "    )\n",
    "    return cloned_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from HelperFuctions import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_nn = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "], name=\"model_sequential\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_nn_1 = keras.Sequential([\n",
    "    keras.Input(shape=(26,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "], name=\"model_sequential_1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_mu = clone_model_regression(model_nn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ex = clone_model_classification(model_nn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_mu.fit(X_train, Y_train, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworks import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
