{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from DefaultParameters import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# seed\n",
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear\n",
    "# activation)\n",
    "model_25 = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "\n",
    "], name=\"model_25\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_25.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    "    # weighted metrics\n",
    "    weighted_metrics=[]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_25.save('model_25')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''''''' For T Learner Only '''''''\n",
    "\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model_26 = keras.Sequential([\n",
    "    keras.Input(shape=(26,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\", kernel_regularizer=regularizers.L2(1e-4)),\n",
    "\n",
    "], name=\"model_26\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_26.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    "    # weighted metrics\n",
    "    weighted_metrics=[]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_26.save('model_26')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''''''' For propensity score '''''''\n",
    "\n",
    "model_ex = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\", kernel_regularizer=regularizers.L2(1e-2)),\n",
    "\n",
    "], name=\"model_ex\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_ex.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    # List of metrics to monitor\n",
    "    metrics=keras.metrics.BinaryAccuracy(),\n",
    "    # weighted metrics\n",
    "    weighted_metrics=[]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_ex.save('model_ex')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''''''' Callbacks '''''''\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
