{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:15:26.723762Z",
     "start_time": "2023-06-02T15:15:26.693795Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  4.907737,  -0.23949 ,   0.202046, ...,  -0.274776,   1.      ,\n          8.      ],\n       [-15.366406,   0.785973,  -0.935212, ...,   0.146119,   1.      ,\n          0.      ],\n       [  8.007676,   1.097277,  -0.962872, ...,  -0.791275,   1.      ,\n          0.      ],\n       ...,\n       [ 14.967829,   1.437212,   0.055816, ...,   0.720359,   0.      ,\n          0.      ],\n       [  8.21161 ,  -0.568252,   0.788217, ...,   0.234121,   0.      ,\n          8.      ],\n       [ -4.206514,  -0.792531,   1.636563, ...,   2.661297,   0.      ,\n          8.      ]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/arberimbibaj/dataset_example_indicatorCATE.csv', header=None, index_col=[0])\n",
    "data = data.to_numpy()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T13:47:36.454046Z",
     "start_time": "2023-06-02T13:47:36.333428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "N = len(data)\n",
    "d = len(data[0,:]) - 3\n",
    "print(N)\n",
    "print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T13:47:39.634340Z",
     "start_time": "2023-06-02T13:47:39.580768Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# train test split\n",
    "random.shuffle(data)\n",
    "training, test = data[:700,:], data[700:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T13:47:44.117484Z",
     "start_time": "2023-06-02T13:47:44.087671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# slice dataset by treatment status\n",
    "training_control = training[training[:,26]==0]\n",
    "training_treatment = training[training[:,26]==1]\n",
    "\n",
    "# Y_train by treatment status\n",
    "Y_train_control = training_control[:,0]\n",
    "Y_train_treatment = training_treatment[:,0]\n",
    "\n",
    "# X_train by treatment status\n",
    "X_train_control = training_control[:,1:26]\n",
    "X_train_treatment = training_treatment[:,1:26]\n",
    "\n",
    "# X and Y test\n",
    "X_test = test[:,1:26]\n",
    "Y_test = test[:,0]\n",
    "\n",
    "# X_train and Y_train (no split by treatment status)\n",
    "X_train = training[:,1:26]\n",
    "Y_train = training[:,0]\n",
    "\n",
    "# W_train and W_test\n",
    "W_train = training[:,26]\n",
    "W_test = test[:,26]\n",
    "\n",
    "# tau_test\n",
    "tau_test = test[:,27]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T13:47:44.666678Z",
     "start_time": "2023-06-02T13:47:44.649766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# T-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# set training and test features for the S-Learner (it views W as no different from other X's)\n",
    "X_W_train = training[:,1:27]\n",
    "X_test_0 = np.concatenate((test[:,1:26],np.zeros((300,1))), axis=1)\n",
    "X_test_1 = np.concatenate((test[:,1:26],np.ones((300,1))), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T13:47:54.813301Z",
     "start_time": "2023-06-02T13:47:54.770372Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "s_learner.fit(X_W_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "s_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_0_hat.fit(X_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_1_hat.fit(X_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "g_x_hat.fit(X_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_test) + probas_0 * x_tau_1_hat.predict(X_test)\n",
    "x_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error (much lower here!)\n",
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "r_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_train)\n",
    "r_probas_0 = r_probas[:,0] # probabilities of W=0\n",
    "r_probas_1 = r_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_tau.fit(X_train,r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)\n",
    "r_tau_hats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "dr_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_train)\n",
    "dr_probas_0 = dr_probas[:,0] # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "dr_learner_mu_0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "dr_learner_mu_1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_train) + (1 - W_train) * dr_learner_mu_0.predict(X_train) # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(X_train) - dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_train,dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "dr_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "ra_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0.predict(X_train)) + (1 - W_train)*(ra_learner_mu1.predict(X_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((ra_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "pw_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train/pw_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(pw_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((pw_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F-Learner: AKA THE SAME AS PW-LEARNER!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### F-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "f_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "f_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "f_pseudo_outcome = (W_train/f_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(f_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "f_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "f_tau_hat = f_tau_hat_learner.predict(X_test)\n",
    "f_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "print(((f_tau_hat - tau_test)**2).mean())\n",
    "print(\"Same as for PW-Learner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "u_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_train))/(W_train - u_learner_e_x.predict_proba(X_train)[:,1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_tau_hat_learner.fit(X_train,u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((u_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Just some lasso tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_poly = LassoCV(cv=10, random_state=0, tol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "lasso_poly.fit(X_poly_train,Y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print(f\"{total:.4f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# degree 3: 10 seconds to fit (100% cpu)\n",
    "# degree 4: 90 seconds to fit (100% cpu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predictions = lasso_poly.predict(X_poly_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((y_predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# just some Neural Network test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(d,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, start_from_epoch=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training Model\")\n",
    "training = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Generate predictions for test samples\")\n",
    "predictions = np.reshape(model.predict(X_test),(300,))\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, Y_test, batch_size=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRY SAME WITH LASSO AND NEURAL NETWORK!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Lasso (or L1-loss for logistic regression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.423163  ,  0.029483  , -0.763018  , ...,  1.27407353,\n         2.13154239,  3.5660995 ],\n       [-1.50415   ,  0.967986  , -0.541778  , ...,  0.34996475,\n        -0.17097233,  0.08352709],\n       [-0.3594    ,  0.617325  , -0.377409  , ...,  0.21010694,\n        -0.20856468,  0.20703374],\n       ...,\n       [ 0.643689  ,  0.137531  , -0.567255  , ...,  0.50074572,\n         0.32062984,  0.20530079],\n       [-0.356429  ,  1.838532  , -2.410945  , ...,  0.01240463,\n         0.06255419,  0.31544884],\n       [-1.148046  ,  0.419502  ,  0.037405  , ...,  0.17600748,\n        -0.06986855,  0.02773527]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:08:50.658165Z",
     "start_time": "2023-06-02T14:08:50.589672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "poly_train_treatment = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train_treatment = poly_train_treatment.fit_transform(X_train_treatment)\n",
    "poly_train_control = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train_control = poly_train_treatment.fit_transform(X_train_control)\n",
    "poly_test = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:30:23.468719Z",
     "start_time": "2023-06-02T14:30:23.429134Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-3.59400000e-01,  6.17325000e-01, -3.77409000e-01, ...,\n         2.10106938e-01, -2.08564680e-01,  2.07033743e-01],\n       [-4.64694000e-01, -2.82128000e-01,  7.95796000e-01, ...,\n         3.76589191e-01,  4.36669135e-01,  5.06334058e-01],\n       [-1.48957600e+00,  2.42270000e-02,  2.87398000e-01, ...,\n         1.75615971e-01,  4.81072377e-01,  1.31782224e+00],\n       ...,\n       [-3.63758000e-01,  1.28144500e+00, -1.16883500e+00, ...,\n         3.70097512e-01, -6.38204485e-01,  1.10053419e+00],\n       [ 2.07388000e+00, -3.06304500e+00,  2.66972200e+00, ...,\n         1.57479600e+01, -1.62172358e+01,  1.67004955e+01],\n       [-3.56429000e-01,  1.83853200e+00, -2.41094500e+00, ...,\n         1.24046328e-02,  6.25541926e-02,  3.15448838e-01]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly_train_control"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:11:44.157615Z",
     "start_time": "2023-06-02T14:11:44.135215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.423163  ,  0.029483  , -0.763018  , ...,  1.27407353,\n         2.13154239,  3.5660995 ],\n       [-1.50415   ,  0.967986  , -0.541778  , ...,  0.34996475,\n        -0.17097233,  0.08352709],\n       [ 0.253921  ,  0.773891  , -1.295039  , ...,  0.8274789 ,\n         0.92029438,  1.02352066],\n       ...,\n       [-1.195314  ,  1.512897  , -1.441811  , ...,  0.64155119,\n        -1.29486699,  2.61347894],\n       [ 0.643689  ,  0.137531  , -0.567255  , ...,  0.50074572,\n         0.32062984,  0.20530079],\n       [-1.148046  ,  0.419502  ,  0.037405  , ...,  0.17600748,\n        -0.06986855,  0.02773527]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly_train_treatment\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:11:56.143010Z",
     "start_time": "2023-06-02T14:11:56.135159Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T-learner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-2.29218340e+00,  5.00908032e+00,  5.23536753e+00, -8.36889645e-02,\n       -1.05221149e+00, -2.72051717e-01, -1.04028329e-01,  1.98170726e+00,\n        1.13551254e+01,  1.34619102e+00,  3.80487589e+00,  5.15734053e+00,\n        9.61243856e+00,  6.15261006e+00,  6.06190221e+00,  1.01536992e+00,\n        4.10380480e+00, -7.28702488e-01,  4.00465252e+00, -2.43254604e+00,\n        9.26057685e+00,  2.53865219e+00,  5.99777413e-01, -2.23093941e+00,\n       -6.59069208e-01,  7.23894133e+00,  7.91543298e+00,  8.60687305e-01,\n        8.83559552e+00,  4.71083043e+00,  9.42553238e+00,  4.05838011e-01,\n        8.20059020e+00,  8.03119550e+00,  3.12076124e-01,  2.98185001e+00,\n        7.46251605e-01,  9.23639733e+00,  1.36506917e+00,  3.57460919e+00,\n        5.22689331e+00,  2.43599950e+00,  7.30069697e-01,  1.26278202e+00,\n       -1.39611641e+00, -2.29825936e+00, -5.03671104e-01,  5.51734674e+00,\n        5.28482151e+00,  2.86900126e+00,  8.02955298e+00,  7.33189681e+00,\n        5.17974029e+00, -1.58474248e-02,  9.13716438e+00, -1.26617873e+00,\n        5.07505100e+00,  3.72722996e+00,  8.67531624e+00,  4.29449037e+00,\n        2.64984200e+00,  8.50378751e+00, -4.37730532e-01,  6.72231065e+00,\n        8.12776526e+00, -1.75169917e+00,  4.54894301e+00, -1.14965150e+00,\n        5.85233183e+00, -1.11984250e+00, -1.75147620e+00,  6.91157364e+00,\n        4.48675584e+00, -1.14757229e+00,  9.67963181e+00,  3.45016487e+00,\n        8.68814238e+00,  7.18202577e+00,  9.77227728e-01,  2.35327441e+00,\n        4.14471688e+00,  1.23753718e+00,  5.05389993e+00,  6.95845668e+00,\n       -1.72141397e+00, -1.77751010e+00,  2.35195738e+00,  2.25264424e+00,\n       -1.92857111e+00,  1.81552829e+01,  2.00784176e+00,  3.85961900e+00,\n        7.49698182e+00,  4.24702923e+00,  6.56169600e-01, -4.37552366e-01,\n        1.41098582e+00, -8.57709329e-01,  6.59263946e+00,  5.33287128e+00,\n        7.59574171e+00,  4.81523973e+00, -1.17167991e+00,  6.98966623e+00,\n        3.54091236e+00, -2.70493914e+00,  4.86582223e-01, -2.58027914e+00,\n        4.88288500e-02,  8.38293941e+00, -4.01431949e-01,  3.94442889e+00,\n        9.09464619e+00,  8.58804802e+00,  8.85361382e+00, -4.27189820e-01,\n        3.55247891e-01,  5.84244615e+00,  7.57046178e+00,  1.05181451e+01,\n        5.85981006e+00, -5.09734712e+00,  4.63388453e+00,  5.54962287e+00,\n       -2.38918468e+00,  3.99640419e-01,  8.10230851e+00,  7.34730071e+00,\n        2.01159453e+00,  3.92006323e+00,  7.08887503e+00,  1.36327045e+00,\n        1.93933132e+00, -4.55980166e-01,  6.54794223e+00,  1.12496134e+01,\n        3.03894659e-01, -3.22562923e-01,  5.57445826e+00,  6.32048689e+00,\n        2.01678816e+00,  1.28099905e+00,  4.89623865e+00,  3.30777985e+00,\n       -1.90233247e+00,  6.90094332e+00,  8.93577212e+00,  6.27558639e+00,\n        8.89349178e+00,  3.54443893e+00,  6.40847150e+00,  1.99305324e+00,\n        5.21064730e+00,  1.04855357e+00,  7.06678571e+00,  9.07843503e+00,\n       -2.02820954e+00,  5.05051609e+00,  1.06765759e+00,  2.98603929e-01,\n        1.75792780e+00,  6.85465872e+00,  3.14005294e+00,  6.59520067e+00,\n       -1.97375594e+00,  6.21704760e+00,  3.62549933e+00,  8.47204603e+00,\n       -4.51082334e+00,  8.95684967e+00, -3.17282858e+00,  5.42292719e+00,\n       -1.04088080e-01,  5.44513261e+00,  8.25176626e+00,  7.95419974e+00,\n       -3.30852252e+00,  1.76533823e+00, -2.01718408e+00,  6.34395988e+00,\n        3.77478715e+00,  4.42890062e+00, -8.34235389e-01,  6.43935061e+00,\n        9.07394368e+00,  2.84007545e+00,  7.45564082e+00, -8.93486185e-01,\n        8.23772805e+00, -1.87721718e+00,  1.10463390e+01, -4.64258576e-02,\n        4.78343743e+00,  9.51430795e+00, -3.25529281e+00,  6.47613077e+00,\n        4.85425066e+00,  1.34023702e+00,  4.84650464e+00,  7.94414525e+00,\n        9.12959212e+00,  2.43951160e+00,  1.01169267e-01,  8.90024905e+00,\n       -5.97524003e-02,  3.52124968e+00,  8.86566655e+00,  2.73267562e+00,\n        6.51002312e+00,  8.07225937e+00,  5.52549600e+00,  2.48353404e+00,\n        7.99267137e+00,  5.32052201e-01,  8.76634992e+00,  6.30914449e-01,\n        2.87920688e+00,  7.28603021e+00, -5.58294880e-01,  4.24013311e+00,\n        5.52928306e+00,  1.90039524e+00, -5.82165610e-01,  5.51365473e+00,\n       -2.21967863e+00, -2.70951350e-01, -2.01480916e+00,  5.91292617e+00,\n        6.08393745e+00,  2.61708022e+00,  3.23950597e+00,  6.51520316e+00,\n        6.67497449e-01, -1.01084348e+00,  9.92531653e+00,  8.49256594e+00,\n        8.02302824e+00,  7.97821365e+00, -5.18740977e-01,  5.75887959e+00,\n       -5.14576637e-03,  2.74038805e+00,  8.72278819e+00,  6.82026313e+00,\n        2.60004108e+00,  9.05863915e+00,  2.84236067e+00, -1.08981141e+00,\n        1.80586420e+00,  8.10888786e+00,  9.18145560e+00,  6.70766450e-01,\n        1.13235278e+01,  8.28630120e+00, -2.17617342e+00,  3.46924297e+00,\n       -5.62191032e-01,  2.55658566e+00,  1.66314040e-01,  8.79856852e+00,\n        6.43387876e+00,  6.25472071e+00,  8.89864045e+00,  1.37648889e+00,\n        2.03553447e+00,  5.36256329e-01,  3.45894965e+00,  1.67605133e+00,\n       -3.07979663e+00,  4.82819225e-01, -3.43897667e-02,  5.76884316e+00,\n        8.40135667e+00,  8.33215339e+00,  6.13952373e+00,  8.15401770e+00,\n        5.95327489e+00,  9.33177345e+00,  3.12950851e+00,  5.73678090e+00,\n        5.88079891e+00, -6.33706514e-01,  3.46697046e+00,  5.38019905e+00,\n        3.77910930e-01,  5.49507064e+00,  7.54819257e+00,  7.44760702e+00,\n        2.79428385e+00,  7.15500997e+00,  3.35865146e+00,  7.80465001e+00,\n        5.97868296e+00,  1.22373870e+00, -3.77210391e+00,  5.86461593e+00,\n        3.70612734e+00, -9.79210193e-01, -3.48475139e+00,  2.43915326e+00])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# T-Learner (example with Lasso)\n",
    "tic = time.perf_counter()\n",
    "# mu_0\n",
    "t_learner_mu0 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_poly_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_poly_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:38:59.544356Z",
     "start_time": "2023-06-02T14:37:49.894969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for calculations: 69.58790112499992\n"
     ]
    }
   ],
   "source": [
    "print(f'Time for calculations: {toc-tic}') # took 69 seconds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:40:24.120270Z",
     "start_time": "2023-06-02T14:40:23.877742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "6.182381044567873"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:46:04.942429Z",
     "start_time": "2023-06-02T14:46:04.899508Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S-leaerner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "xw_poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_W_poly_train = poly_train_treatment.fit_transform(X_W_train)\n",
    "\n",
    "xw_poly_test_0 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_0 = xw_poly_test_0.fit_transform(X_test_0)\n",
    "\n",
    "xw_poly_test_1 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_1 = xw_poly_test_0.fit_transform(X_test_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T14:59:23.720059Z",
     "start_time": "2023-06-02T14:59:23.655464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computation; 69.74598945799971\n"
     ]
    }
   ],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "tic = time.perf_counter()\n",
    "# mu_x\n",
    "s_learner = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "s_learner.fit(X_W_poly_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_poly_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_poly_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "print(f'Time for computation: {toc-tic}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:09:10.343144Z",
     "start_time": "2023-06-02T15:08:00.574680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "4.432630658397295"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:07:47.826905Z",
     "start_time": "2023-06-02T15:07:47.751011Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## X-learner with lasso (or l1-penalty)\n",
    "### TAKES A LOT OF TIME!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# mu_0 (same procedure as for t-learner, maybe can speed up process)\u001B[39;00m\n\u001B[1;32m      6\u001B[0m x_learner_mu0 \u001B[38;5;241m=\u001B[39m LassoCV(cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mx_learner_mu0\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_poly_train_control\u001B[49m\u001B[43m,\u001B[49m\u001B[43mY_train_control\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# mu_1 (same procedure as for t-learner, maybe can speed up process)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m x_learner_mu1 \u001B[38;5;241m=\u001B[39m LassoCV(cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:1668\u001B[0m, in \u001B[0;36mLinearModelCV.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1648\u001B[0m \u001B[38;5;66;03m# We do a double for loop folded in one, in order to be able to\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m \u001B[38;5;66;03m# iterate in parallel on l1_ratio and folds\u001B[39;00m\n\u001B[1;32m   1650\u001B[0m jobs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1651\u001B[0m     delayed(_path_residuals)(\n\u001B[1;32m   1652\u001B[0m         X,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1666\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m folds\n\u001B[1;32m   1667\u001B[0m )\n\u001B[0;32m-> 1668\u001B[0m mse_paths \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1672\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1673\u001B[0m mse_paths \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(mse_paths, (n_l1_ratio, \u001B[38;5;28mlen\u001B[39m(folds), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m   1674\u001B[0m \u001B[38;5;66;03m# The mean is computed over folds.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     62\u001B[0m )\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1051\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[1;32m   1049\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1051\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1055\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[1;32m   1057\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:864\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    863\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 864\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:782\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    781\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[0;32m--> 782\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[1;32m    786\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[1;32m    787\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[1;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[0;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 263\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    264\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 263\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    264\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    121\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:1392\u001B[0m, in \u001B[0;36m_path_residuals\u001B[0;34m(X, y, sample_weight, train, test, fit_intercept, path, path_params, alphas, l1_ratio, X_order, dtype)\u001B[0m\n\u001B[1;32m   1389\u001B[0m \u001B[38;5;66;03m# Do the ordering and type casting here, as if it is done in the path,\u001B[39;00m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;66;03m# X is copied and a reference is kept here\u001B[39;00m\n\u001B[1;32m   1391\u001B[0m X_train \u001B[38;5;241m=\u001B[39m check_array(X_train, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype, order\u001B[38;5;241m=\u001B[39mX_order)\n\u001B[0;32m-> 1392\u001B[0m alphas, coefs, _ \u001B[38;5;241m=\u001B[39m \u001B[43mpath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpath_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1393\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m X_train, y_train\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1396\u001B[0m     \u001B[38;5;66;03m# Doing this so that it becomes coherent with multioutput.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:332\u001B[0m, in \u001B[0;36mlasso_path\u001B[0;34m(X, y, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, **params)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlasso_path\u001B[39m(\n\u001B[1;32m    177\u001B[0m     X,\n\u001B[1;32m    178\u001B[0m     y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    191\u001B[0m ):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute Lasso path with coordinate descent.\u001B[39;00m\n\u001B[1;32m    193\u001B[0m \n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m    The Lasso optimization function varies for mono and multi-outputs.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;124;03m     [0.2159048  0.4425765  0.23668876]]\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menet_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m        \u001B[49m\u001B[43ml1_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_alphas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_alphas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m        \u001B[49m\u001B[43malphas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malphas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprecompute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprecompute\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mXy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mXy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy_X\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoef_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoef_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpositive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpositive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_n_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_n_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631\u001B[0m, in \u001B[0;36menet_path\u001B[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001B[0m\n\u001B[1;32m    617\u001B[0m     model \u001B[38;5;241m=\u001B[39m cd_fast\u001B[38;5;241m.\u001B[39menet_coordinate_descent_gram(\n\u001B[1;32m    618\u001B[0m         coef_,\n\u001B[1;32m    619\u001B[0m         l1_reg,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    628\u001B[0m         positive,\n\u001B[1;32m    629\u001B[0m     )\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m precompute \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m--> 631\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mcd_fast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menet_coordinate_descent\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoef_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml1_reg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml2_reg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrng\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpositive\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrecompute should be one of True, False, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or array-like. Got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;241m%\u001B[39m precompute\n\u001B[1;32m    638\u001B[0m     )\n",
      "File \u001B[0;32msklearn/linear_model/_cd_fast.pyx:135\u001B[0m, in \u001B[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:46\u001B[0m, in \u001B[0;36m_sum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_amin\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     43\u001B[0m           initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m umr_minimum(a, axis, \u001B[38;5;28;01mNone\u001B[39;00m, out, keepdims, initial, where)\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     47\u001B[0m          initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prod\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     51\u001B[0m           initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### X-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_poly_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_poly_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_0_hat.fit(X_poly_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_1_hat.fit(X_poly_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = LogisticRegressionCV(cv=KFold(10), penalty='l1', tol=1, random_state=0)\n",
    "g_x_hat.fit(X_poly_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_poly_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_poly_test) + probas_0 * x_tau_1_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:25:40.941917Z",
     "start_time": "2023-06-02T15:25:18.879459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
