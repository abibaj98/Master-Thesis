{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T22:01:13.220353Z",
     "start_time": "2023-07-18T22:01:10.148746Z"
    }
   },
   "outputs": [],
   "source": [
    "from MetaLearner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import jsonpickle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:19:44.766925Z",
     "start_time": "2023-07-21T16:19:44.713127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo: https://stackoverflow.com/questions/24168569/how-to-add-names-to-a-numpy-array-without-changing-its-dimension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This file runs the experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:26:29.614633Z",
     "start_time": "2023-07-19T01:26:29.601088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:26:29.855671Z",
     "start_time": "2023-07-19T01:26:29.853969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "file_name = \"/Users/arberimbibaj/Documents/Master Thesis ETH/DataSets /Generated/data_synthetic_one_run.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "data = pickle.load(open_file)\n",
    "open_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T18:29:03.056251Z",
     "start_time": "2023-07-21T18:29:03.041213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# load with jsonpickle\n",
    "file_name = \"/Users/arberimbibaj/Documents/Master Thesis ETH/DataSets /Generated/data_all.json\"\n",
    "f = open(file_name, 'r')\n",
    "json_str = f.read()\n",
    "data = jsonpickle.decode(json_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T18:37:20.653691Z",
     "start_time": "2023-07-21T18:37:18.918495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "n_setups = 1\n",
    "sample_sizes = [500, 1000, 2000, 5000]\n",
    "n_runs = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:09:18.038944Z",
     "start_time": "2023-07-20T04:09:18.034918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learners = [TLearner('rf'), SLearner('rf'), XLearner('rf'), RLearner('rf'), DRLearner('rf'), RALearner('rf'),\n",
    "            PWLearner('rf'), ULearner('rf'),\n",
    "            TLearner('lasso'), SLearner('lasso'), XLearner('lasso'), RLearner('lasso'), DRLearner('lasso'),\n",
    "            RALearner('lasso'), PWLearner('lasso'), ULearner('lasso'),\n",
    "            TLearner('nn'), SLearner('nn'), XLearner('nn'), RLearner('nn'), DRLearner('nn'), RALearner('nn'),\n",
    "            PWLearner('nn'), ULearner('nn')]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "learners = [TLearner('nn'), SLearner('nn'), XLearner('nn'), RLearner('nn'), DRLearner('nn'), RALearner('nn'),\n",
    "            PWLearner('nn'), ULearner('nn')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T22:01:18.532538Z",
     "start_time": "2023-07-18T22:01:18.024392Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learners = [RLearner('nn')]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# What we want: For each setting and base learner --> matrix which includes the MSE, columns: Metalearners + SampleSize, rows: Run(1-10)\n",
    "# --> So we can plot the mean and 1se of the MSE for each Metalearner, each samplesize; giving the setting and baselearner\n",
    "# --> number of plots: settings*baselearner."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[], [], []]]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(n_setups):\n",
    "    results.append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for baselearner in range(3):\n",
    "        results[i].append([])\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:10:37.821694Z",
     "start_time": "2023-07-20T04:10:37.817493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:10:51.264784Z",
     "start_time": "2023-07-20T04:10:51.261277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dim = data[0][0][0][0].shape[1]  # dimension of x\n",
    "\n",
    "\n",
    "# helper function to get y, x, w, tau, out of data[][][][]\n",
    "def get_variables(dataset, setup, samplesize, run, train_test):\n",
    "    y = dataset[setup][samplesize][run][train_test][:, 0]\n",
    "    x = dataset[setup][samplesize][run][train_test][:, 1:(dim - 2)]\n",
    "    w = dataset[setup][samplesize][run][train_test][:, (dim - 2)]\n",
    "    tau = dataset[setup][samplesize][run][train_test][:, (dim - 1)]\n",
    "    return y, x, w, tau"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T22:01:20.437192Z",
     "start_time": "2023-07-18T22:01:20.434475Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: 1\n",
      "Sample Size: 1\n",
      "Run: 1\n",
      "-------------\n",
      "---\n",
      "BaseLearner: 1\n",
      "Learner 1: TLearner\n",
      "Time: 3.8361 seconds.\n",
      "Learner 2: SLearner\n",
      "Time: 1.615 seconds.\n",
      "Learner 3: XLearner\n",
      "Time: 6.4887 seconds.\n",
      "Learner 4: RLearner\n",
      "Time: 4.3666 seconds.\n",
      "Learner 5: DRLearner\n",
      "Time: 51.4826 seconds.\n",
      "Learner 6: RALearner\n",
      "Time: 5.8176 seconds.\n",
      "Learner 7: PWLearner\n",
      "Time: 2.691 seconds.\n",
      "Learner 8: ULearner\n",
      "Time: 4.9662 seconds.\n",
      "Sample Size: 2\n",
      "Run: 1\n",
      "-------------\n",
      "---\n",
      "BaseLearner: 1\n",
      "Learner 1: TLearner\n",
      "Time: 5.951 seconds.\n",
      "Learner 2: SLearner\n",
      "Time: 3.1454 seconds.\n",
      "Learner 3: XLearner\n",
      "Time: 10.4338 seconds.\n",
      "Learner 4: RLearner\n",
      "Time: 6.2985 seconds.\n",
      "Learner 5: DRLearner\n",
      "Time: 18.9648 seconds.\n",
      "Learner 6: RALearner\n",
      "Time: 5.5957 seconds.\n",
      "Learner 7: PWLearner\n",
      "Time: 3.1783 seconds.\n",
      "Learner 8: ULearner\n",
      "Time: 5.4885 seconds.\n",
      "Sample Size: 3\n",
      "Run: 1\n",
      "-------------\n",
      "---\n",
      "BaseLearner: 1\n",
      "Learner 1: TLearner\n",
      "Time: 7.0587 seconds.\n",
      "Learner 2: SLearner\n",
      "Time: 4.2848 seconds.\n",
      "Learner 3: XLearner\n",
      "Time: 15.7597 seconds.\n",
      "Learner 4: RLearner\n",
      "Time: 13.5847 seconds.\n",
      "Learner 5: DRLearner\n",
      "Time: 44.163 seconds.\n",
      "Learner 6: RALearner\n",
      "Time: 11.0494 seconds.\n",
      "Learner 7: PWLearner\n",
      "Time: 5.4954 seconds.\n",
      "Learner 8: ULearner\n",
      "Time: 11.8037 seconds.\n",
      "Sample Size: 4\n",
      "Run: 1\n",
      "-------------\n",
      "---\n",
      "BaseLearner: 1\n",
      "Learner 1: TLearner\n",
      "Time: 10.1822 seconds.\n",
      "Learner 2: SLearner\n",
      "Time: 6.2321 seconds.\n",
      "Learner 3: XLearner\n",
      "Time: 22.3088 seconds.\n",
      "Learner 4: RLearner\n",
      "Time: 17.7796 seconds.\n",
      "Learner 5: DRLearner\n",
      "Time: 87.3093 seconds.\n",
      "Learner 6: RALearner\n",
      "Time: 21.1303 seconds.\n",
      "Learner 7: PWLearner\n",
      "Time: 16.4132 seconds.\n",
      "Learner 8: ULearner\n",
      "Time: 24.2625 seconds.\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "m = 0\n",
    "s = 0\n",
    "\n",
    "for i in range(n_setups):\n",
    "    print(f'Setup: {i + 1}')\n",
    "    # array for all mses for one setup\n",
    "    setup_mse = np.empty(shape=(0, 24))\n",
    "    s = 0  # restart index for samplesize\n",
    "    for size in sample_sizes:\n",
    "        print(f'Sample Size: {s + 1}')\n",
    "        # array for all mses for one setup and samplesize.\n",
    "        size_mse = np.empty(shape=(0, 24))\n",
    "        for r in range(n_runs):\n",
    "            print(f'Run: {r + 1}')\n",
    "            print('-------------')\n",
    "            # array for all mses in one setup, samplesize and run.\n",
    "            mses = np.empty(shape=(1, 24))\n",
    "            # get data for specific setup, samplesize and run.\n",
    "            temp_y_train, temp_x_train, temp_w_train, temp_tau_train = get_variables(dataset=data, setup=i,\n",
    "                                                                                     samplesize=s, run=r,\n",
    "                                                                                     train_test=0)\n",
    "            temp_y_test, temp_x_test, temp_w_test, temp_tau_test = get_variables(dataset=data, setup=i,\n",
    "                                                                                 samplesize=s, run=r,\n",
    "                                                                                 train_test=1)\n",
    "            # restart index for metalearner\n",
    "            m = 0\n",
    "            for l in learners:\n",
    "                if m % 8 == 0:\n",
    "                    print('---')\n",
    "                    print(f'BaseLearner: {int((m / 8) + 1)}')\n",
    "                print(f'Learner {m + 1}: {l.name}')\n",
    "                tic = time.time()\n",
    "                # training and testing MetaLearner.\n",
    "                learner = l\n",
    "                learner.fit(temp_x_train, temp_y_train, temp_w_train)\n",
    "                predictions = learner.predict(temp_x_test)\n",
    "                temp_mse = ((predictions - temp_tau_test) ** 2).mean()\n",
    "                # append mse of specific metalearner to 'mses'.\n",
    "                mses[0, m] = temp_mse\n",
    "                # print time\n",
    "                toc = time.time()\n",
    "                print(f'Time: {round(toc - tic, 4)} seconds.')\n",
    "                # update index\n",
    "                m += 1\n",
    "            # append 'mses' to 'size_mse'.\n",
    "            size_mse = np.append(size_mse, mses, axis=0)\n",
    "        # append 'size_mse' to 'setup_mse'.\n",
    "        setup_mse = np.append(setup_mse, size_mse, axis=0)\n",
    "        # update index\n",
    "        s += 1\n",
    "    # append to results\n",
    "    results[i][0] = setup_mse[:, 0:8]  # random forest\n",
    "    #results[i][1] = setup_mse[:, 8:16]  # lasso\n",
    "    #results[i][2] = setup_mse[:, 16:24]  # neural network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this should make sense i guess! :)\n",
    "# TODO: CHECK IF IT MAKES IT FASTER IF EVERYTHING IS EITHER IN NP OR IN LIST/ARRAY BASE PYTHON.\n",
    "# TODO: DONE :)\n",
    "# TODO: DONE :)\n",
    "# TODO: DONE :)\n",
    "\n",
    "### IT TOOK 6M 40S FOR --> SAMPLE SIZES [200, 200], TEST SIZE = 50, 2 SETUPS, 2 RUNS LOL\n",
    "\n",
    "### 35min for all Baselearners, 1 Setup, 1 Run, 4 Sample Sizes  ()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T22:09:00.302893Z",
     "start_time": "2023-07-18T22:01:21.160805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Training tau0 and tau1 takes extremely long!! compared to mu0 and mu1 in lasso!!! probably because of the max_iter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "setup_mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set n_trees to 1000 to gain time or maybe even 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[8.82242677e-01, 1.34512945e-01, 4.13139923e-01, 3.46823828e-01,\n        1.72831295e-01, 5.51353617e-01, 6.00251483e+00, 1.04135023e+00],\n       [8.21603347e-01, 2.02012140e-01, 4.50496680e-01, 1.79659982e+00,\n        9.39436409e+01, 7.24155584e-01, 2.98456305e+01, 1.04056304e+01],\n       [6.90702094e-01, 1.56491740e-01, 4.91725492e-01, 1.77548610e+00,\n        2.96085594e+00, 6.09572286e-01, 3.72494029e+01, 1.95863895e+00],\n       [3.35695025e-01, 4.81432903e-02, 2.13545315e-01, 6.87465220e-01,\n        7.27462659e+00, 3.46110284e-01, 8.90549421e+01, 1.89862817e+00]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:29:43.426747Z",
     "start_time": "2023-07-19T01:29:43.424054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# file name\n",
    "file_name = \"/Users/arberimbibaj/Documents/Master Thesis ETH/DataSets /Generated/results.json\"\n",
    "\n",
    "# SAVE LIST AS JSON FILE\n",
    "f = open(file_name, 'w')\n",
    "json_obj = jsonpickle.encode(results)\n",
    "f.write(json_obj)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:30:48.235879Z",
     "start_time": "2023-07-19T01:30:48.224807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# open\n",
    "# LOAD JSON FILE\n",
    "f = open(file_name, 'r')\n",
    "json_str = f.read()\n",
    "results_load = jsonpickle.decode(json_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:32:05.755443Z",
     "start_time": "2023-07-19T01:32:05.730209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[8.82242677e-01, 1.34512945e-01, 4.13139923e-01, 3.46823828e-01,\n        1.72831295e-01, 5.51353617e-01, 6.00251483e+00, 1.04135023e+00],\n       [8.21603347e-01, 2.02012140e-01, 4.50496680e-01, 1.79659982e+00,\n        9.39436409e+01, 7.24155584e-01, 2.98456305e+01, 1.04056304e+01],\n       [6.90702094e-01, 1.56491740e-01, 4.91725492e-01, 1.77548610e+00,\n        2.96085594e+00, 6.09572286e-01, 3.72494029e+01, 1.95863895e+00],\n       [3.35695025e-01, 4.81432903e-02, 2.13545315e-01, 6.87465220e-01,\n        7.27462659e+00, 3.46110284e-01, 8.90549421e+01, 1.89862817e+00]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_load[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:32:28.631953Z",
     "start_time": "2023-07-19T01:32:28.627798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.885576,0.134742,0.411388,0.344136,0.128143,0.588750,5.880639,1.026057\n",
    "# 0.815042,0.201147,0.439917,1.577994,10.549566,0.710207,29.923138,10.357554\n",
    "# 0.679716,0.154199,0.399313,1.942728,0.317815,0.607700,37.089789,1.663813\n",
    "# 0.285524,0.053260,0.222201,0.769516,6.082601,0.306323,92.644239,2.648658\n",
    "\n",
    "# PW gets worse with increasing sample size, lol"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results[0][2]\n",
    "# 0.694569,0.167760,0.567150,2.997832,0.467543,0.944197,16.717111,12.709246\n",
    "# 0.610678,0.187881,0.759497,22.874956,6.021213,1.137736,58.716848,1734.143168\n",
    "# 0.864565,0.183118,0.793892,7.919859,10.800849,1.035885,72.427161,343.769983\n",
    "# 0.233476,0.074466,0.366872,3.135558,34.819663,0.473895,124.064119,1850.331233\n",
    "\n",
    "# --> PW and U-learner are bad here (why?)\n",
    "# --> looks nice otherwise :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results[0][1]\n",
    "# 2.308568,1.004442,1.526483,0.028155,2.156328,2.115060,0.036090,0.033795\n",
    "# 1.791405,1.193947,1.260019,0.044737,0.323602,1.588854,5.308641,0.042311\n",
    "# 0.315428,0.336822,0.348231,0.023158,0.071988,0.244252,0.000031,0.017461\n",
    "# 0.023137,0.060074,0.034748,0.000366,0.000355,0.008674,0.225957,0.000360\n",
    "\n",
    "# --> nice :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results[0][0]\n",
    "# 2.164445,0.004278,0.921012,3.660252,1.532306,1.524151,4.093397,4.733766\n",
    "# 3.453443,0.004395,0.898049,4.667126,1.998041,2.072289,11.121975,5.068848\n",
    "# 2.233662,0.006844,0.710762,4.107797,1.644121,1.576521,14.268573,4.718986\n",
    "# 0.385062,0.001257,0.100568,0.423951,0.210232,0.208753,16.626195,0.465520\n",
    "\n",
    "\n",
    "# --> nice :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[0][0][0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://amueller.github.io/aml/02-supervised-learning/06-linear-models-classification.html\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T04:06:38.115762Z",
     "start_time": "2023-07-20T04:06:38.112936Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 1  # = setup number\n",
    "\n",
    "# here comes the same loop\n",
    "\n",
    "# change\n",
    "\n",
    "# results[0] = ...\n",
    "# results[1] = ...\n",
    "# results[2] = ...\n",
    "\n",
    "\n",
    "# save results.setupnumber.jsonpickle lol.\n",
    "\n",
    "# --> it means 24 files :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results from all on eth cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# open\n",
    "# LOAD JSON FILE\n",
    "file_name = \"/Users/arberimbibaj/results.json\"\n",
    "f = open(file_name, 'r')\n",
    "json_str = f.read()\n",
    "results_all = jsonpickle.decode(json_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:20:34.892853Z",
     "start_time": "2023-07-21T16:20:34.878195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.91420779e-01, 1.94469960e-02, 1.94350904e-01, 1.10410404e+03,\n        1.87133219e-01, 1.84476011e-01, 1.10355854e-01, 3.47640137e+04],\n       [4.21495760e-01, 1.41175229e+00, 1.04331432e+00, 3.49327398e+02,\n        4.59807370e-01, 4.58598492e-01, 5.86746323e-01, 9.64299545e+03],\n       [3.53130519e-01, 1.84869952e-01, 4.20593556e-01, 2.86513368e+02,\n        3.71464544e-01, 3.73274726e-01, 1.64980995e-01, 6.97978561e+04],\n       [1.69484118e-01, 1.19301257e-01, 2.26812348e-01, 3.69318065e+02,\n        1.65059822e-01, 1.66206162e-01, 7.44889481e-01, 1.86449892e+04]])"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all[23][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T19:28:26.043605Z",
     "start_time": "2023-07-21T19:28:26.039329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-Learner best, X_learner good, T, DR, RA, also good, only PW-bad\n",
    "# almost same as one, PW also good?\n",
    "# X-learner best, S-learner worst (PW actually worst, but extreme)\n",
    "# X-learner best, DR/PW also good, rest oke (PW very bad)\n",
    "# T-learner best?, R/RA very good\n",
    "# X-learner best, but S/T/DR/RA all very good\n",
    "# S-learner best, then X-learner\n",
    "# S-learner best,\n",
    "# X-learner best, (especially for RF)\n",
    "# X-learner best,\n",
    "# T-learner best\n",
    "# S-learner best?, T/X/DR/RA all very good\n",
    "# S-learner best, X also very good\n",
    "# S-learner best, X also very good, R bad\n",
    "# X-learner best, DR-learner also very good\n",
    "# X-learner best, DR/RA good but then at siz=5000 bad?\n",
    "# DR/RA best, X also very good\n",
    "# S-learner best ??? probably because the cates are not scaled enough\n",
    "# S-learner best\n",
    "# S-learner best\n",
    "# X best, DR/RA very good\n",
    "# X best, DR/RA very good,\n",
    "# X best, but all perform a bit bad\n",
    "# T, S, X R DR all good, probably because of not scalling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CONCLUSIONS\n",
    "# -----------\n",
    "\n",
    "# 1: If zero cate --> S-learner is the best, X-learner also performs good\n",
    "# 2: If simple cate --> X-learner is the best, often DR/RA/R learner also perform good\n",
    "# 3: if complex cate --> T very good, somehow S-learner also good (probably to bad scalling of outcomes), DR/RA/R learner also good\n",
    "# 4: PW learner very bad mostly, U-learner also not good\n",
    "\n",
    "# Base learners\n",
    "# -------------\n",
    "# RF and Linear model seem to be more stable than Neural Network, at least with these hyperparameters.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# All \"pseudo-outome\" metalearners very bad with Neural Network??? --> probably the classification task is bad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.34471335e-01, -5.28563234e-01, -2.80141251e-01, ...,\n         6.81146622e-01,  0.00000000e+00,  1.49932657e-09],\n       [ 1.81080934e-01,  6.49321509e-01, -5.33576639e-01, ...,\n        -4.15951057e-01,  1.00000000e+00,  1.40798480e-05],\n       [ 5.53730887e-01, -5.58128760e-03,  2.92908514e-01, ...,\n        -1.90266595e-01,  0.00000000e+00,  7.11509053e-04],\n       ...,\n       [ 6.57050299e-03, -6.73184449e-01,  3.23333753e-01, ...,\n         5.75459890e-01,  0.00000000e+00,  3.29637519e-07],\n       [ 5.09824942e-02,  1.20463867e-02, -8.25670819e-01, ...,\n         4.34569234e-01,  0.00000000e+00,  1.40946009e-09],\n       [ 7.15493657e-01,  2.74820625e-01, -1.06849057e+00, ...,\n         1.56119488e-01,  1.00000000e+00,  1.68305846e-09]])"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5][0][0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T18:39:56.640423Z",
     "start_time": "2023-07-21T18:39:56.633558Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
