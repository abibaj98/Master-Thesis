{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.saving import load_model\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set global seed\n",
    "tf.keras.utils.set_random_seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/arberimbibaj/dataset_example_indicatorCATE.csv', header=None, index_col=[0])\n",
    "data = data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "d = len(data[0,:]) - 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train test split\n",
    "random.shuffle(data)\n",
    "training, test = data[:700,:], data[700:,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# slice dataset by treatment status\n",
    "training_control = training[training[:,26]==0]\n",
    "training_treatment = training[training[:,26]==1]\n",
    "\n",
    "# slice test set by treatment status\n",
    "test_control = test[test[:,26]==0]\n",
    "test_treatment = test[test[:,26]==1]\n",
    "\n",
    "# Y_train by treatment status\n",
    "Y_train_control = training_control[:,0]\n",
    "Y_train_treatment = training_treatment[:,0]\n",
    "\n",
    "# Y_test by treatment status\n",
    "Y_test_control = test_control[:,0]\n",
    "Y_test_treatment = test_treatment[:,0]\n",
    "\n",
    "# X_train by treatment status\n",
    "X_train_control = training_control[:,1:26]\n",
    "X_train_treatment = training_treatment[:,1:26]\n",
    "\n",
    "# X_test by treatment status\n",
    "X_test_control = test_control[:,1:26]\n",
    "X_test_treatment = test_treatment[:,1:26]\n",
    "\n",
    "# X and Y test\n",
    "X_test = test[:,1:26]\n",
    "Y_test = test[:,0]\n",
    "\n",
    "# X_train and Y_train (no split by treatment status)\n",
    "X_train = training[:,1:26]\n",
    "Y_train = training[:,0]\n",
    "\n",
    "# W_train and W_test\n",
    "W_train = training[:,26]\n",
    "W_test = test[:,26]\n",
    "\n",
    "# tau_test\n",
    "tau_test = test[:,27]\n",
    "tau_test_control = test_control[:,27]\n",
    "tau_test_treatment = test_treatment[:,27]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set training and test features for the S-Learner (it views W as no different from other X's)\n",
    "X_W_train = training[:,1:27]\n",
    "X_W_test = test[:,1:27]\n",
    "X_test_0 = np.concatenate((test[:,1:26],np.zeros((300,1))), axis=1)\n",
    "X_test_1 = np.concatenate((test[:,1:26],np.ones((300,1))), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train_treatment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# T-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "t_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### S-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "s_learner.fit(X_W_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "s_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_0_hat.fit(X_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "x_tau_1_hat.fit(X_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "g_x_hat.fit(X_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_test) + probas_0 * x_tau_1_hat.predict(X_test)\n",
    "x_tau_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error (much lower here!)\n",
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "r_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_train)\n",
    "r_probas_0 = r_probas[:,0] # probabilities of W=0\n",
    "r_probas_1 = r_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "r_learner_tau.fit(X_train,r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)\n",
    "r_tau_hats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "dr_learner_e_x.fit(X_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_train)\n",
    "dr_probas_0 = dr_probas[:,0] # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_mu_0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_mu_1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_train) + (1 - W_train) * dr_learner_mu_0.predict(X_train) # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(X_train) - dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = RandomForestRegressor(n_estimators=100,max_depth=100, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_train,dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "dr_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RA-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "ra_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0.predict(X_train)) + (1 - W_train)*(ra_learner_mu1.predict(X_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((ra_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PW-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "pw_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train/pw_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(pw_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((pw_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F-Learner: AKA THE SAME AS PW-LEARNER!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### F-Learner\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu0 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu0.fit(X_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "f_learner_mu1 = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_learner_mu1.fit(X_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "f_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "f_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "f_pseudo_outcome = (W_train/f_learner_e_x.predict_proba(X_train)[:,1] - (1 - W_train)/(f_learner_e_x.predict_proba(X_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "f_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "f_tau_hat_learner.fit(X_train, pw_pseudo_outcome)\n",
    "f_tau_hat = f_tau_hat_learner.predict(X_test)\n",
    "f_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "print(((f_tau_hat - tau_test)**2).mean())\n",
    "print(\"Same as for PW-Learner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "u_learner_e_x.fit(X_train,W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_learner_mu_x.fit(X_train,Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_train))/(W_train - u_learner_e_x.predict_proba(X_train)[:,1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = RandomForestRegressor(max_depth=100, random_state=0)\n",
    "u_tau_hat_learner.fit(X_train,u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "((u_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Just some lasso tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_poly = LassoCV(cv=10, random_state=0, tol=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# degree 3: 10 seconds to fit (100% cpu)\n",
    "# degree 4: 90 seconds to fit (100% cpu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predictions = lasso_poly.predict(X_poly_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((y_predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# just some Neural Network test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(d,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, start_from_epoch=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training Model\")\n",
    "training = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Generate predictions for test samples\")\n",
    "predictions = np.reshape(model.predict(X_test),(300,))\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, Y_test, batch_size=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((predictions - Y_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRY SAME WITH LASSO AND NEURAL NETWORK!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Lasso (or L1-loss for logistic regression)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_train = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train = poly_train.fit_transform(X_train)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)\n",
    "X_poly_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "poly_train_treatment = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_treatment = poly_train_treatment.fit_transform(X_train_treatment)\n",
    "poly_train_control = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_train_control = poly_train_treatment.fit_transform(X_train_control)\n",
    "poly_test = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly_test = poly_test.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_poly_train_control"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_poly_train_treatment\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T-learner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# T-Learner (example with Lasso)\n",
    "tic = time.perf_counter()\n",
    "# mu_0\n",
    "t_learner_mu0 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_poly_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "t_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_poly_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "t_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Time for calculations: {toc-tic}') # took 69 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((t_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S-learner Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute polynomial features for treatment and control groups in training set\n",
    "xw_poly_train = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_W_poly_train = poly_train_treatment.fit_transform(X_W_train)\n",
    "\n",
    "xw_poly_test_0 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_0 = xw_poly_test_0.fit_transform(X_test_0)\n",
    "\n",
    "xw_poly_test_1 = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n",
    "X_poly_test_1 = xw_poly_test_0.fit_transform(X_test_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "tic = time.perf_counter()\n",
    "# mu_x\n",
    "s_learner = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "s_learner.fit(X_W_poly_train,Y_train)\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_poly_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_poly_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n",
    "toc = time.perf_counter()\n",
    "print(f'Time for computation: {toc-tic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((s_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## X-learner with lasso (or l1-penalty)\n",
    "### TAKES A LOT OF TIME!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# compute imputed treatment effect D_0 and D_1\n",
    "# d_0\n",
    "imputed_0 = x_learner_mu1.predict(X_poly_train_control) - Y_train_control\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - x_learner_mu0.predict(X_poly_train_treatment)\n",
    "\n",
    "# regress imputed on X\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_0_hat.fit(X_poly_train_control ,imputed_0)\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "x_tau_1_hat.fit(X_poly_train_treatment ,imputed_1)\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "g_x_hat.fit(X_poly_train,W_train)\n",
    "probabilities = g_x_hat.predict_proba(X_poly_test)\n",
    "probas_1 = probabilities[:,1]\n",
    "probas_0 = probabilities[:,0]\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = probas_1 * x_tau_0_hat.predict(X_poly_test) + probas_0 * x_tau_1_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic}') # 127 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((x_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## R-learner with lasso (or l1-penalty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "r_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# get e_x predictions\n",
    "r_probas = r_learner_e_x.predict_proba(X_poly_train)\n",
    "r_probas_0 = r_probas[:,0] # probabilities of W=0\n",
    "r_probas_1 = r_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_mu_x.fit(X_poly_train,Y_train)\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - r_learner_mu_x.predict(X_poly_train)) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = LassoCV(cv=10, tol=1, random_state=0)\n",
    "r_learner_tau.fit(X_poly_train,r_learner_pseudo_outcomes, sample_weight=r_learner_weights)\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_poly_test)\n",
    "r_tau_hats\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds') # 98 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((r_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dr-learner with lasso (l1-penalty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "dr_learner_e_x.fit(X_poly_train, W_train)\n",
    "\n",
    "dr_probas = dr_learner_e_x.predict_proba(X_poly_train)\n",
    "dr_probas_0 = dr_probas[:,0] # probabilities of W=0\n",
    "dr_probas_1 = dr_probas[:,1] # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_mu_1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1.predict(X_poly_train) + (1 - W_train) * dr_learner_mu_0.predict(X_poly_train) # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1.predict(X_poly_train) - dr_learner_mu_0.predict(X_poly_train)\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = LassoCV(cv=10, tol=1, random_state=0)\n",
    "dr_learner_tau_hat.fit(X_poly_train,dr_pseudo_outcomes)\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc-tic} seconds') # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((dr_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ra-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "ra_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "ra_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0.predict(X_poly_train)) + (1 - W_train)*(ra_learner_mu1.predict(X_poly_train) - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "ra_tau_hat_learner.fit(X_poly_train, ra_pseudo_outcome)\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 121 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((ra_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PW-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu0 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu0.fit(X_poly_train_control,Y_train_control)\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "pw_learner_mu1 = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_learner_mu1.fit(X_poly_train_treatment,Y_train_treatment)\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "pw_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train/pw_learner_e_x.predict_proba(X_poly_train)[:,1] - (1 - W_train)/(pw_learner_e_x.predict_proba(X_poly_train)[:,0]))*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "pw_tau_hat_learner.fit(X_poly_train, pw_pseudo_outcome)\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 117 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((pw_tau_hat - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## U-learner with lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# estimate e_x\n",
    "u_learner_e_x = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "u_learner_e_x.fit(X_poly_train,W_train)\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_learner_mu_x.fit(X_poly_train,Y_train)\n",
    "\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x.predict(X_poly_train))/(W_train - u_learner_e_x.predict_proba(X_poly_train)[:,1])\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = LassoCV(cv=10, tol=1, random_state=0)\n",
    "u_tau_hat_learner.fit(X_poly_train,u_learner_residuals)\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_poly_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 98 seconds\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((u_tau_hats - tau_test)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# now do it with neural networks\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model_25 = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_25.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_25.save('model_25')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# same model, but input shape=26, for t-learner only\n",
    "# 3 layers with 200 units (elu activation), 2 layers with 100 units (elu activations), 1 output layer (linear activation)\n",
    "model_26 = keras.Sequential([\n",
    "    keras.Input(shape=(26,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_26.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.MeanSquaredError()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_26.save('model_26')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ex = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"linear\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network_Classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_ex.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.5),\n",
    "    # List of metrics to monitor\n",
    "    metrics=keras.metrics.BinaryAccuracy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_ex.save('model_ex')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, start_from_epoch=30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ex_sigmoid = keras.Sequential([\n",
    "    keras.Input(shape=(25,)),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(units=200, activation=\"relu\", name=\"layer3\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer4\"),\n",
    "    layers.Dense(units=100, activation=\"relu\", name=\"layer5\"),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\", name=\"layer6\"),\n",
    "\n",
    "], name=\"Dense_Neural_Network_Classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_ex_sigmoid.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.5),\n",
    "    # List of metrics to monitor\n",
    "    metrics=keras.metrics.BinaryAccuracy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_ex_sigmoid.save('model_ex_sigmoid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# T-Learner (example with Random Forest)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# mu_0\n",
    "t_learner_mu0 = load_model('model_25')\n",
    "print('Training mu0')\n",
    "t_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "t_mu_0_hat = t_learner_mu0.predict(X_test)\n",
    "\n",
    "# mu_1\n",
    "t_learner_mu1 = load_model('model_25')\n",
    "print('Training mu1')\n",
    "t_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "t_mu_1_hat = t_learner_mu1.predict(X_test)\n",
    "\n",
    "# Prediction = mu_1 - mu_0\n",
    "t_tau_hat = t_mu_1_hat - t_mu_0_hat\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time for computation: {toc-tic} seconds.') # 3 seconds\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(t_tau_hat,(300,)) - tau_test)**2).mean() # 3.18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S-learner with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# S-learner (example with Random Forest)\n",
    "\n",
    "# mu_x\n",
    "s_learner = load_model('model_26')\n",
    "s_learner.fit(X_W_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_W_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# mu_0_hat\n",
    "s_mu_0_hat = s_learner.predict(X_test_0)\n",
    "\n",
    "# mu_1_hat\n",
    "s_mu_1_hat = s_learner.predict(X_test_1)\n",
    "\n",
    "# tau_hat\n",
    "s_tau_hat = s_mu_1_hat - s_mu_0_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(s_tau_hat,(300,)) - tau_test)**2).mean() # 1.98"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### X-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu0 = load_model('model_25')\n",
    "x_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# d_1\n",
    "imputed_1 = Y_train_treatment - np.reshape(x_learner_mu0.predict(X_train_treatment),(len(Y_train_treatment),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "x_learner_mu1 = load_model('model_25')\n",
    "x_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# d_0\n",
    "imputed_0 = np.reshape(x_learner_mu1.predict(X_train_control),(len(Y_train_control),)) - Y_train_control\n",
    "\n",
    "\n",
    "# regress imputed on X\n",
    "\n",
    "# tau_hat_1\n",
    "x_tau_1_hat = load_model('model_25')\n",
    "x_tau_1_hat.fit(X_train_treatment,imputed_1,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, tau_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "x_tau_1_hat_predicts = np.reshape(x_tau_1_hat.predict(X_test),(len(X_test),))\n",
    "\n",
    "# tau_hat_0\n",
    "x_tau_0_hat = load_model('model_25')\n",
    "x_tau_0_hat.fit(X_train_control,imputed_0,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, tau_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "x_tau_0_hat_predicts = np.reshape(x_tau_0_hat.predict(X_test),(len(X_test),))\n",
    "\n",
    "# estimate e_x to use as g_x\n",
    "g_x_hat = load_model('model_ex')\n",
    "g_x_hat.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "x_probabilities = g_x_hat.predict(X_test)\n",
    "x_probs_1 = np.reshape(keras.activations.sigmoid(x_probabilities),(len(x_probabilities,)))\n",
    "x_probs_0 = 1 - x_probs_1\n",
    "\n",
    "# final estimator of tau\n",
    "x_tau_hat = x_probs_1 * x_tau_0_hat_predicts + x_probs_0 * x_tau_1_hat_predicts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(x_tau_hat,(300,)) - tau_test)**2).mean() # 3.1614 with smoothing of 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-learner with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### R-Learner\n",
    "\n",
    "# estimate e_x\n",
    "r_learner_e_x = load_model('model_ex')\n",
    "r_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get e_x predictions\n",
    "r_probabilities = np.reshape(keras.activations.sigmoid(r_learner_e_x.predict(X_train)),len(X_train,))\n",
    "r_probas_1 = r_probabilities# probabilities of W=1\n",
    "r_probas_0 = 1 - r_probabilities # probabilities of W=0\n",
    "\n",
    "# estimate mu_x\n",
    "r_learner_mu_x = load_model('model_25')\n",
    "r_learner_mu_x.fit(X_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# compute r-pseudo-outcome and weights\n",
    "r_learner_pseudo_outcomes = (Y_train - np.reshape(r_learner_mu_x.predict(X_train),(len(X_train),))) / (W_train - r_probas_1)\n",
    "r_learner_weights = (W_train - r_probas_1)**2\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X, weight by (W-e(x))^2)\n",
    "r_learner_tau = load_model('model_25')\n",
    "r_learner_tau.fit(X_train,r_learner_pseudo_outcomes,\n",
    "    sample_weight=r_learner_weights,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# predict tau\n",
    "r_tau_hats = r_learner_tau.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(r_tau_hats,(len(X_test))) - tau_test)**2).mean() #47.81"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's reproducible now!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DR-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DR-Learner\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# TODO: APPLY CROSS-FITTING?\n",
    "# estimate e_x\n",
    "dr_learner_e_x = load_model('model_ex')\n",
    "dr_learner_e_x.fit(X_train, W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_probabilities = np.reshape(keras.activations.sigmoid(dr_learner_e_x.predict(X_train)),len(X_train,))\n",
    "dr_probas_0 = 1 - dr_probabilities # probabilities of W=0\n",
    "dr_probas_1 = dr_probabilities # probabilities of W=1\n",
    "\n",
    "# estimate mu_0\n",
    "dr_learner_mu_0 = load_model('model_25')\n",
    "dr_learner_mu_0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_learner_mu_0_predictions = dr_learner_mu_0.predict(X_train)\n",
    "\n",
    "# estimate mu_1\n",
    "dr_learner_mu_1 = load_model('model_25')\n",
    "dr_learner_mu_1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "dr_learner_mu_1_predictions = dr_learner_mu_1.predict(X_train)\n",
    "\n",
    "# DR-pseudo-outcomes\n",
    "mu_w = W_train * dr_learner_mu_1_predictions + (1 - W_train) * dr_learner_mu_0_predictions # this is mu_w for each observation, i.e mu_1 for units in the treatment groups, and mu_0 for units in the control group\n",
    "dr_pseudo_outcomes = (W_train - dr_probas_1) / (dr_probas_1 * dr_probas_0) * (Y_train - mu_w) + dr_learner_mu_1_predictions - dr_learner_mu_0_predictions\n",
    "\n",
    "# estimate tau (regress pseudo-outcomes on X) # TODO: USE \"Test Set\" for this estimation\n",
    "dr_learner_tau_hat = load_model('model_25')\n",
    "dr_learner_tau_hat.fit(X_train,dr_pseudo_outcomes,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# predict tau\n",
    "dr_tau_hat = dr_learner_tau_hat.predict(X_test)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f'Time needed for computation: {toc-tic} seconds') # 104 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(dr_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 8.3514"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RA-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RA-Learner\n",
    "\n",
    "# mu_0 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu0 = load_model('model_25')\n",
    "ra_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu0_predictions = np.reshape(ra_learner_mu0.predict(X_train),(len(X_train),))\n",
    "\n",
    "# mu_1 (same procedure as for t-learner, maybe can speed up process)\n",
    "ra_learner_mu1 = load_model('model_25')\n",
    "ra_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "# get hats\n",
    "ra_learner_mu1_predictions = np.reshape(ra_learner_mu1.predict(X_train),(len(X_train),))\n",
    "\n",
    "\n",
    "# e_x TODO: IS IT NEEDED?\n",
    "\"\"\"ra_learner_e_x = load_model('model_ex')\n",
    "ra_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=[callback] # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "ra_pseudo_outcome = W_train*(Y_train - ra_learner_mu0_predictions) + (1 - W_train)*(ra_learner_mu1_predictions - Y_train)\n",
    "\n",
    "# tau_hat\n",
    "ra_tau_hat_learner = load_model('model_25')\n",
    "ra_tau_hat_learner.fit(X_train, ra_pseudo_outcome,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "ra_tau_hat = ra_tau_hat_learner.predict(X_test)\n",
    "ra_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(ra_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 3.397"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PW-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PW-Learner\n",
    "# mu_0 TODO: really needed\n",
    "# pw_learner_mu0 = load_model('model_25')\n",
    "\"\"\"pw_learner_mu0.fit(X_train_control,Y_train_control,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_control, Y_test_control),\n",
    "    callbacks=None # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# mu_1 TODO: really needed?\n",
    "#pw_learner_mu1 = load_model('model_25')\n",
    "\"\"\"pw_learner_mu1.fit(X_train_treatment,Y_train_treatment,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_treatment, Y_test_treatment),\n",
    "    callbacks=None # include early stopping\n",
    ")\"\"\"\n",
    "\n",
    "# e_x\n",
    "pw_learner_e_x = load_model('model_ex')\n",
    "pw_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "pw_probabilities = np.reshape(keras.activations.sigmoid(pw_learner_e_x.predict(X_train)),len(X_train,))\n",
    "pw_probs_1 = pw_probabilities\n",
    "pw_probs_0 = 1 - pw_probabilities\n",
    "\n",
    "# ra-pseudo-outcome\n",
    "pw_pseudo_outcome = (W_train / pw_probs_1 - (1 - W_train) / pw_probs_0)*Y_train\n",
    "\n",
    "# tau_hat\n",
    "pw_tau_hat_learner = load_model('model_25')\n",
    "pw_tau_hat_learner.fit(X_train, pw_pseudo_outcome,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "pw_tau_hat = pw_tau_hat_learner.predict(X_test)\n",
    "pw_tau_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(pw_tau_hat,(len(tau_test),)) - tau_test)**2).mean() # 271.842 TODO: CHECK IF IT REALLY IS CORRECT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-learner with Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### U-Learner\n",
    "# estimate e_x\n",
    "u_learner_e_x = load_model('model_ex')\n",
    "u_learner_e_x.fit(X_train,W_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, W_test),\n",
    "    callbacks=None # include early stopping\n",
    ")\n",
    "\n",
    "u_probs_1 = np.reshape(u_learner_e_x.predict(X_train),(len(X_train),))\n",
    "u_probs_0 = 1 - u_probs_1\n",
    "\n",
    "# estimate mu_x\n",
    "u_learner_mu_x = load_model('model_25')\n",
    "u_learner_mu_x.fit(X_train,Y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test,Y_test),\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "u_learner_mu_x_predictions = np.reshape(u_learner_mu_x.predict(X_train),(len(X_train),))\n",
    "# compute residuals\n",
    "u_learner_residuals = (Y_train - u_learner_mu_x_predictions)/(W_train - u_probs_1)\n",
    "\n",
    "# tau_hat - regress residuals on X\n",
    "u_tau_hat_learner = load_model('model_25')\n",
    "u_tau_hat_learner.fit(X_train,u_learner_residuals,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_data=None,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "u_tau_hats = u_tau_hat_learner.predict(X_test)\n",
    "u_tau_hats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "((np.reshape(u_tau_hats,(len(tau_test),)) - tau_test)**2).mean() # 74893.80 TODO: CHECK IF IT REALLY IS CORRECT!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### STABLE ESTIMATOR? SINCE EXTREME VALUES FOR ESTIMATED PROBABILITIES!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Metalearner():\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class T Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):# TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1])\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w==0], y[w==0])\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w==1], y[w==1])\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0],\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           validation_data=(X_test_control, Y_test_control),\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1],\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           validation_data=(X_test_treatment, Y_test_treatment),\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x): # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x)\n",
    "            mu1_hats = self.mu1_model.predict(x)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of mu_1 & mu_0\n",
    "            mu0_hats = self.mu0_model.predict(x_poly_test)\n",
    "            mu1_hats = self.mu1_model.predict(x_poly_test)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            mu0_hats = self.mu0_model.predict(x, verbose=0)\n",
    "            mu1_hats = self.mu1_model.predict(x, verbose=0)\n",
    "            predictions = np.reshape(mu1_hats-mu0_hats,(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_nn = TLearner(method='lasso')\n",
    "t_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = t_nn.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 10.091322530886687\n",
    "# lasso: 5.583461099392904\n",
    "# nn: 3.1867804239471273"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class S Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=0)\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1e-2, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_26')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w): # TODO: training process\n",
    "        x_w = np.concatenate((x,np.reshape(w,(len(w),1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting random forest for mu_x\")\n",
    "            self.mux_model.fit(x_w, y)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x_w)\n",
    "\n",
    "            # 1: train mu_x\n",
    "            print(\"Fitting lasso for mu_x\")\n",
    "            self.mux_model.fit(x_poly_train, y)\n",
    "\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_x\n",
    "            print(\"Training neural network for mu_x\")\n",
    "            self.mux_model.fit(x_w, y,\n",
    "                           batch_size=100,\n",
    "                           epochs=100,\n",
    "                           callbacks=None, # include early stopping\n",
    "                           verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x, w): # TODO:\n",
    "        x_0 = np.concatenate((x,np.zeros((len(x),1))), axis=1)\n",
    "        x_1 = np.concatenate((x,np.ones((len(x),1))), axis=1)\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_0)\n",
    "            mu1_hats = self.mux_model.predict(x_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_0 = self.poly.fit_transform(x_0)\n",
    "            x_poly_1 = self.poly.fit_transform(x_1)\n",
    "\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_poly_0)\n",
    "            mu1_hats = self.mux_model.predict(x_poly_1)\n",
    "            predictions = mu1_hats - mu0_hats\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: calculate hats of mu_x with X and W=1 or W=0\n",
    "            mu0_hats = self.mux_model.predict(x_0, verbose=0)\n",
    "            mu1_hats = self.mux_model.predict(x_1, verbose=0)\n",
    "            predictions = np.reshape(mu1_hats-mu0_hats,(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_nn = SLearner('lasso')\n",
    "s_nn.fit(X_train, Y_train, W_train)\n",
    "predictions = s_nn.predict(X_test, W_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 18.134009488483855\n",
    "# lasso: 5.559126710289806\n",
    "# nn: 1.987529792077956"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class X Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class XLearner(): # TODO: comment what is what.\n",
    "    def __init__(self, method): # TODO: or maybe not give base_learners but method, i.e. : 'lasso', 'rf' or 'nn'\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.tau1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau0_model = load_model('model_25')\n",
    "            self.tau1_model = load_model('model_25')\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified')\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x, y, w):# TODO: training process\n",
    "        if self.method == 'rf':\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting random forest for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0])\n",
    "            imputed_1 = y[w==1] - self.mu0_model.predict(x[w==1])\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting random forest for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1])\n",
    "            imputed_0 = self.mu1_model.predict(x[w==0]) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w==0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w==1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x,w)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: train mu_0 and get imputed_1\n",
    "            print(\"Fitting lasso for mu_0\")\n",
    "            self.mu0_model.fit(x_poly_train[w==0], y[w==0])\n",
    "            imputed_1 = y[w==1] - self.mu0_model.predict(x_poly_train[w==1])\n",
    "\n",
    "\n",
    "            # 2: train mu_1 and get imputed_0\n",
    "            print(\"Fitting lasso for mu_1\")\n",
    "            self.mu1_model.fit(x_poly_train[w==1], y[w==1])\n",
    "            imputed_0 = self.mu1_model.predict(x_poly_train[w==0]) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x_poly_train[w==0], imputed_0)\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x_poly_train[w==1], imputed_1)\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: train mu_0\n",
    "            print(\"Training neural network for mu_0\")\n",
    "            self.mu0_model.fit(x[w==0], y[w==0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None, # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_1 = y[w==1] - np.reshape(self.mu0_model.predict(x[w==1], verbose=0),(len(x[w==1]),))\n",
    "\n",
    "            # 2: train mu_1\n",
    "            print(\"Training neural network for mu_1\")\n",
    "            self.mu1_model.fit(x[w==1], y[w==1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None, # include early stopping\n",
    "                               verbose=0\n",
    "                               )\n",
    "            imputed_0 = np.reshape(self.mu1_model.predict(x[w==0], verbose=0),(len(x[w==0]),)) - y[w==0]\n",
    "\n",
    "            # 3: train tau_0\n",
    "            print(\"Fitting random forest for tau_0\")\n",
    "            self.tau0_model.fit(x[w==0], imputed_0,\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None, # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 4: train tau_1\n",
    "            print(\"Fitting random forest for tau_1\")\n",
    "            self.tau1_model.fit(x[w==1], imputed_1,\n",
    "                                batch_size=100,\n",
    "                                epochs=100,\n",
    "                                callbacks=None, # include early stopping\n",
    "                                verbose=0\n",
    "                                )\n",
    "\n",
    "            # 5: train e_x\n",
    "            print(\"Fitting random forest for e_x\")\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None, # include early stopping\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,\n",
    "                x): # TODO:\n",
    "        if self.method == 'rf':\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x)\n",
    "            tau_1_hats = self.tau1_model.predict(x)\n",
    "            # 2: probabilities\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            # 3: final predictions\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            # make polynomial features\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = self.tau0_model.predict(x_poly_test)\n",
    "            tau_1_hats = self.tau1_model.predict(x_poly_test)\n",
    "            probs = self.ex_model.predict_proba(x_poly_test)[:,1]\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            # 1: calculate hats of tau_0 and tau_1\n",
    "            tau_0_hats = np.reshape(self.tau0_model.predict(x, verbose=0),(len(x),))\n",
    "            tau_1_hats = np.reshape(self.tau1_model.predict(x, verbose=0),(len(x),))\n",
    "            # 2: probabilities\n",
    "            logit = self.ex_model.predict(x, verbose=0)\n",
    "            probs = np.reshape(keras.activations.sigmoid(logit),(len(logit,)))\n",
    "            # 3: final predictions\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        predictions = probs * tau_0_hats + (1 - probs) * tau_1_hats\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_rf = XLearner('nn')\n",
    "x_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = x_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: # 3.1369636408859614 --> same (good)\n",
    "# lasso: # nn: 7.667219448077926 --> same (good)\n",
    "# nn: 3.161416602361538 --> same (good)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# R-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RLearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'rf':\n",
    "            self.mux_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mux_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mux_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting random forest for mu_x')\n",
    "            self.mux_model.fit(x,y)\n",
    "\n",
    "            print('Fitting random forest for e_x')\n",
    "            # 2: fit ex\n",
    "            self.ex_model.fit(x,w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x)) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            print('Fitting random forest for tau_x')\n",
    "            # 4: fit tau\n",
    "            self.tau_model.fit(x,pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Fitting lasso for mu_x')\n",
    "            self.mux_model.fit(x_poly_train,y)\n",
    "\n",
    "            # 2: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "            pseudo_outcomes = (y - self.mux_model.predict(x_poly_train)) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes, sample_weight=weights)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_x\n",
    "            print('Training NN for mu_x')\n",
    "            self.mux_model.fit(x,y,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "            # 2: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            # 3: calculate pseudo_outcomes & weights\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "            pseudo_outcomes = (y - np.reshape(self.mux_model.predict(x, verbose=0),(len(x),))) / (w - probs)\n",
    "            weights = (w - probs)**2\n",
    "\n",
    "            # 4: fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                               sample_weight=weights,\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               validation_data=None,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in fit')\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_rf = RLearner('nn')\n",
    "r_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = r_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 17.722925118749608\n",
    "# lasso: 5.50038865455844\n",
    "# nn: 47.81939839016621"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class DR-Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DRLearner():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        if method == 'rf':\n",
    "            self.mu0_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.mu1_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.ex_model = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "            self.tau_model = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "        elif method == 'lasso':\n",
    "            self.mu0_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.mu1_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.ex_model = LogisticRegressionCV(cv=KFold(10), penalty='l1', solver='saga', tol=1, random_state=0)\n",
    "            self.tau_model = LassoCV(cv=10, tol=1, random_state=0)\n",
    "            self.poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "        elif method == 'nn':\n",
    "            self.mu0_model = load_model('model_25')\n",
    "            self.mu1_model = load_model('model_25')\n",
    "            self.ex_model = load_model('model_ex')\n",
    "            self.tau_model = load_model('model_25')\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified or typo')\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y, w):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting random forest for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting random forest for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting random forest for e_x')\n",
    "            self.ex_model.fit(x,w)\n",
    "            probs = self.ex_model.predict_proba(x)[:,1]\n",
    "            neg_prob = self.ex_model.predict_proba(x)[:,0]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x) + (1 - w) * self.mu0_model.predict(x)\n",
    "            pseudo_outcomes = (w - probs) / (probs * neg_prob) * (y - mu_w) + self.mu1_model.predict(x) - self.mu0_model.predict(x)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting random forest for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_train = self.poly.fit_transform(x)\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Fitting lasso for mu_0')\n",
    "            self.mu0_model.fit(x_poly_train[w==0],y[w==0])\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Fitting lasso for mu_1')\n",
    "            self.mu1_model.fit(x_poly_train[w==1],y[w==1])\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Fitting lasso for e_x')\n",
    "            self.ex_model.fit(x_poly_train,w)\n",
    "            probs = self.ex_model.predict_proba(x_poly_train)[:,1]\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x_poly_train) + (1 - w) * self.mu0_model.predict(x_poly_train)\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + self.mu1_model.predict(x_poly_train) - self.mu0_model.predict(x_poly_train)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Fitting lasso for tau_x')\n",
    "            self.tau_model.fit(x_poly_train,pseudo_outcomes)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "\n",
    "            # 1: fit mu_0\n",
    "            print('Training NN for mu_0')\n",
    "            self.mu0_model.fit(x[w==0],y[w==0],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 2: fit mu_1\n",
    "            print('Training NN for mu_1')\n",
    "            self.mu1_model.fit(x[w==1],y[w==1],\n",
    "                               batch_size=100,\n",
    "                               epochs=100,\n",
    "                               callbacks=None,\n",
    "                               verbose=0\n",
    "                               )\n",
    "\n",
    "            # 3: fit ex\n",
    "            print('Training NN for e_x')\n",
    "            self.ex_model.fit(x,w,\n",
    "                              batch_size=100,\n",
    "                              epochs=100,\n",
    "                              callbacks=None,\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "            probs = np.reshape(keras.activations.sigmoid(self.ex_model.predict(x, verbose=0)),len(x,))\n",
    "\n",
    "            # calculate pseudo_outcomes\n",
    "            mu_w = w * self.mu1_model.predict(x, verbose=0) + (1 - w) * self.mu0_model.predict(x, verbose=0)\n",
    "            pseudo_outcomes = (w - probs) / (probs * (1 - probs)) * (y - mu_w) + self.mu1_model.predict(x, verbose=0) - self.mu0_model.predict(x, verbose=0)\n",
    "\n",
    "            # 4 fit tau\n",
    "            print('Training NN for tau_x')\n",
    "            self.tau_model.fit(x,pseudo_outcomes,\n",
    "                                   batch_size=100,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=None,\n",
    "                                   callbacks=None,\n",
    "                                   verbose=0\n",
    "                                   )\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.method == 'rf':\n",
    "            predictions = self.tau_model.predict(x)\n",
    "\n",
    "\n",
    "        elif self.method == 'lasso':\n",
    "            x_poly_test = self.poly.fit_transform(x)\n",
    "            predictions = self.tau_model.predict(x_poly_test)\n",
    "\n",
    "        elif self.method == 'nn':\n",
    "            predictions = np.reshape(self.tau_model.predict(x, verbose=0),(len(x),))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Base learner method not specified in predict')\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dr_rf = DRLearner('rf')\n",
    "dr_rf.fit(X_train, Y_train, W_train)\n",
    "predictions = dr_rf.predict(X_test)\n",
    "((predictions - tau_test)**2).mean()\n",
    "# rf: 5.385491721300538 # why different??? ---> because if you take 1 - probs its not exactly the same as taking the [:,0] column!!\n",
    "# lasso: 6.252082321980517\n",
    "# nn: 8.35142898943478"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHECK THIS: CHANGE (1 - PROBS) TO [:,0] TO BE MORE EXACT!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RALearner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        # 1: fit mu_0\n",
    "        # 2: fit mu_1\n",
    "        # calculate pseudo_outcomes\n",
    "        # 4 fit tau\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # 1: prediction: tau_hat\n",
    "        pass\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PWLearner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        # 1: fit ex\n",
    "        # 2: calculate pseudo_outcomes\n",
    "        # 3: fit tau\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # 1: prediction: tau_hat\n",
    "        pass\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ULearner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        # 1: fit mu_x\n",
    "        # 2: fit ex\n",
    "        # 3: calculate residuals\n",
    "        # 4: fit tau\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # 1: prediction: tau_hat\n",
    "        pass\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
