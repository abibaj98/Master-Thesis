{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import r packages if needed\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "rpackages.importr(\"clusterGeneration\")\n",
    "rpackages.importr(\"mpower\")\n",
    "rpackages.importr(\"base\")\n",
    "rpackages.importr(\"utils\")\n",
    "cluster_generation = robjects.packages.importr(\"clusterGeneration\")\n",
    "mpower = robjects.packages.importr(\"mpower\")\n",
    "base = robjects.packages.importr(\"base\")\n",
    "utils = robjects.packages.importr(\"utils\")\n",
    "\n",
    "# set seed from R?\n",
    "r = robjects.r\n",
    "set_seed = r('set.seed')\n",
    "from numpy import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todos\n",
    "# TODO 1: decide which parameters you include in specific functions (e.g. for beta confounded ex), check\n",
    "# TODO 2: CLEAN CODE !\n",
    "# TODO 3: Check if it makes sense to change betaconfoundedness ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds for reproducibility --> NICE :-D\n",
    "set_seed(8953)\n",
    "random.seed(8953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATION SETUP: \"META-PARAMETERS\"\n",
    "\n",
    "sample_sizes = [1000, 2000, 5000, 10000]\n",
    "test_size = 1000\n",
    "runs = 30\n",
    "n_setups = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fully Synthetic Data Sets (Set up from Künzel et al.)\n",
    "\n",
    "# 1: Simulate the d-dimensional X.\n",
    "# 2: Create Potential Outcomes Y(1) and Y(0).\n",
    "# 3: Simulate Treatment Assignments trough W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Simulate the d-dimensional X\n",
    "\n",
    "# Setup parameters\n",
    "d = 25\n",
    "N = 1000\n",
    "\n",
    "# X Correlation matrix and mean\n",
    "mean = np.zeros(d)\n",
    "cov = np.array(mpower.cvine(d=d, alpha=0.5, beta=0.5))  # TODO: set cov according to setup\n",
    "\n",
    "# Simulate X\n",
    "X = random.multivariate_normal(mean=mean, cov=cov, size=N, check_valid='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  # reproducibility: first value = 0.761312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Create Potential Outcomes Y(1) and Y(0).\n",
    "\n",
    "# 2.1 Simulate errors\n",
    "e_0 = random.normal(loc=0.0, scale=1.0, size=N)\n",
    "e_1 = random.normal(loc=0.0, scale=1.0, size=N)\n",
    "\n",
    "# TODO: if needed take student_t distributed errors to see simulation with heavy-tailed errors\n",
    "\n",
    "# e_o = random.standard_t(df=1,size=N)\n",
    "# e_1 = random.standard_t(df=1,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Chaos here --> Clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create Response Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function\n",
    "def simple_indicator_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas) + 5 * np.int8(x_train[:, 0] > 0.5)\n",
    "    mu_1 = mu_0 + 8 * np.int8(x_train[:, 1] > 0.1)  # indicator\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas) + 5 * np.int8(x_test[:, 0] > 0.5)\n",
    "    mu_1_test = mu_0_test + 8 * np.int8(x_test[:, 1] > 0.1)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a, b, c, d, e, f, g = simple_indicator_cate(X, X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI1.1 simple cate - indicator  (no confounding)\n",
    "betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "mu_0 = np.matmul(X, betas_0) + 5 * np.int8(X[:, 0] > 0.5)\n",
    "mu_1 = mu_0 + 8 * np.int8(X[:, 1] > 0.1)  # indicator\n",
    "tau = mu_1 - mu_0\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function\n",
    "def simple_linear_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas) + 5 * x_train[:, 0]\n",
    "    mu_1 = mu_0 + 8 * x_train[:, 1]\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas) + 5 * x_test[:, 0]\n",
    "    mu_1_test = mu_0_test + 8 * x_test[:, 1]\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI1.2 simple cate - linear (no confounding)\n",
    "betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "mu_0 = np.matmul(X, betas_0) + 5 * X[:, 0]\n",
    "mu_1 = mu_0 + 8 * X[:, 1]  # linear\n",
    "tau = mu_1 - mu_0\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function # TODO: what does betas???\n",
    "def simple_quadratic_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = 2 * x_train[:, 0] - 1\n",
    "    mu_1 = mu_0 + 2 * (x_train[:, 1] ** 2)\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = 2 * x_test[:, 0] - 1\n",
    "    mu_1_test = mu_0_test + 2 * (x_test[:, 1] ** 2)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI1.3 simple cate - quadratic (no confounding)\n",
    "betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "mu_0 = np.matmul(X, betas_0) + 5 * (X[:, 0] ** 2)\n",
    "mu_1 = mu_0 + 8 * (X[:, 1] ** 2)  # quadratic\n",
    "tau = mu_1 - mu_0\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def complex_linear_cate(x_train, x_test):\n",
    "    betas_0 = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "    betas_1 = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas_0)\n",
    "    mu_1 = np.matmul(x_train, betas_1)\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas_0)\n",
    "    mu_1_test = np.matmul(x_test, betas_1)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI2 complex linear cate  (in Künzel it is low=1, high=30)\n",
    "betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "betas_1 = random.uniform(low=-5, high=5, size=d)\n",
    "mu_0 = np.matmul(X, betas_0)\n",
    "mu_1 = np.matmul(X, betas_1)\n",
    "tau = mu_1 - mu_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lahn das Laufe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varsigma_funct(x):\n",
    "    return 2 / (1 + np.exp(-12 * (x - 1 / 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def complex_nonlinear_cate(x_train, x_test):\n",
    "    # train\n",
    "    mu_0 = -1 / 2 * varsigma_funct(x=x_train[:, 0]) * varsigma_funct(x=x_train[:, 1])\n",
    "    mu_1 = 1 / 2 * varsigma_funct(x=x_train[:, 0]) * varsigma_funct(x=x_train[:, 1])\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = 2 * x_test[:, 0] - 1\n",
    "    mu_1_test = mu_0_test + 1 / 2 * varsigma_funct(x=x_test[:, 0]) * varsigma_funct(x=x_test[:, 1])\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI3 complex non-linear\n",
    "mu_0 = -1 / 2 * varsigma_funct(x=X[:, 0]) * varsigma_funct(x=X[:, 1])\n",
    "mu_1 = 1 / 2 * varsigma_funct(x=X[:, 0]) * varsigma_funct(x=X[:, 1])\n",
    "tau = mu_1 - mu_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def global_linear_response(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas)\n",
    "    mu_1 = mu_0\n",
    "    tau = np.zeros(len(x_train))\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas)\n",
    "    mu_1_test = mu_0_test\n",
    "    tau_test = np.zeros(len(x_test))\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI4 no treatment effect (global linear response)\n",
    "betas = random.uniform(low=-5, high=5, size=d)\n",
    "mu_0 = np.matmul(X, betas)\n",
    "mu_1 = mu_0\n",
    "tau = np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function for creating piecewise beta vector\n",
    "def piecewise_beta(d):\n",
    "    betas = np.random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "    betas_l = copy.copy(betas)  # betas_lower\n",
    "    betas_l[5:d] = 0\n",
    "\n",
    "    betas_m = copy.copy(betas)  #betas_middle\n",
    "    betas_m[0:5] = 0\n",
    "    betas_m[10:d] = 0\n",
    "\n",
    "    betas_u = copy.copy(betas)  #betas_upper\n",
    "    betas_u[0:10] = 0\n",
    "    betas_u[15:d] = 0\n",
    "\n",
    "    return betas_l, betas_m, betas_u\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def piecewise_linear_new(x):\n",
    "    condition_l = x[:, 19] < -0.4\n",
    "    condition_u = x[:, 19] > 0.4\n",
    "\n",
    "    array = np.zeros(len(x))\n",
    "    array[condition_l] = np.matmul(x[condition_l, :], betas_l)\n",
    "    array[~condition_l & ~condition_u] = np.matmul(x[~condition_l & ~condition_u, :], betas_m)\n",
    "    array[condition_u] = np.matmul(x[condition_u, :], betas_u)\n",
    "\n",
    "    return array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def piecewise_linear_response(x_train, x_test):\n",
    "    betas_l, betas_m, betas_u = piecewise_beta(x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = piecewise_linear_new(x_train)\n",
    "    mu_1 = mu_0\n",
    "    tau = np.zeros(len(x_train))\n",
    "\n",
    "    # test\n",
    "    mu_0_test = piecewise_linear_new(x_test)\n",
    "    mu_1_test = mu_0_test\n",
    "    tau_test = np.zeros(len(x_test))\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SI5 no treatment effect (piecewise linear response)\n",
    "betas = np.random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "betas_l = copy.copy(betas)  # betas_lower\n",
    "betas_l[5:d] = 0\n",
    "\n",
    "betas_m = copy.copy(betas)  #betas_middle\n",
    "betas_m[0:5] = 0\n",
    "betas_m[10:d] = 0\n",
    "\n",
    "betas_u = copy.copy(betas)  #betas_upper\n",
    "betas_u[0:10] = 0\n",
    "betas_u[15:d] = 0\n",
    "\n",
    "\n",
    "def piecewise_linear_new(x):\n",
    "    condition_l = x[:, 19] < -0.4\n",
    "    condition_u = x[:, 19] > 0.4\n",
    "\n",
    "    array = np.zeros(N)\n",
    "    array[condition_l] = np.matmul(x[condition_l, :], betas_l)\n",
    "    array[~condition_l & ~condition_u] = np.matmul(x[~condition_l & ~condition_u, :], betas_m)\n",
    "    array[condition_u] = np.matmul(x[condition_u, :], betas_u)\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "mu_0 = piecewise_linear_new(X)\n",
    "\n",
    "mu_1 = mu_0\n",
    "\n",
    "tau = np.zeros(N)\n",
    "\n",
    "# TODO: CHECK IF RIGHT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI5 no treatment effect (piecewise linear response)\n",
    "betas = np.random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "betas_l = copy.copy(betas)  # betas_lower\n",
    "betas_l[5:d] = 0\n",
    "\n",
    "betas_m = copy.copy(betas)  #betas_middle\n",
    "betas_m[0:5] = 0\n",
    "betas_m[10:d] = 0\n",
    "\n",
    "betas_u = copy.copy(betas)  #betas_upper\n",
    "betas_u[0:10] = 0\n",
    "betas_u[15:d] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_linear_new(x):\n",
    "    condition_l = x[:, 19] < -0.4\n",
    "    condition_u = x[:, 19] > 0.4\n",
    "\n",
    "    array = np.zeros(len(x))\n",
    "    array[condition_l] = np.matmul(x[condition_l, :], betas_l)\n",
    "    array[~condition_l & ~condition_u] = np.matmul(x[~condition_l & ~condition_u, :], betas_m)\n",
    "    array[condition_u] = np.matmul(x[condition_u, :], betas_u)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI6.1 beta confounded, no treatment effect # TODO: CHECK WHETER WHEN CONFOUNDED IT MAKES SENSE TO TAKE ALL BETAS, OR JUST LIKE HERE?\n",
    "X = random.uniform(low=0, high=1, size=(N, d))  # ACHTUNG: nöd wiederhole!\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0\n",
    "tau = np.zeros(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI6.2 beta confounded, simple cate (indicator)\n",
    "X = random.uniform(low=0, high=1, size=(N, d))\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0 + 2 * np.int8(X[:, 1] > 0.4)\n",
    "tau = mu_1 - mu_0\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI6.3 beta confounded, simple cate (linear)\n",
    "X = random.uniform(low=0, high=1, size=(N, d))\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0 + 2 * X[:, 1]\n",
    "tau = mu_1 - mu_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI6.4 beta confounded, simple cate (quadratic)\n",
    "X = random.uniform(low=0, high=1, size=(N, d))\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0 + 2 * (X[:, 1] ** 2)\n",
    "tau = mu_1 - mu_0\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI7.1 beta confounded, complex cate (linear) # TODO: SAME CHECK FOR CONFOUNDED AND BETAS!\n",
    "betas = random.uniform(low=-15, high=15, size=d)\n",
    "\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0 + np.matmul(X, betas)\n",
    "tau = mu_1 - mu_0\n",
    "\n",
    "# TODO: CHECK WHETHER THIS SETUP MAKES SENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI7.2 beta confounded, complex cate (non-linear)\n",
    "mu_0 = 2 * X[:, 0] - 1\n",
    "mu_1 = mu_0 + 1 / 2 * varsigma_funct(x=X[:, 0]) * varsigma_funct(x=X[:, 1])\n",
    "\n",
    "# TODO: CHECK WHETER THIS SETUP MAKES SENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fixed parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 2.3 Creat Potential Outcomes, FIX\n",
    "Y_0 = mu_0 + e_0\n",
    "Y_1 = mu_1 + e_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check Y_0\n",
    "Y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Check Y_1\n",
    "Y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 3.1 Propensity score setups # TODO: change for setup\n",
    "\n",
    "# i) constant, balanced\n",
    "e_x = 0.5\n",
    "\n",
    "# ii) constant, unbalanced\n",
    "e_x = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def beta_balanced(x_train, x_test):\n",
    "    beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "    # train\n",
    "    cdf_values = stats.norm.cdf(x_train[:, 0])\n",
    "    beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "    e_x = 1 / 4 * (1 + beta_values)\n",
    "\n",
    "    # test\n",
    "    cdf_values_test = stats.norm.cdf(x_test[:, 0])\n",
    "    beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "    e_x_test = 1 / 4 * (1 + beta_values_test)\n",
    "\n",
    "    return e_x, e_x_test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) beta confounded (balanced)\n",
    "beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "cdf_values = stats.norm.cdf(X[:, 0])\n",
    "beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "e_x = 1 / 4 * (1 + beta_values)\n",
    "\n",
    "pd.DataFrame(e_x).describe()  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(e_x).hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if balanced\n",
    "W = random.binomial(size=N, n=1, p=e_x)\n",
    "pd.DataFrame(W).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def beta_unbalanced(x_train, x_test):\n",
    "    beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "    # train\n",
    "    cdf_values = stats.norm.cdf(x_train[:, 0])\n",
    "    beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "    e_x = 1 / 40 * (1 + beta_values)\n",
    "\n",
    "    # test\n",
    "    cdf_values_test = stats.norm.cdf(x_test[:, 0])\n",
    "    beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "    e_x_test = 1 / 40 * (1 + beta_values_test)\n",
    "\n",
    "    return e_x, e_x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) beta confounded (unbalanced)\n",
    "beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "cdf_values = stats.norm.cdf(X[:, 0])\n",
    "beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "e_x = 1 / 20 * (1 + beta_values)\n",
    "\n",
    "pd.DataFrame(e_x).describe()  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(e_x).hist(bins=20)  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if unbalanced and how much\n",
    "W = random.binomial(size=N, n=1, p=e_x)\n",
    "pd.DataFrame(W).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "cdf_values = stats.norm.cdf(X[:, 0])\n",
    "beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "e_x = 1 / 4 * (1 + beta_values)\n",
    "\n",
    "pd.DataFrame(e_x).describe()  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(e_x).hist(bins=20)  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = random.binomial(size=N, n=1, p=e_x)\n",
    "pd.DataFrame(W).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Simulate Treatment Assignments trough W\n",
    "\n",
    "# Simulate Treatment Assignment, FIX\n",
    "W = random.binomial(size=N, n=1, p=e_x)\n",
    "\n",
    "# Create Observed Outcome, FIX\n",
    "ones = np.ones(N)\n",
    "Y = np.multiply(W, Y_1) + np.multiply(ones - W, Y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(e_x).describe()  # summary stats of e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out observed outcomes\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y).describe()  # summary stats of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((np.reshape(Y, (N, 1)), X, np.reshape(W, (N, 1)), np.reshape(tau, (N, 1))), axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.apppend(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATION SETUP: \"META-PARAMETERS\"\n",
    "\n",
    "sample_sizes = [1000, 2000, 5000, 10000]\n",
    "test_size = 1000\n",
    "n_runs = 30\n",
    "n_setups = 28\n",
    "d = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all settings of e_x\n",
    "exs = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "       'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced',\n",
    "       'beta_balanced',\n",
    "       'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced',\n",
    "       'beta_unbalanced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all settings of mu_0/mu_1\n",
    "cates = ['glinear', 'plinear', 'sindcate', 'slinearcate', 'squadcate', 'clinearcate', 'cnonlincate'] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TRIPPLE CHECK THIS FUNCTION, IT IS VERY IMPORTANT\n",
    "def generate(ex, cate, sample_size, test_size):  # TODO: add test set\n",
    "\n",
    "    # 1 generated x_train & x_test\n",
    "    cov = np.array(mpower.cvine(d=d, alpha=0.5, beta=0.5))  # TODO: set cov according to setup\n",
    "    X_train = random.multivariate_normal(mean=mean, cov=cov, size=sample_size, check_valid='warn')\n",
    "    X_test = random.multivariate_normal(mean=mean, cov=cov, size=test_size, check_valid='warn')\n",
    "\n",
    "    # 2 generate e_0 & e_0 (TODO: add test?)\n",
    "    # train\n",
    "    e_0 = random.normal(loc=0.0, scale=1.0, size=sample_size)\n",
    "    e_1 = random.normal(loc=0.0, scale=1.0, size=sample_size)\n",
    "\n",
    "    # test\n",
    "    e_0_test = random.normal(loc=0.0, scale=1.0, size=test_size)\n",
    "    e_1_test = random.normal(loc=0.0, scale=1.0, size=test_size)\n",
    "\n",
    "    # 3 compute mu_0 & mu_1 --> based on setting\n",
    "    if cate == 'glinear':  # 'global linear response' setting (no treatment effect)\n",
    "        betas = random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "        # train\n",
    "        mu_0 = np.matmul(X_train, betas)\n",
    "        mu_1 = mu_0\n",
    "        tau = np.zeros(sample_size)\n",
    "\n",
    "        # test\n",
    "        mu_0_test = np.matmul(X_test, betas)\n",
    "        mu_1_test = mu_0_test\n",
    "        tau_test = np.zeros(test_size)\n",
    "\n",
    "\n",
    "    elif cate == 'plinear':  # 'piecewise linear response' setting (no treatment effect)\n",
    "        betas = np.random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "        betas_l = copy.copy(betas)  # betas_lower\n",
    "        betas_l[5:d] = 0\n",
    "\n",
    "        betas_m = copy.copy(betas)  #betas_middle\n",
    "        betas_m[0:5] = 0\n",
    "        betas_m[10:d] = 0\n",
    "\n",
    "        betas_u = copy.copy(betas)  #betas_upper\n",
    "        betas_u[0:10] = 0\n",
    "        betas_u[15:d] = 0\n",
    "\n",
    "        # train\n",
    "        mu_0 = piecewise_linear_new(X_train)\n",
    "        mu_1 = mu_0\n",
    "        tau = np.zeros(sample_size)\n",
    "\n",
    "        # test\n",
    "        mu_0_test = piecewise_linear_new(X_test)\n",
    "        mu_1_test = mu_0_test\n",
    "        tau_test = np.zeros(test_size)\n",
    "\n",
    "\n",
    "    elif cate == 'sindcate':  # 'simple indicator cate' setting\n",
    "        betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "        # train\n",
    "        mu_0 = np.matmul(X_train, betas_0) + 5 * np.int8(X_train[:, 0] > 0.5)\n",
    "        mu_1 = mu_0 + 8 * np.int8(X_train[:, 1] > 0.1)\n",
    "        tau = mu_1 - mu_0\n",
    "\n",
    "        # test\n",
    "        mu_0_test = np.matmul(X_test, betas_0) + 5 * np.int8(X_test[:, 0] > 0.5)\n",
    "        mu_1_test = mu_0_test + 8 * np.int8(X_test[:, 1] > 0.1)\n",
    "        tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    elif cate == 'slinearcate':  # 'simple linear cate' setting\n",
    "        betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "        # train\n",
    "        mu_0 = np.matmul(X_train, betas_0) + 5 * X_train[:, 0]\n",
    "        mu_1 = mu_0 + 8 * X_train[:, 1]\n",
    "        tau = mu_1 - mu_0\n",
    "\n",
    "        # test\n",
    "        mu_0_test = np.matmul(X_test, betas_0) + 5 * X_test[:, 0]\n",
    "        mu_1_test = mu_0_test + 8 * X_test[:, 1]\n",
    "        tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    elif cate == 'squadcate':  # 'simple quadratic cate' setting\n",
    "        betas_0 = random.uniform(low=-5, high=5, size=d)  # TODO: CHANGE THIS SETTING ? AKA ADD THE BETAS?\n",
    "\n",
    "        # train\n",
    "        mu_0 = 2 * X_train[:, 0] - 1\n",
    "        mu_1 = mu_0 + 2 * (X_train[:, 1] ** 2)\n",
    "        tau = mu_1 - mu_0\n",
    "\n",
    "        # test\n",
    "        mu_0_test = 2 * X_test[:, 0] - 1\n",
    "        mu_1_test = mu_0_test + 2 * (X_test[:, 1] ** 2)\n",
    "        tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    elif cate == 'clinearcate':  # 'complex linear cate' setting\n",
    "        betas_0 = random.uniform(low=-5, high=5, size=d)\n",
    "        betas_1 = random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "        # train\n",
    "        mu_0 = np.matmul(X_train, betas_0)\n",
    "        mu_1 = np.matmul(X_train, betas_1)\n",
    "        tau = mu_1 - mu_0\n",
    "\n",
    "        # test\n",
    "        mu_0_test = np.matmul(X_test, betas_0)\n",
    "        mu_1_test = np.matmul(X_test, betas_1)\n",
    "        tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    elif cate == 'cnonlincate':  # 'complex non-linear cate' setting\n",
    "        # train\n",
    "        mu_0 = 2 * X_train[:, 0] - 1\n",
    "        mu_1 = mu_0 + 1 / 2 * varsigma_funct(x=X_train[:, 0]) * varsigma_funct(x=X_train[:, 1])\n",
    "        tau = mu_1 - mu_0\n",
    "\n",
    "        # test\n",
    "        mu_0_test = 2 * X_test[:, 0] - 1\n",
    "        mu_1_test = mu_0_test + 1 / 2 * varsigma_funct(x=X_test[:, 0]) * varsigma_funct(x=X_test[:, 1])\n",
    "        tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    else:\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    # 4 create potential outcomes Y_0 & Y_1 (TODO: test set?)\n",
    "    # train\n",
    "    Y_0 = mu_0 + e_0\n",
    "    Y_1 = mu_1 + e_1\n",
    "\n",
    "    # test\n",
    "    Y_0_test = mu_0_test + e_0_test\n",
    "    Y_1_test = mu_1_test + e_1_test\n",
    "\n",
    "    # 5 SET PROPENSITY SCORE E_X --> based on setting\n",
    "    if isinstance(ex, float):\n",
    "        e_x = ex\n",
    "        e_x_test = ex\n",
    "\n",
    "    elif ex == 'beta_balanced':\n",
    "        beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "        # train\n",
    "        cdf_values = stats.norm.cdf(X_train[:, 0])\n",
    "        beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "        e_x = 1 / 4 * (1 + beta_values)\n",
    "\n",
    "        # test\n",
    "        cdf_values_test = stats.norm.cdf(X_test[:, 0])\n",
    "        beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "        e_x_test = 1 / 4 * (1 + beta_values_test)\n",
    "\n",
    "    elif ex == 'beta_unbalanced':\n",
    "        beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "        #train\n",
    "        cdf_values = stats.norm.cdf(X_train[:, 0])\n",
    "        beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "        e_x = 1 / 20 * (1 + beta_values)\n",
    "\n",
    "        # test\n",
    "        #train\n",
    "        cdf_values_test = stats.norm.cdf(X_test[:, 0])\n",
    "        beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "        e_x_test = 1 / 20 * (1 + beta_values_test)\n",
    "\n",
    "    # 6 generate treatment assignment  W\n",
    "    W = random.binomial(size=sample_size, n=1, p=e_x)\n",
    "    W_test = random.binomial(size=test_size, n=1, p=e_x_test)\n",
    "\n",
    "    # 7 create observed variables Y\n",
    "    Y = np.multiply(W, Y_1) + np.multiply(np.ones(sample_size) - W, Y_0)\n",
    "    Y_test = np.multiply(W_test, Y_1_test) + np.multiply(np.ones(test_size) - W_test, Y_0_test)\n",
    "\n",
    "    # 8 create train & test sets\n",
    "    dataset_train = np.concatenate(\n",
    "        (np.reshape(Y, (sample_size, 1)), X_train, np.reshape(W, (sample_size, 1)), np.reshape(tau, (sample_size, 1))),\n",
    "        axis=1)\n",
    "    dataset_test = np.concatenate((np.reshape(Y_test, (test_size, 1)), X_test, np.reshape(W_test, (test_size, 1)),\n",
    "                                   np.reshape(tau_test, (test_size, 1))), axis=1)\n",
    "\n",
    "    # 9 return both sets\n",
    "    return [dataset_train, dataset_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(): --> für ein dataset!\n",
    "\n",
    "# loop over setups (28)\n",
    "## loop over sample_sizes (4)\n",
    "### loop over runs (30)\n",
    "\n",
    "\n",
    "# data[0][0][0][0]  # data[setup][sample_size][run][train/test]\n",
    "\n",
    "\n",
    "generate('beta_unbalanced', 'cnonlincate', 1000, 500)[1]\n",
    "\n",
    "# NICE :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to save all datasets\n",
    "data = []\n",
    "for i in range(n_setups):\n",
    "    data.append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        data[i].append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        for r in range(n_runs):\n",
    "            data[i][s].append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        for r in range(n_runs):\n",
    "            for t in range(2):\n",
    "                data[i][s][r].append([])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all datasets\n",
    "s = 0\n",
    "for setup in range(n_setups):\n",
    "    for size in sample_sizes:\n",
    "        for run in range(n_runs):\n",
    "            print(setup)\n",
    "            gen = generate(ex=exs[setup], cate=cates[setup], sample_size=size, test_size=test_size)\n",
    "            data[setup][s][run][0] = gen[0]  # add train-set\n",
    "            data[setup][s][run][1] = gen[1]  # add test-set\n",
    "        s += 1  # update index for sample_sizes\n",
    "    s = 0  # reset index for sample_sizes\n",
    "\n",
    "# it took 43 seconds # 4.4 GB of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Change the generate function such that test size is always the same, you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[27][3][29][1]  # NICE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 4 * 30 * 7 * 3  # 3360 datasets! 70560 trainings!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "sample_list = [1, 2, 3]\n",
    "file_name = \"/Users/arberimbibaj/Documents/Master Thesis ETH/DataSets /Generated/data_synthetic.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(data, open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "loaded_list = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(loaded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/Users/arberimbibaj/Documents/Master Thesis ETH/DataSets /Generated/data_synthetic.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "loaded_list = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_list[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS BACK TO WHAT IT WAS, SO NO CRASHES CAN HAPPEN.\n",
    "# Current values:\n",
    "# NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "# NotebookApp.rate_limit_window=3.0 (secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FROM SCRATCH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# seeds for reproducibility --> NICE :-D\n",
    "set_seed(8953)\n",
    "random.seed(8953)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SIMULATION SETUP: \"META-PARAMETERS\"\n",
    "sample_sizes = [100, 200, 500, 1000]\n",
    "test_size = 50\n",
    "n_runs = 5\n",
    "n_setups = 28\n",
    "d = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all settings of e_x\n",
    "exs = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "       'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced', 'beta_balanced',\n",
    "       'beta_balanced',\n",
    "       'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced', 'beta_unbalanced',\n",
    "       'beta_unbalanced']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all settings of mu_0 / mu_1\n",
    "cates = ['glinear', 'plinear', 'sindcate', 'slinearcate', 'squadcate', 'clinearcate', 'cnonlincate'] * 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cate Settings function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simple_indicator_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas) + 5 * np.int8(x_train[:, 0] > 0.5)\n",
    "    mu_1 = mu_0 + 8 * np.int8(x_train[:, 1] > 0.1)  # indicator\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas) + 5 * np.int8(x_test[:, 0] > 0.5)\n",
    "    mu_1_test = mu_0_test + 8 * np.int8(x_test[:, 1] > 0.1)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simple_linear_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas) + 5 * x_train[:, 0]\n",
    "    mu_1 = mu_0 + 8 * x_train[:, 1]\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas) + 5 * x_test[:, 0]\n",
    "    mu_1_test = mu_0_test + 8 * x_test[:, 1]\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: what does betas???\n",
    "def simple_quadratic_cate(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = 2 * x_train[:, 0] - 1\n",
    "    mu_1 = mu_0 + 2 * (x_train[:, 1] ** 2)\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = 2 * x_test[:, 0] - 1\n",
    "    mu_1_test = mu_0_test + 2 * (x_test[:, 1] ** 2)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def complex_linear_cate(x_train, x_test):\n",
    "    betas_0 = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "    betas_1 = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas_0)\n",
    "    mu_1 = np.matmul(x_train, betas_1)\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas_0)\n",
    "    mu_1_test = np.matmul(x_test, betas_1)\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def varsigma_funct(x):\n",
    "    return 2 / (1 + np.exp(-12 * (x - 1 / 2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def complex_nonlinear_cate(x_train, x_test):\n",
    "    # train\n",
    "    mu_0 = -1 / 2 * varsigma_funct(x=x_train[:, 0]) * varsigma_funct(x=x_train[:, 1])\n",
    "    mu_1 = 1 / 2 * varsigma_funct(x=x_train[:, 0]) * varsigma_funct(x=x_train[:, 1])\n",
    "    tau = mu_1 - mu_0\n",
    "\n",
    "    # test\n",
    "    mu_0_test = 2 * x_test[:, 0] - 1\n",
    "    mu_1_test = mu_0_test + 1 / 2 * varsigma_funct(x=x_test[:, 0]) * varsigma_funct(x=x_test[:, 1])\n",
    "    tau_test = mu_1_test - mu_0_test\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def global_linear_response(x_train, x_test):\n",
    "    betas = random.uniform(low=-5, high=5, size=x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = np.matmul(x_train, betas)\n",
    "    mu_1 = mu_0\n",
    "    tau = np.zeros(len(x_train))\n",
    "\n",
    "    # test\n",
    "    mu_0_test = np.matmul(x_test, betas)\n",
    "    mu_1_test = mu_0_test\n",
    "    tau_test = np.zeros(len(x_test))\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def piecewise_beta(d):\n",
    "    betas = np.random.uniform(low=-5, high=5, size=d)\n",
    "\n",
    "    betas_l = copy.copy(betas)  # betas_lower\n",
    "    betas_l[5:d] = 0\n",
    "\n",
    "    betas_m = copy.copy(betas)  #betas_middle\n",
    "    betas_m[0:5] = 0\n",
    "    betas_m[10:d] = 0\n",
    "\n",
    "    betas_u = copy.copy(betas)  #betas_upper\n",
    "    betas_u[0:10] = 0\n",
    "    betas_u[15:d] = 0\n",
    "\n",
    "    return betas_l, betas_m, betas_u\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def piecewise_linear_new(x, betas_l, betas_u, betas_m):\n",
    "    condition_l = x[:, 19] < -0.4\n",
    "    condition_u = x[:, 19] > 0.4\n",
    "\n",
    "    array = np.zeros(len(x))\n",
    "    array[condition_l] = np.matmul(x[condition_l, :], betas_l)\n",
    "    array[~condition_l & ~condition_u] = np.matmul(x[~condition_l & ~condition_u, :], betas_m)\n",
    "    array[condition_u] = np.matmul(x[condition_u, :], betas_u)\n",
    "\n",
    "    return array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def piecewise_linear_response(x_train, x_test):\n",
    "    betas_l, betas_m, betas_u = piecewise_beta(x_train.shape[1])\n",
    "\n",
    "    # train\n",
    "    mu_0 = piecewise_linear_new(x_train, betas_l, betas_u, betas_m)\n",
    "    mu_1 = mu_0\n",
    "    tau = np.zeros(len(x_train))\n",
    "\n",
    "    # test\n",
    "    mu_0_test = piecewise_linear_new(x_test, betas_l, betas_u, betas_m)\n",
    "    mu_1_test = mu_0_test\n",
    "    tau_test = np.zeros(len(x_test))\n",
    "\n",
    "    return mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ex functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def beta_balanced(x_train, x_test):\n",
    "    beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "    # train\n",
    "    cdf_values = stats.norm.cdf(x_train[:, 0])\n",
    "    beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "    e_x = 1 / 4 * (1 + beta_values)\n",
    "\n",
    "    # test\n",
    "    cdf_values_test = stats.norm.cdf(x_test[:, 0])\n",
    "    beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "    e_x_test = 1 / 4 * (1 + beta_values_test)\n",
    "\n",
    "    return e_x, e_x_test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def beta_unbalanced(x_train, x_test):\n",
    "    beta_dist = stats.beta(a=2, b=4)  # set beta distribution\n",
    "\n",
    "    # train\n",
    "    cdf_values = stats.norm.cdf(x_train[:, 0])\n",
    "    beta_values = beta_dist.pdf(cdf_values)  # calculate pdf values for x1\n",
    "    e_x = 1 / 40 * (1 + beta_values)\n",
    "\n",
    "    # test\n",
    "    cdf_values_test = stats.norm.cdf(x_test[:, 0])\n",
    "    beta_values_test = beta_dist.pdf(cdf_values_test)  # calculate pdf values for x1\n",
    "    e_x_test = 1 / 40 * (1 + beta_values_test)\n",
    "\n",
    "    return e_x, e_x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate train/test function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: TRIPPLE CHECK THIS FUNCTION, IT IS VERY IMPORTANT\n",
    "def generate(ex, cate, sample_size, test_size):\n",
    "\n",
    "    # 1 generated x_train & x_test\n",
    "    mean = np.zeros(d)\n",
    "    cov = np.array(mpower.cvine(d=d, alpha=0.5, beta=0.5))  # TODO: set cov according to setup\n",
    "    X_train = random.multivariate_normal(mean=mean, cov=cov, size=sample_size, check_valid='warn')\n",
    "    X_test = random.multivariate_normal(mean=mean, cov=cov, size=test_size, check_valid='warn')\n",
    "\n",
    "    # 2 generate e_0 & e_0\n",
    "    # train\n",
    "    e_0 = random.normal(loc=0.0, scale=1.0, size=sample_size)\n",
    "    e_1 = random.normal(loc=0.0, scale=1.0, size=sample_size)\n",
    "\n",
    "    # test\n",
    "    e_0_test = random.normal(loc=0.0, scale=1.0, size=test_size)\n",
    "    e_1_test = random.normal(loc=0.0, scale=1.0, size=test_size)\n",
    "\n",
    "    # 3 compute mu_0 & mu_1 --> based on setting\n",
    "    if cate == 'glinear':  # 'global linear response' setting (no treatment effect)\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = global_linear_response(X_train, X_test)\n",
    "\n",
    "    elif cate == 'plinear':  # 'piecewise linear response' setting (no treatment effect)\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = piecewise_linear_response(X_train, X_test)\n",
    "\n",
    "    elif cate == 'sindcate':  # 'simple indicator cate' setting\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = simple_indicator_cate(X_train, X_test)\n",
    "\n",
    "    elif cate == 'slinearcate':  # 'simple linear cate' setting\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = simple_linear_cate(X_train, X_test)\n",
    "\n",
    "    elif cate == 'squadcate':  # 'simple quadratic cate' setting\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = simple_quadratic_cate(X_train, X_test)\n",
    "\n",
    "    elif cate == 'clinearcate':  # 'complex linear cate' setting\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = complex_linear_cate(X_train, X_test)\n",
    "\n",
    "    elif cate == 'cnonlincate':  # 'complex non-linear cate' setting\n",
    "        mu_0, mu_1, tau, mu_0_test, mu_1_test, tau_test = complex_nonlinear_cate(X_train, X_test)\n",
    "\n",
    "    else:\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    # 4 create potential outcomes Y_0 & Y_1\n",
    "    # train\n",
    "    Y_0 = mu_0 + e_0\n",
    "    Y_1 = mu_1 + e_1\n",
    "\n",
    "    # test\n",
    "    Y_0_test = mu_0_test + e_0_test\n",
    "    Y_1_test = mu_1_test + e_1_test\n",
    "\n",
    "    # 5 SET PROPENSITY SCORE E_X --> based on setting\n",
    "    if isinstance(ex, float or int):\n",
    "        e_x = ex\n",
    "        e_x_test = ex\n",
    "\n",
    "    elif ex == 'beta_balanced':\n",
    "        e_x, e_x_test = beta_balanced(X_train, X_test)\n",
    "\n",
    "    elif ex == 'beta_unbalanced':\n",
    "        e_x, e_x_test = beta_unbalanced(X_train, X_test)\n",
    "\n",
    "    else:\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    # 6 generate treatment assignment  W\n",
    "    W = random.binomial(size=sample_size, n=1, p=e_x)\n",
    "    W_test = random.binomial(size=test_size, n=1, p=e_x_test)\n",
    "\n",
    "    # 7 create observed variables Y\n",
    "    Y = np.multiply(W, Y_1) + np.multiply(np.ones(sample_size) - W, Y_0)\n",
    "    Y_test = np.multiply(W_test, Y_1_test) + np.multiply(np.ones(test_size) - W_test, Y_0_test)\n",
    "\n",
    "    # 8 create train & test sets\n",
    "    dataset_train = np.concatenate(\n",
    "        (np.reshape(Y, (sample_size, 1)), X_train, np.reshape(W, (sample_size, 1)), np.reshape(tau, (sample_size, 1))),\n",
    "        axis=1)\n",
    "    dataset_test = np.concatenate((np.reshape(Y_test, (test_size, 1)), X_test, np.reshape(W_test, (test_size, 1)),\n",
    "                                   np.reshape(tau_test, (test_size, 1))), axis=1)\n",
    "\n",
    "    # 9 return both sets\n",
    "    return [dataset_train, dataset_test]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "empty list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create empty list to save all datasets\n",
    "data = []\n",
    "for i in range(n_setups):\n",
    "    data.append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        data[i].append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        for r in range(n_runs):\n",
    "            data[i][s].append([])\n",
    "\n",
    "for i in range(n_setups):\n",
    "    for s in range(4):\n",
    "        for r in range(n_runs):\n",
    "            for t in range(2):\n",
    "                data[i][s][r].append([])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate all datasets\n",
    "s = 0\n",
    "for setup in range(n_setups):\n",
    "    for size in sample_sizes:\n",
    "        for run in range(n_runs):\n",
    "            print(setup)\n",
    "            gen = generate(ex=exs[setup], cate=cates[setup], sample_size=size, test_size=test_size)\n",
    "            data[setup][s][run][0] = gen[0]  # add train-set\n",
    "            data[setup][s][run][1] = gen[1]  # add test-set\n",
    "        s += 1  # update index for sample_sizes\n",
    "    s = 0  # reset index for sample_sizes\n",
    "\n",
    "# it took 43 seconds # 4.4 GB of data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[0][0][0][0] # :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -1.992791, -7.838958 -> reproducibility check over :)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
